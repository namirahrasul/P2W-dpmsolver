data:
    dataset: "CELEBA"
    category: null
    #category not null for lsun
    image_size: 256
    channels: 3
    logit_transform: false
    # logit_transform not in p2w 
    uniform_dequantization: false
    # uniform_dequantization not in p2w 
    gaussian_dequantization: false
    #gaussian_dequantization not in p2w
    random_flip: true
    #random_flip matches p2w
    rescaled: true
    #rescaled matches p2w where this is hardcoded to always convert [0,255] to [-1,1]
    num_workers: 8
    #num_workers matches our mobile lab setup for p2w our checkpoint
    num_classes: null
    # was 1 originally but uncond image generation works with null.

model:
   ##only problem is matching num_heads to training checkpoint
    model_type: "p2-weighing"
    is_upsampling: false
    #is_upsampling matches p2w
    image_size: 256
    in_channels: 3 
    #in channels matches p2w
    model_channels: 128
    #model_channels matches num_channels in p2w
    out_channels: 6
    #out_channels matches p2w; 3 if learned_sigma=false
    num_res_blocks: 1
    #num_res_blocks matches p2w
    attention_resolutions: [16]
    #attention_resolutions matches p2w
    dropout: 0.0
    #dropout matches p2w
    channel_mult: [1, 1, 2, 2, 4, 4]
    #channel_mult matches p2w when image size is 256
    conv_resample: true
    # conv_resample matche p2w default but not used since resblock_updown=true
    dims: 2
    #dims matches p2w for 2d convolution 
    num_classes: null
    #num_classes null for image generation
    use_checkpoint: false
    #use_checkpoint for gradient checkpointing during training. False to match default value of p2w
    use_fp16: true
    # use_fp16 matches our p2w checkpoint 
    num_heads: 1
    #problem: training and sampling in p2w uses default 4, m2s sampling uses 1. grok suggest our_checkpoint actually is 8 and says num_heads =1 will give fastest inference but sacrifice quality
    num_head_channels: 64
    #num_head_channels matches p2w
    num_heads_upsample: -1
    #num_heads_upsample matches p2w default
    use_scale_shift_norm: true
    #use_scale_shift_norm: matches p2w
    resblock_updown: true
    #resblock_updown matches p2w
    use_new_attention_order: false
    #use_new_attention_order matches p2w default
    var_type: fixedlarge
    #var_type matches p2w for ddim ;p2w learned for ddpm sampling when learned_sigma=true; no learned option in dpmsolver
    p2_gamma: 1
    p2_k: 1
    ema: true
    #ema matches p2w where no ema only ema_rate
    ema_rate: 0.9999
    #ema_rate matches p2w default
    ckpt_dir: "./ddpm_ckpt/celebahq/256x256.pt"

    ##no rescale_learned_sigma in dpmsolver unlike p2w. not needed for sampling but needed in training

diffusion:
#all matches p2w
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000

sampling:
    total_N: 5
    batch_size: 1
    last_only: True
    fid_stats_dir: "./fid_stats/VIRTUAL_lsun_bedroom256.npz"
    fid_total_samples: 5
    fid_batch_size: 1
    cond_class: false
    classifier_scale: 0.0
