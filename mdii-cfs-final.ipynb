{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea554e30",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:03.657901Z",
     "iopub.status.busy": "2025-08-05T17:41:03.657598Z",
     "iopub.status.idle": "2025-08-05T17:41:05.127408Z",
     "shell.execute_reply": "2025-08-05T17:41:05.126338Z"
    },
    "papermill": {
     "duration": 1.482407,
     "end_time": "2025-08-05T17:41:05.130283",
     "exception": false,
     "start_time": "2025-08-05T17:41:03.647876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed37f457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:05.154021Z",
     "iopub.status.busy": "2025-08-05T17:41:05.153700Z",
     "iopub.status.idle": "2025-08-05T17:41:06.948623Z",
     "shell.execute_reply": "2025-08-05T17:41:06.947747Z"
    },
    "papermill": {
     "duration": 1.804971,
     "end_time": "2025-08-05T17:41:06.950050",
     "exception": false,
     "start_time": "2025-08-05T17:41:05.145079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/kaggle/working/p2w'...\r\n",
      "remote: Enumerating objects: 4013, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (43/43), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\r\n",
      "remote: Total 4013 (delta 17), reused 19 (delta 13), pack-reused 3970 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (4013/4013), 28.09 MiB | 40.92 MiB/s, done.\r\n",
      "Resolving deltas: 100% (123/123), done.\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /kaggle/working/p2w\n",
    "!git clone --branch inpaint https://github.com/namirahrasul/P2W-dpmsolver.git /kaggle/working/p2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a264b1d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:06.967047Z",
     "iopub.status.busy": "2025-08-05T17:41:06.966357Z",
     "iopub.status.idle": "2025-08-05T17:41:08.121177Z",
     "shell.execute_reply": "2025-08-05T17:41:08.119902Z"
    },
    "papermill": {
     "duration": 1.164911,
     "end_time": "2025-08-05T17:41:08.122978",
     "exception": false,
     "start_time": "2025-08-05T17:41:06.958067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!rm -rf /kaggle/working/miniconda\n",
    "!wget -q https://repo.anaconda.com/miniconda/Miniconda3-py39_23.5.2-0-Linux-x86_64.sh -O miniconda.sh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd69226",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:08.139865Z",
     "iopub.status.busy": "2025-08-05T17:41:08.139091Z",
     "iopub.status.idle": "2025-08-05T17:41:16.543995Z",
     "shell.execute_reply": "2025-08-05T17:41:16.543105Z"
    },
    "papermill": {
     "duration": 8.415067,
     "end_time": "2025-08-05T17:41:16.545661",
     "exception": false,
     "start_time": "2025-08-05T17:41:08.130594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX=/kaggle/working/miniconda\r\n",
      "Unpacking payload ...\r\n",
      "\r\n",
      "Installing base environment...\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages\r\n",
      "\r\n",
      "\r\n",
      "Downloading and Extracting Packages\r\n",
      "\r\n",
      "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\r\n",
      "installation finished.\r\n",
      "WARNING:\r\n",
      "    You currently have a PYTHONPATH environment variable set. This may cause\r\n",
      "    unexpected behavior when running the Python interpreter in Miniconda3.\r\n",
      "    For best results, please verify that your PYTHONPATH only points to\r\n",
      "    directories of packages that are compatible with the Python interpreter\r\n",
      "    in Miniconda3: /kaggle/working/miniconda\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x miniconda.sh\n",
    "!bash ./miniconda.sh -b -f -p /kaggle/working/miniconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7e752a",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:16.566853Z",
     "iopub.status.busy": "2025-08-05T17:41:16.566589Z",
     "iopub.status.idle": "2025-08-05T17:41:16.691021Z",
     "shell.execute_reply": "2025-08-05T17:41:16.689875Z"
    },
    "papermill": {
     "duration": 0.136441,
     "end_time": "2025-08-05T17:41:16.692662",
     "exception": false,
     "start_time": "2025-08-05T17:41:16.556221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrwxrwxrwx 1 root root 9 Aug  5 17:41 /kaggle/working/miniconda/bin/python -> python3.9\r\n"
     ]
    }
   ],
   "source": [
    "# Verify Miniconda installation (check for python binary)\n",
    "!ls -l /kaggle/working/miniconda/bin/python || echo \"Python binary missing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a1f3b8",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:16.714755Z",
     "iopub.status.busy": "2025-08-05T17:41:16.714485Z",
     "iopub.status.idle": "2025-08-05T17:41:17.067459Z",
     "shell.execute_reply": "2025-08-05T17:41:17.066538Z"
    },
    "papermill": {
     "duration": 0.365378,
     "end_time": "2025-08-05T17:41:17.069088",
     "exception": false,
     "start_time": "2025-08-05T17:41:16.703710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure Conda binaries are executable\n",
    "!chmod +x /kaggle/working/miniconda/bin/conda\n",
    "!chmod +x /kaggle/working/miniconda/bin/python  # Explicitly make python executable\n",
    "!chmod +x /kaggle/working/miniconda/etc/profile.d/conda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd06e82d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:17.089580Z",
     "iopub.status.busy": "2025-08-05T17:41:17.089287Z",
     "iopub.status.idle": "2025-08-05T17:41:17.093726Z",
     "shell.execute_reply": "2025-08-05T17:41:17.093090Z"
    },
    "papermill": {
     "duration": 0.015778,
     "end_time": "2025-08-05T17:41:17.094929",
     "exception": false,
     "start_time": "2025-08-05T17:41:17.079151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add Conda to PATH\n",
    "import os\n",
    "os.environ['PATH'] = \"/kaggle/working/miniconda/bin:\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58528a1",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:41:17.114494Z",
     "iopub.status.busy": "2025-08-05T17:41:17.114256Z",
     "iopub.status.idle": "2025-08-05T17:42:26.823957Z",
     "shell.execute_reply": "2025-08-05T17:42:26.822768Z"
    },
    "papermill": {
     "duration": 69.721465,
     "end_time": "2025-08-05T17:42:26.825836",
     "exception": false,
     "start_time": "2025-08-05T17:41:17.104371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!conda install conda=25.1.1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cde14ad7",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:42:26.850832Z",
     "iopub.status.busy": "2025-08-05T17:42:26.850434Z",
     "iopub.status.idle": "2025-08-05T17:42:41.054925Z",
     "shell.execute_reply": "2025-08-05T17:42:41.053544Z"
    },
    "papermill": {
     "duration": 14.219536,
     "end_time": "2025-08-05T17:42:41.056893",
     "exception": false,
     "start_time": "2025-08-05T17:42:26.837357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Initialize Conda\n",
    "!conda init bash\n",
    "\n",
    "# Create Conda environment with Python 3.9\n",
    "!conda create -n p2wd python=3.9 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdebaab",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:42:41.079119Z",
     "iopub.status.busy": "2025-08-05T17:42:41.078803Z",
     "iopub.status.idle": "2025-08-05T17:42:44.160876Z",
     "shell.execute_reply": "2025-08-05T17:42:44.159484Z"
    },
    "papermill": {
     "duration": 3.094758,
     "end_time": "2025-08-05T17:42:44.162865",
     "exception": false,
     "start_time": "2025-08-05T17:42:41.068107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!conda install -n p2wd mpi4py openmpi -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da309bc1",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T17:42:44.186742Z",
     "iopub.status.busy": "2025-08-05T17:42:44.186035Z",
     "iopub.status.idle": "2025-08-05T17:45:22.830284Z",
     "shell.execute_reply": "2025-08-05T17:45:22.829196Z"
    },
    "papermill": {
     "duration": 158.657411,
     "end_time": "2025-08-05T17:45:22.832064",
     "exception": false,
     "start_time": "2025-08-05T17:42:44.174653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2wd && \\\n",
    "cd /kaggle/working/p2w && python setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb85ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:22.853502Z",
     "iopub.status.busy": "2025-08-05T17:45:22.853142Z",
     "iopub.status.idle": "2025-08-05T17:45:23.206644Z",
     "shell.execute_reply": "2025-08-05T17:45:23.205101Z"
    },
    "papermill": {
     "duration": 0.366335,
     "end_time": "2025-08-05T17:45:23.208682",
     "exception": false,
     "start_time": "2025-08-05T17:45:22.842347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/p2w/ddpm_ckpt\n",
    "!mkdir /kaggle/working/p2w/ddpm_ckpt/celeba\n",
    "!mkdir /kaggle/working/p2w/ddpm_ckpt/imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09343a08",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.010157,
     "end_time": "2025-08-05T17:45:23.229383",
     "exception": false,
     "start_time": "2025-08-05T17:45:23.219226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "our checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cc7e17e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:23.250984Z",
     "iopub.status.busy": "2025-08-05T17:45:23.250664Z",
     "iopub.status.idle": "2025-08-05T17:45:23.256993Z",
     "shell.execute_reply": "2025-08-05T17:45:23.256307Z"
    },
    "papermill": {
     "duration": 0.018577,
     "end_time": "2025-08-05T17:45:23.258163",
     "exception": false,
     "start_time": "2025-08-05T17:45:23.239586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alexnet_download_str = 'import os\\n' \\\n",
    "'import torch\\n' \\\n",
    "'import torchvision.models as models\\n' \\\n",
    "'from torchvision.models import AlexNet_Weights\\n' \\\n",
    "'import shutil\\n' \\\n",
    "'from lpips import LPIPS\\n' \\\n",
    "'import importlib.util\\n' \\\n",
    "'\\n' \\\n",
    "'# Set cache directory\\n' \\\n",
    "'cache_dir = \"/kaggle/working/p2w/.cache/torch/\"\\n' \\\n",
    "'os.environ[\"TORCH_HOME\"] = cache_dir\\n' \\\n",
    "'os.makedirs(os.path.join(cache_dir, \"hub/checkpoints\"), exist_ok=True)' \\\n",
    "'\\n' \\\n",
    "'# Download AlexNet weights\\n' \\\n",
    "'print(\"Pre-downloading AlexNet weights...\")\\n' \\\n",
    "'alexnet = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\\n' \\\n",
    "'print(\"AlexNet weights downloaded successfully!\")\\n' \\\n",
    "'\\n' \\\n",
    "'# Check for weights file\\n' \\\n",
    "'weights_path = os.path.join(cache_dir, \"hub/checkpoints/alexnet-owt-7be5be79.pth\")\\n' \\\n",
    "'print(f\"Checking for weights at: {weights_path}\")\\n' \\\n",
    "'if os.path.exists(weights_path):\\n' \\\n",
    "'    print(f\"Weights file found at: {weights_path}\")\\n' \\\n",
    "'else:\\n' \\\n",
    "'    print(\"Warning: Weights file not found. Saving manually...\")\\n' \\\n",
    "'    torch.save(alexnet.state_dict(), weights_path)\\n' \\\n",
    "'    if os.path.exists(weights_path):\\n' \\\n",
    "'        print(f\"Weights saved to: {weights_path}\")\\n' \\\n",
    "'    else:\\n' \\\n",
    "'        print(\"Failed to save weights.\")\\n' \\\n",
    "'# Preload LPIPS to ensure weights are available (bundled in package)\\n'\\\n",
    "'print(\"Pre-downloading LPIPS weights...\")\\n'\\\n",
    "'lpips_model = LPIPS(net=\"alex\")\\n'\\\n",
    "'print(\"LPIPS weights downloaded successfully!\")\\n'\\\n",
    "'\\n'\\\n",
    "'# Dynamically get LPIPS weights path from installed package (alternative to __file__)\\n'\\\n",
    "'lpips_spec = importlib.util.find_spec(\"lpips\")\\n'\\\n",
    "'if lpips_spec is not None:\\n'\\\n",
    "'    module_dir = os.path.dirname(lpips_spec.origin)\\n'\\\n",
    "'    package_weights_dir = os.path.join(module_dir, \"weights\", \"v0.1\")\\n'\\\n",
    "'    default_lpips_path = os.path.join(package_weights_dir, \"alex.pth\")\\n'\\\n",
    "'    print(f\"LPIPS source weights path: {default_lpips_path}\")\\n'\\\n",
    "'else:\\n'\\\n",
    "'    print(\"Error: lpips module not found.\")\\n'\\\n",
    "'    default_lpips_path = None\\n'\\\n",
    "'\\n'\\\n",
    "'# Copy LPIPS weights to custom persistent path\\n'\\\n",
    "'lpips_cache_dir = \"/kaggle/working/p2w/.cache/lpips/\"\\n'\\\n",
    "'os.makedirs(lpips_cache_dir, exist_ok=True)\\n'\\\n",
    "'custom_lpips_path = os.path.join(lpips_cache_dir, \"alex.pth\")  # Corrected filename to match default\\n'\\\n",
    "'if default_lpips_path and os.path.exists(default_lpips_path):\\n'\\\n",
    "'    shutil.copy(default_lpips_path, custom_lpips_path)\\n'\\\n",
    "'    print(f\"LPIPS weights copied to: {custom_lpips_path}\")\\n'\\\n",
    "'else:\\n'\\\n",
    "'    print(\"Warning: LPIPS weights not found in package path.\")\\n'\n",
    "with open('/kaggle/working/p2w/alexnet_download.py', 'w') as f:\n",
    "    f.write(alexnet_download_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4bd70b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:23.279939Z",
     "iopub.status.busy": "2025-08-05T17:45:23.279336Z",
     "iopub.status.idle": "2025-08-05T17:45:30.693043Z",
     "shell.execute_reply": "2025-08-05T17:45:30.692155Z"
    },
    "papermill": {
     "duration": 7.427314,
     "end_time": "2025-08-05T17:45:30.695518",
     "exception": false,
     "start_time": "2025-08-05T17:45:23.268204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-downloading AlexNet weights...\r\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /kaggle/working/p2w/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\r\n",
      "100%|█████████████████████████████████████████| 233M/233M [00:01<00:00, 166MB/s]\r\n",
      "AlexNet weights downloaded successfully!\r\n",
      "Checking for weights at: /kaggle/working/p2w/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\r\n",
      "Weights file found at: /kaggle/working/p2w/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\r\n",
      "Pre-downloading LPIPS weights...\r\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\r\n",
      "/kaggle/working/miniconda/envs/p2wd/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\r\n",
      "  warnings.warn(\r\n",
      "/kaggle/working/miniconda/envs/p2wd/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\r\n",
      "  warnings.warn(msg)\r\n",
      "Loading model from: /kaggle/working/miniconda/envs/p2wd/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\r\n",
      "LPIPS weights downloaded successfully!\r\n",
      "LPIPS source weights path: /kaggle/working/miniconda/envs/p2wd/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\r\n",
      "LPIPS weights copied to: /kaggle/working/p2w/.cache/lpips/alex.pth\r\n"
     ]
    }
   ],
   "source": [
    "!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2wd && \\\n",
    "cd /kaggle/working/p2w && python alexnet_download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170fabfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:30.720470Z",
     "iopub.status.busy": "2025-08-05T17:45:30.719417Z",
     "iopub.status.idle": "2025-08-05T17:45:54.684091Z",
     "shell.execute_reply": "2025-08-05T17:45:54.683078Z"
    },
    "papermill": {
     "duration": 23.97797,
     "end_time": "2025-08-05T17:45:54.685357",
     "exception": false,
     "start_time": "2025-08-05T17:45:30.707387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1OsL6UrYFUtmwBLcOdRViRcfTSdoIySjR\n",
      "From (redirected): https://drive.google.com/uc?id=1OsL6UrYFUtmwBLcOdRViRcfTSdoIySjR&confirm=t&uuid=dce0ff07-eb36-4a6d-b405-8c9df4128954\n",
      "To: /kaggle/working/p2w/ddpm_ckpt/imagenet/64_ema_0.9999_1000000.pt\n",
      "100%|██████████| 273M/273M [00:03<00:00, 81.9MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1fX0mcIWPAz7fd7tIWcVxFuPf65bqCjj3\n",
      "From (redirected): https://drive.google.com/uc?id=1fX0mcIWPAz7fd7tIWcVxFuPf65bqCjj3&confirm=t&uuid=2b011f84-3289-49f8-805d-4a354a72829e\n",
      "To: /kaggle/working/p2w/ddpm_ckpt/imagenet/256_ema_0.9999_1000000.pt\n",
      "100%|██████████| 374M/374M [00:03<00:00, 104MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GWdEYCqcGd55Gqkli80Z_O_t9cKuJWWP\n",
      "From (redirected): https://drive.google.com/uc?id=1GWdEYCqcGd55Gqkli80Z_O_t9cKuJWWP&confirm=t&uuid=bc28916d-857d-48df-9158-334a786ee0f5\n",
      "To: /kaggle/working/p2w/ddpm_ckpt/celeba/64_ema_0.9999_1000000.pt\n",
      "100%|██████████| 273M/273M [00:04<00:00, 64.1MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=16Hz_Fis2PPmq2RrFy9Kj9WkZoXdsc3K0\n",
      "From (redirected): https://drive.google.com/uc?id=16Hz_Fis2PPmq2RrFy9Kj9WkZoXdsc3K0&confirm=t&uuid=6cd79819-531c-488c-beda-f6a695ccc170\n",
      "To: /kaggle/working/p2w/ddpm_ckpt/celeba/256_ema_0.9999_1000000.pt\n",
      "100%|██████████| 374M/374M [00:05<00:00, 72.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/p2w/ddpm_ckpt/celeba/256_ema_0.9999_1000000.pt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "#imagenet(1M epoch)\n",
    "file_id_64_i = '1OsL6UrYFUtmwBLcOdRViRcfTSdoIySjR'\n",
    "file_id_256_i = '1fX0mcIWPAz7fd7tIWcVxFuPf65bqCjj3'\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id_64_i}', '/kaggle/working/p2w/ddpm_ckpt/imagenet/64_ema_0.9999_1000000.pt', quiet=False)\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id_256_i}', '/kaggle/working/p2w/ddpm_ckpt/imagenet/256_ema_0.9999_1000000.pt', quiet=False)\n",
    "#celebahq(1M epoch)\n",
    "file_id_64_c = '1GWdEYCqcGd55Gqkli80Z_O_t9cKuJWWP' \n",
    "file_id_256_c = '16Hz_Fis2PPmq2RrFy9Kj9WkZoXdsc3K0'\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id_64_c}', '/kaggle/working/p2w/ddpm_ckpt/celeba/64_ema_0.9999_1000000.pt', quiet=False)\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id_256_c}', '/kaggle/working/p2w/ddpm_ckpt/celeba/256_ema_0.9999_1000000.pt', quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bec6fcfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:54.722564Z",
     "iopub.status.busy": "2025-08-05T17:45:54.722130Z",
     "iopub.status.idle": "2025-08-05T17:45:54.769457Z",
     "shell.execute_reply": "2025-08-05T17:45:54.768427Z"
    },
    "papermill": {
     "duration": 0.069644,
     "end_time": "2025-08-05T17:45:54.770949",
     "exception": false,
     "start_time": "2025-08-05T17:45:54.701305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote sampler.py to /kaggle/working/p2w/dpm_solver/sampler.py\n"
     ]
    }
   ],
   "source": [
    "sampler_str = 'import torch\\n' \\\n",
    "'import torch.nn.functional as F\\n' \\\n",
    "'import math\\n' \\\n",
    "'import os\\n' \\\n",
    "'import torchvision.utils as tvu\\n' \\\n",
    "'import inspect\\n' \\\n",
    "'import traceback\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'class NoiseScheduleVP:\\n' \\\n",
    "'    def __init__(\\n' \\\n",
    "'            self,\\n' \\\n",
    "'            schedule=\"discrete\",\\n' \\\n",
    "'            betas=None,\\n' \\\n",
    "'            alphas_cumprod=None,\\n' \\\n",
    "'            continuous_beta_0=0.1,\\n' \\\n",
    "'            continuous_beta_1=20.,\\n' \\\n",
    "'            dtype=torch.float32,\\n' \\\n",
    "'        ):\\n' \\\n",
    "'        \"\"\"Create a wrapper class for the forward SDE (VP type).\\n' \\\n",
    "'\\n' \\\n",
    "'        ***\\n' \\\n",
    "'        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\\n' \\\n",
    "'                We recommend to use schedule=\"discrete\" for the discrete-time diffusion models, especially for high-resolution images.\\n' \\\n",
    "'        ***\\n' \\\n",
    "'\\n' \\\n",
    "'        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\\n' \\\n",
    "'        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\\n' \\\n",
    "'        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\\n' \\\n",
    "'\\n' \\\n",
    "'            log_alpha_t = self.marginal_log_mean_coeff(t)\\n' \\\n",
    "'            sigma_t = self.marginal_std(t)\\n' \\\n",
    "'            lambda_t = self.marginal_lambda(t)\\n' \\\n",
    "'\\n' \\\n",
    "'        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\\n' \\\n",
    "'\\n' \\\n",
    "'            t = self.inverse_lambda(lambda_t)\\n' \\\n",
    "'\\n' \\\n",
    "'        ===============================================================\\n' \\\n",
    "'\\n' \\\n",
    "'        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\\n' \\\n",
    "'\\n' \\\n",
    "'        1. For discrete-time DPMs:\\n' \\\n",
    "'\\n' \\\n",
    "'            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\\n' \\\n",
    "'                t_i = (i + 1) / N\\n' \\\n",
    "'            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\\n' \\\n",
    "'            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\\n' \\\n",
    "'\\n' \\\n",
    "'            Args:\\n' \\\n",
    "'                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\\n' \\\n",
    "'                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\\n' \\\n",
    "'\\n' \\\n",
    "'            Note that we always have alphas_cumprod = cumprod(1 - betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\\n' \\\n",
    "'\\n' \\\n",
    "'            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\\n' \\\n",
    "'                The `alphas_cumprod` is the \\\\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\\n' \\\n",
    "'                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\\\sqrt{\\\\hat{alpha_n}} * x_0, (1 - \\\\hat{alpha_n}) * I ).\\n' \\\n",
    "'                Therefore, the notation \\\\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\\n' \\\n",
    "'                    alpha_{t_n} = \\\\sqrt{\\\\hat{alpha_n}},\\n' \\\n",
    "'                and\\n' \\\n",
    "'                    log(alpha_{t_n}) = 0.5 * log(\\\\hat{alpha_n}).\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'        2. For continuous-time DPMs:\\n' \\\n",
    "'\\n' \\\n",
    "'            We support the linear VPSDE for the continuous time setting. The hyperparameters for the noise\\n' \\\n",
    "'            schedule are the default settings in Yang Song\\'s ScoreSDE:\\n' \\\n",
    "'\\n' \\\n",
    "'            Args:\\n' \\\n",
    "'                beta_min: A `float` number. The smallest beta for the linear schedule.\\n' \\\n",
    "'                beta_max: A `float` number. The largest beta for the linear schedule.\\n' \\\n",
    "'                T: A `float` number. The ending time of the forward process.\\n' \\\n",
    "'\\n' \\\n",
    "'        ===============================================================\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            schedule: A `str`. The noise schedule of the forward SDE. \\'discrete\\' for discrete-time DPMs,\\n' \\\n",
    "'                    \\'linear\\' for continuous-time DPMs.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            A wrapper object of the forward SDE (VP type).\\n' \\\n",
    "'        \\n' \\\n",
    "'        ===============================================================\\n' \\\n",
    "'\\n' \\\n",
    "'        Example:\\n' \\\n",
    "'\\n' \\\n",
    "'        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\\n' \\\n",
    "'        >>> ns = NoiseScheduleVP(\\'discrete\\', betas=betas)\\n' \\\n",
    "'\\n' \\\n",
    "'        # For discrete-time DPMs, given alphas_cumprod (the \\\\hat{alpha_n} array for n = 0, 1, ..., N - 1):\\n' \\\n",
    "'        >>> ns = NoiseScheduleVP(\\'discrete\\', alphas_cumprod=alphas_cumprod)\\n' \\\n",
    "'\\n' \\\n",
    "'        # For continuous-time DPMs (VPSDE), linear schedule:\\n' \\\n",
    "'        >>> ns = NoiseScheduleVP(\\'linear\\', continuous_beta_0=0.1, continuous_beta_1=20.)\\n' \\\n",
    "'\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'\\n' \\\n",
    "'        if schedule not in [\\'discrete\\', \\'linear\\']:\\n' \\\n",
    "'            raise ValueError(\"Unsupported noise schedule {}. The schedule needs to be \\'discrete\\' or \\'linear\\'\".format(schedule))\\n' \\\n",
    "'\\n' \\\n",
    "'        self.schedule = schedule\\n' \\\n",
    "'        if schedule == \\'discrete\\':\\n' \\\n",
    "'            if betas is not None:\\n' \\\n",
    "'                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                assert alphas_cumprod is not None\\n' \\\n",
    "'                log_alphas = 0.5 * torch.log(alphas_cumprod)\\n' \\\n",
    "'            self.alphas_cumprod = alphas_cumprod\\n' \\\n",
    "'            self.T = 1.\\n' \\\n",
    "'            self.log_alpha_array = self.numerical_clip_alpha(log_alphas).reshape((1, -1,)).to(dtype=dtype, device=alphas_cumprod.device)\\n' \\\n",
    "'            self.total_N = self.log_alpha_array.shape[1]\\n' \\\n",
    "'            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1)).to(dtype=dtype, device=alphas_cumprod.device)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            self.T = 1.\\n' \\\n",
    "'            self.total_N = 1000\\n' \\\n",
    "'            self.beta_0 = continuous_beta_0\\n' \\\n",
    "'            self.beta_1 = continuous_beta_1\\n' \\\n",
    "'\\n' \\\n",
    "'    def numerical_clip_alpha(self, log_alphas, clipped_lambda=-5.1):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        For some beta schedules such as cosine schedule, the log-SNR has numerical isssues. \\n' \\\n",
    "'        We clip the log-SNR near t=T within -5.1 to ensure the stability.\\n' \\\n",
    "'        Such a trick is very useful for diffusion models with the cosine schedule, such as i-DDPM, guided-diffusion and GLIDE.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        log_sigmas = 0.5 * torch.log(1. - torch.exp(2. * log_alphas))\\n' \\\n",
    "'        lambs = log_alphas - log_sigmas\\n' \\\n",
    "'        idx = torch.searchsorted(torch.flip(lambs, [0]), clipped_lambda)\\n' \\\n",
    "'        if idx > 0:\\n' \\\n",
    "'            log_alphas = log_alphas[:-idx]\\n' \\\n",
    "'        return log_alphas\\n' \\\n",
    "'\\n' \\\n",
    "'    def marginal_log_mean_coeff(self, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute log(alpha_t) of a given continuous-time label t in [0, T].\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if self.schedule == \\'discrete\\':\\n' \\\n",
    "'            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\\n' \\\n",
    "'        elif self.schedule == \\'linear\\':\\n' \\\n",
    "'            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\\n' \\\n",
    "'\\n' \\\n",
    "'    def marginal_alpha(self, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute alpha_t of a given continuous-time label t in [0, T].\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        return torch.exp(self.marginal_log_mean_coeff(t))\\n' \\\n",
    "'\\n' \\\n",
    "'    def marginal_std(self, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute sigma_t of a given continuous-time label t in [0, T].\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\\n' \\\n",
    "'\\n' \\\n",
    "'    def marginal_lambda(self, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        log_mean_coeff = self.marginal_log_mean_coeff(t)\\n' \\\n",
    "'        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\\n' \\\n",
    "'        return log_mean_coeff - log_std\\n' \\\n",
    "'\\n' \\\n",
    "'    def inverse_lambda(self, lamb):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if self.schedule == \\'linear\\':\\n' \\\n",
    "'            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\\n' \\\n",
    "'            Delta = self.beta_0**2 + tmp\\n' \\\n",
    "'            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\\n' \\\n",
    "'        elif self.schedule == \\'discrete\\':\\n' \\\n",
    "'            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\\n' \\\n",
    "'            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\\n' \\\n",
    "'            return t.reshape((-1,))\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'def model_wrapper(\\n' \\\n",
    "'    model,\\n' \\\n",
    "'    noise_schedule,\\n' \\\n",
    "'    model_type=\"noise\",\\n' \\\n",
    "'    model_kwargs={},\\n' \\\n",
    "'    guidance_type=\"uncond\",\\n' \\\n",
    "'    condition=None,\\n' \\\n",
    "'    unconditional_condition=None,\\n' \\\n",
    "'    guidance_scale=1.,\\n' \\\n",
    "'    classifier_fn=None,\\n' \\\n",
    "'    classifier_kwargs={},\\n' \\\n",
    "'):\\n' \\\n",
    "'    \"\"\"Create a wrapper function for the noise prediction model.\\n' \\\n",
    "'\\n' \\\n",
    "'    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\\n' \\\n",
    "'    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\\n' \\\n",
    "'\\n' \\\n",
    "'    We support four types of the diffusion model by setting `model_type`:\\n' \\\n",
    "'\\n' \\\n",
    "'        1. \"noise\": noise prediction model. (Trained by predicting noise).\\n' \\\n",
    "'\\n' \\\n",
    "'        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\\n' \\\n",
    "'\\n' \\\n",
    "'        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\\n' \\\n",
    "'            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\\n' \\\n",
    "'\\n' \\\n",
    "'            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\\n' \\\n",
    "'                arXiv preprint arXiv:2202.00512 (2022).\\n' \\\n",
    "'            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\\n' \\\n",
    "'                arXiv preprint arXiv:2210.02303 (2022).\\n' \\\n",
    "'    \\n' \\\n",
    "'        4. \"score\": marginal score function. (Trained by denoising score matching).\\n' \\\n",
    "'            Note that the score function and the noise prediction model follows a simple relationship:\\n' \\\n",
    "'            ```\\n' \\\n",
    "'                noise(x_t, t) = -sigma_t * score(x_t, t)\\n' \\\n",
    "'            ```\\n' \\\n",
    "'\\n' \\\n",
    "'    We support three types of guided sampling by DPMs by setting `guidance_type`:\\n' \\\n",
    "'        1. \"uncond\": unconditional sampling by DPMs.\\n' \\\n",
    "'            The input `model` has the following format:\\n' \\\n",
    "'            ``\\n' \\\n",
    "'                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\\n' \\\n",
    "'            ``\\n' \\\n",
    "'\\n' \\\n",
    "'        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\\n' \\\n",
    "'            The input `model` has the following format:\\n' \\\n",
    "'            ``\\n' \\\n",
    "'                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\\n' \\\n",
    "'            `` \\n' \\\n",
    "'\\n' \\\n",
    "'            The input `classifier_fn` has the following format:\\n' \\\n",
    "'            ``\\n' \\\n",
    "'                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\\n' \\\n",
    "'            ``\\n' \\\n",
    "'\\n' \\\n",
    "'            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\\n' \\\n",
    "'                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\\n' \\\n",
    "'\\n' \\\n",
    "'        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\\n' \\\n",
    "'            The input `model` has the following format:\\n' \\\n",
    "'            ``\\n' \\\n",
    "'                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\\n' \\\n",
    "'            `` \\n' \\\n",
    "'            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\\n' \\\n",
    "'\\n' \\\n",
    "'            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\\n' \\\n",
    "'                arXiv preprint arXiv:2207.12598 (2022).\\n' \\\n",
    "'        \\n' \\\n",
    "'\\n' \\\n",
    "'    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\\n' \\\n",
    "'    or continuous-time labels (i.e. epsilon to T).\\n' \\\n",
    "'\\n' \\\n",
    "'    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\\n' \\\n",
    "'    ``\\n' \\\n",
    "'        def model_fn(x, t_continuous) -> noise:\\n' \\\n",
    "'            t_input = get_model_input_time(t_continuous)\\n' \\\n",
    "'            return noise_pred(model, x, t_input, **model_kwargs)         \\n' \\\n",
    "'    ``\\n' \\\n",
    "'    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\\n' \\\n",
    "'\\n' \\\n",
    "'    ===============================================================\\n' \\\n",
    "'\\n' \\\n",
    "'    Args:\\n' \\\n",
    "'        model: A diffusion model with the corresponding format described above.\\n' \\\n",
    "'        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\\n' \\\n",
    "'        model_type: A `str`. The parameterization type of the diffusion model.\\n' \\\n",
    "'                    \"noise\" or \"x_start\" or \"v\" or \"score\".\\n' \\\n",
    "'        model_kwargs: A `dict`. A dict for the other inputs of the model function.\\n' \\\n",
    "'        guidance_type: A `str`. The type of the guidance for sampling.\\n' \\\n",
    "'                    \"uncond\" or \"classifier\" or \"classifier-free\".\\n' \\\n",
    "'        condition: A pytorch tensor. The condition for the guided sampling.\\n' \\\n",
    "'                    Only used for \"classifier\" or \"classifier-free\" guidance type.\\n' \\\n",
    "'        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\\n' \\\n",
    "'                    Only used for \"classifier-free\" guidance type.\\n' \\\n",
    "'        guidance_scale: A `float`. The scale for the guided sampling.\\n' \\\n",
    "'        classifier_fn: A classifier function. Only used for the classifier guidance.\\n' \\\n",
    "'        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\\n' \\\n",
    "'    Returns:\\n' \\\n",
    "'        A noise prediction model that accepts the noised data and the continuous time as the inputs.\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'\\n' \\\n",
    "'    def get_model_input_time(t_continuous):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\\n' \\\n",
    "'        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\\n' \\\n",
    "'        For continuous-time DPMs, we just use `t_continuous`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if noise_schedule.schedule == \\'discrete\\':\\n' \\\n",
    "'            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            return t_continuous\\n' \\\n",
    "'\\n' \\\n",
    "'    def noise_pred_fn(x, t_continuous, cond=None):\\n' \\\n",
    "'        t_input = get_model_input_time(t_continuous)\\n' \\\n",
    "'        if cond is None:\\n' \\\n",
    "'            output = model(x, t_input, **model_kwargs)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            output = model(x, t_input, cond, **model_kwargs)\\n' \\\n",
    "'        if model_type == \"noise\":\\n' \\\n",
    "'            return output\\n' \\\n",
    "'        elif model_type == \"x_start\":\\n' \\\n",
    "'            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\\n' \\\n",
    "'            return (x - alpha_t * output) / sigma_t\\n' \\\n",
    "'        elif model_type == \"v\":\\n' \\\n",
    "'            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\\n' \\\n",
    "'            return alpha_t * output + sigma_t * x\\n' \\\n",
    "'        elif model_type == \"score\":\\n' \\\n",
    "'            sigma_t = noise_schedule.marginal_std(t_continuous)\\n' \\\n",
    "'            return -sigma_t * output\\n' \\\n",
    "'\\n' \\\n",
    "'    def cond_grad_fn(x, t_input):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        with torch.enable_grad():\\n' \\\n",
    "'            x_in = x.detach().requires_grad_(True)\\n' \\\n",
    "'            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\\n' \\\n",
    "'            return torch.autograd.grad(log_prob.sum(), x_in)[0]\\n' \\\n",
    "'\\n' \\\n",
    "'    def model_fn(x, t_continuous):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        The noise predicition model function that is used for DPM-Solver.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if guidance_type == \"uncond\":\\n' \\\n",
    "'            return noise_pred_fn(x, t_continuous)\\n' \\\n",
    "'        elif guidance_type == \"classifier\":\\n' \\\n",
    "'            assert classifier_fn is not None\\n' \\\n",
    "'            t_input = get_model_input_time(t_continuous)\\n' \\\n",
    "'            cond_grad = cond_grad_fn(x, t_input)\\n' \\\n",
    "'            sigma_t = noise_schedule.marginal_std(t_continuous)\\n' \\\n",
    "'            noise = noise_pred_fn(x, t_continuous)\\n' \\\n",
    "'            return noise - guidance_scale * expand_dims(sigma_t, x.dim()) * cond_grad\\n' \\\n",
    "'        elif guidance_type == \"classifier-free\":\\n' \\\n",
    "'            if guidance_scale == 1. or unconditional_condition is None:\\n' \\\n",
    "'                return noise_pred_fn(x, t_continuous, cond=condition)\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                x_in = torch.cat([x] * 2)\\n' \\\n",
    "'                t_in = torch.cat([t_continuous] * 2)\\n' \\\n",
    "'                c_in = torch.cat([unconditional_condition, condition])\\n' \\\n",
    "'                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\\n' \\\n",
    "'                return noise_uncond + guidance_scale * (noise - noise_uncond)\\n' \\\n",
    "'\\n' \\\n",
    "'    assert model_type in [\"noise\", \"x_start\", \"v\", \"score\"]\\n' \\\n",
    "'    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\\n' \\\n",
    "'    return model_fn\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'class DPM_Solver:\\n' \\\n",
    "'    def __init__(\\n' \\\n",
    "'        self,\\n' \\\n",
    "'        model_fn,\\n' \\\n",
    "'        noise_schedule,\\n' \\\n",
    "'        algorithm_type=\"dpmsolver++\",\\n' \\\n",
    "'        correcting_x0_fn=None,\\n' \\\n",
    "'        correcting_xt_fn=None,\\n' \\\n",
    "'        thresholding_max_val=1.,\\n' \\\n",
    "'        dynamic_thresholding_ratio=0.995,\\n' \\\n",
    "'    ):\\n' \\\n",
    "'        \"\"\"Construct a DPM-Solver.\\n' \\\n",
    "'\\n' \\\n",
    "'        We support both DPM-Solver (`algorithm_type=\"dpmsolver\"`) and DPM-Solver++ (`algorithm_type=\"dpmsolver++\"`).\\n' \\\n",
    "'\\n' \\\n",
    "'        We also support the \"dynamic thresholding\" method in Imagen[1]. For pixel-space diffusion models, you\\n' \\\n",
    "'        can set both `algorithm_type=\"dpmsolver++\"` and `correcting_x0_fn=\"dynamic_thresholding\"` to use the\\n' \\\n",
    "'        dynamic thresholding. The \"dynamic thresholding\" can greatly improve the sample quality for pixel-space\\n' \\\n",
    "'        DPMs with large guidance scales. Note that the thresholding method is **unsuitable** for latent-space\\n' \\\n",
    "'        DPMs (such as stable-diffusion).\\n' \\\n",
    "'\\n' \\\n",
    "'        To support advanced algorithms in image-to-image applications, we also support corrector functions for\\n' \\\n",
    "'        both x0 and xt.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            model_fn: A noise prediction model function which accepts the continuous-time input (t in [epsilon, T]):\\n' \\\n",
    "'                ``\\n' \\\n",
    "'                def model_fn(x, t_continuous):\\n' \\\n",
    "'                    return noise\\n' \\\n",
    "'                ``\\n' \\\n",
    "'                The shape of `x` is `(batch_size, **shape)`, and the shape of `t_continuous` is `(batch_size,)`.\\n' \\\n",
    "'            noise_schedule: A noise schedule object, such as NoiseScheduleVP.\\n' \\\n",
    "'            algorithm_type: A `str`. Either \"dpmsolver\" or \"dpmsolver++\".\\n' \\\n",
    "'            correcting_x0_fn: A `str` or a function with the following format:\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                def correcting_x0_fn(x0, t):\\n' \\\n",
    "'                    x0_new = ...\\n' \\\n",
    "'                    return x0_new\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                This function is to correct the outputs of the data prediction model at each sampling step. e.g.,\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                x0_pred = data_pred_model(xt, t)\\n' \\\n",
    "'                if correcting_x0_fn is not None:\\n' \\\n",
    "'                    x0_pred = correcting_x0_fn(x0_pred, t)\\n' \\\n",
    "'                xt_1 = update(x0_pred, xt, t)\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                If `correcting_x0_fn=\"dynamic_thresholding\"`, we use the dynamic thresholding proposed in Imagen[1].\\n' \\\n",
    "'            correcting_xt_fn: A function with the following format:\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                def correcting_xt_fn(xt, t, step):\\n' \\\n",
    "'                    x_new = ...\\n' \\\n",
    "'                    return x_new\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                This function is to correct the intermediate samples xt at each sampling step. e.g.,\\n' \\\n",
    "'                ```\\n' \\\n",
    "'                xt = ...\\n' \\\n",
    "'                xt = correcting_xt_fn(xt, t, step)\\n' \\\n",
    "'                ```\\n' \\\n",
    "'            thresholding_max_val: A `float`. The max value for thresholding.\\n' \\\n",
    "'                Valid only when use `dpmsolver++` and `correcting_x0_fn=\"dynamic_thresholding\"`.\\n' \\\n",
    "'            dynamic_thresholding_ratio: A `float`. The ratio for dynamic thresholding (see Imagen[1] for details).\\n' \\\n",
    "'                Valid only when use `dpmsolver++` and `correcting_x0_fn=\"dynamic_thresholding\"`.\\n' \\\n",
    "'\\n' \\\n",
    "'        [1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\\n' \\\n",
    "'            Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models\\n' \\\n",
    "'            with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        self.model = lambda x, t: model_fn(x, t.expand((x.shape[0])))\\n' \\\n",
    "'        self.noise_schedule = noise_schedule\\n' \\\n",
    "'        assert algorithm_type in [\"dpmsolver\", \"dpmsolver++\"]\\n' \\\n",
    "'        self.algorithm_type = algorithm_type\\n' \\\n",
    "'        if correcting_x0_fn == \"dynamic_thresholding\":\\n' \\\n",
    "'            self.correcting_x0_fn = self.dynamic_thresholding_fn\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            self.correcting_x0_fn = correcting_x0_fn\\n' \\\n",
    "'        self.correcting_xt_fn = correcting_xt_fn\\n' \\\n",
    "'        self.dynamic_thresholding_ratio = dynamic_thresholding_ratio\\n' \\\n",
    "'        self.thresholding_max_val = thresholding_max_val\\n' \\\n",
    "'        # Add attributes for saving intermediate images\\n' \\\n",
    "'        self.save_intermediate_enabled = False  # Enable/disable saving\\n' \\\n",
    "'        self.save_frequency = 10  # Save every 10 steps\\n' \\\n",
    "'        self.intermediate_dir = os.path.join(os.getcwd(), \"experiments\", \"celebahq\", \"intermediate_steps\")\\n' \\\n",
    "'        os.makedirs(self.intermediate_dir, exist_ok=True)\\n' \\\n",
    "'        #print(f\"Intermediate images will be saved to: {self.intermediate_dir}\")\\n' \\\n",
    "'        # Image counter to track the number of images processed\\n' \\\n",
    "'        self.image_counter = 0  # Reset counter at initialization\\n' \\\n",
    "'\\n' \\\n",
    "'    def save_intermediate(self, x, step, stage, device):\\n' \\\n",
    "'        if not self.save_intermediate_enabled:\\n' \\\n",
    "'            return\\n' \\\n",
    "'        if step % self.save_frequency == 0:  # Save only every save_frequency steps\\n' \\\n",
    "'            if x is None:\\n' \\\n",
    "'                print(f\"Warning: x is None at step {step}, stage {stage}\")\\n' \\\n",
    "'                return\\n' \\\n",
    "'            try:\\n' \\\n",
    "'                # Create a subdirectory for this image based on image_counter\\n' \\\n",
    "'                image_dir = os.path.join(self.intermediate_dir, f\"intermediate_steps_image_{self.image_counter}\")\\n' \\\n",
    "'                os.makedirs(image_dir, exist_ok=True)\\n' \\\n",
    "'                # If x is a batch, save each image in the batch separately\\n' \\\n",
    "'                if len(x.shape) == 4:  # Shape: (batch_size, channels, height, width)\\n' \\\n",
    "'                    batch_size = x.shape[0]\\n' \\\n",
    "'                    for batch_idx in range(batch_size):\\n' \\\n",
    "'                        x_single = x[batch_idx:batch_idx+1]  # Shape: (1, channels, height, width)\\n' \\\n",
    "'                        # Normalize x from [-1, 1] to [0, 1] for saving\\n' \\\n",
    "'                        x_normalized = (x + 1) / 2\\n' \\\n",
    "'                        filename = os.path.join(image_dir, f\"batch_{batch_idx}_step_{step:03d}_{stage}.png\")\\n' \\\n",
    "'                        tvu.save_image(x_normalized.to(device), filename, nrow=1)\\n' \\\n",
    "'                        # Get the line number of the calling code\\n' \\\n",
    "'                        frame = inspect.currentframe().f_back\\n' \\\n",
    "'                        line_number = frame.f_lineno\\n' \\\n",
    "'                        print(f\"Saved intermediate image: {filename} at line {line_number}\")\\n' \\\n",
    "'                        # Also log to a file\\n' \\\n",
    "'                        with open(os.path.join(self.intermediate_dir, \"save_log.txt\"), \"a\") as log_file:\\n' \\\n",
    "'                            log_file.write(f\"Saved intermediate image: {filename} at line {line_number}\\\\n\")\\n' \\\n",
    "'                else:\\n' \\\n",
    "'                    # Single image case (unlikely, but handle for completeness)\\n' \\\n",
    "'                    x_normalized = (x + 1) / 2\\n' \\\n",
    "'                    filename = os.path.join(image_dir, f\"step_{step:03d}_{stage}.png\")\\n' \\\n",
    "'                    tvu.save_image(x_normalized.to(device), filename, nrow=1)\\n' \\\n",
    "'                    frame = inspect.currentframe().f_back\\n' \\\n",
    "'                    line_number = frame.f_lineno\\n' \\\n",
    "'                    print(f\"Saved intermediate image: {filename} at line {line_number}\")\\n' \\\n",
    "'                    with open(os.path.join(image_dir, \"save_log.txt\"), \"a\") as log_file:\\n' \\\n",
    "'                        log_file.write(f\"Saved intermediate image: {filename} at line {line_number}\\\\n\")\\n' \\\n",
    "'            except Exception as e:\\n' \\\n",
    "'                print(f\"Failed to save image {filename}: {str(e)}\")\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'    def dynamic_thresholding_fn(self, x0, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        The dynamic thresholding method.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        dims = x0.dim()\\n' \\\n",
    "'        p = self.dynamic_thresholding_ratio\\n' \\\n",
    "'        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\\n' \\\n",
    "'        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\\n' \\\n",
    "'        x0 = torch.clamp(x0, -s, s) / s\\n' \\\n",
    "'        return x0\\n' \\\n",
    "'\\n' \\\n",
    "'    def noise_prediction_fn(self, x, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Return the noise prediction model.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        return self.model(x, t)\\n' \\\n",
    "'\\n' \\\n",
    "'    def data_prediction_fn(self, x, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Return the data prediction model (with corrector).\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        noise = self.noise_prediction_fn(x, t)\\n' \\\n",
    "'        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'        x0 = (x - sigma_t * noise) / alpha_t\\n' \\\n",
    "'        x0 = x0.clamp(-1, 1)\\n' \\\n",
    "'        if self.correcting_x0_fn is not None:\\n' \\\n",
    "'            x0 = self.correcting_x0_fn(x0, t)\\n' \\\n",
    "'        return x0\\n' \\\n",
    "'\\n' \\\n",
    "'    def model_fn(self, x, t):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Convert the model to the noise prediction model or the data prediction model.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if self.algorithm_type == \"dpmsolver++\":\\n' \\\n",
    "'            return self.data_prediction_fn(x, t)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            return self.noise_prediction_fn(x, t)\\n' \\\n",
    "'\\n' \\\n",
    "'    def get_time_steps(self, skip_type, t_T, t_0, N, device):\\n' \\\n",
    "'        \"\"\"Compute the intermediate time steps for sampling.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            skip_type: A `str`. The type for the spacing of the time steps. We support three types:\\n' \\\n",
    "'                - \\'logSNR\\': uniform logSNR for the time steps.\\n' \\\n",
    "'                - \\'time_uniform\\': uniform time for the time steps. (**Recommended for high-resolutional data**.)\\n' \\\n",
    "'                - \\'time_quadratic\\': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)\\n' \\\n",
    "'            t_T: A `float`. The starting time of the sampling (default is T).\\n' \\\n",
    "'            t_0: A `float`. The ending time of the sampling (default is epsilon).\\n' \\\n",
    "'            N: A `int`. The total number of the spacing of the time steps.\\n' \\\n",
    "'            device: A torch device.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            A pytorch tensor of the time steps, with the shape (N + 1,).\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if skip_type == \\'logSNR\\':\\n' \\\n",
    "'            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\\n' \\\n",
    "'            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\\n' \\\n",
    "'            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\\n' \\\n",
    "'            return self.noise_schedule.inverse_lambda(logSNR_steps)\\n' \\\n",
    "'        elif skip_type == \\'time_uniform\\':\\n' \\\n",
    "'            return torch.linspace(t_T, t_0, N + 1).to(device)\\n' \\\n",
    "'        elif skip_type == \\'time_quadratic\\':\\n' \\\n",
    "'            t_order = 2\\n' \\\n",
    "'            t = torch.linspace(t_T**(1. / t_order), t_0**(1. / t_order), N + 1).pow(t_order).to(device)\\n' \\\n",
    "'            return t\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            raise ValueError(\"Unsupported skip_type {}, need to be \\'logSNR\\' or \\'time_uniform\\' or \\'time_quadratic\\'\".format(skip_type))\\n' \\\n",
    "'\\n' \\\n",
    "'    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Get the order of each step for sampling by the singlestep DPM-Solver.\\n' \\\n",
    "'\\n' \\\n",
    "'        We combine both DPM-Solver-1,2,3 to use all the function evaluations, which is named as \"DPM-Solver-fast\".\\n' \\\n",
    "'        Given a fixed number of function evaluations by `steps`, the sampling procedure by DPM-Solver-fast is:\\n' \\\n",
    "'            - If order == 1:\\n' \\\n",
    "'                We take `steps` of DPM-Solver-1 (i.e. DDIM).\\n' \\\n",
    "'            - If order == 2:\\n' \\\n",
    "'                - Denote K = (steps // 2). We take K or (K + 1) intermediate time steps for sampling.\\n' \\\n",
    "'                - If steps % 2 == 0, we use K steps of DPM-Solver-2.\\n' \\\n",
    "'                - If steps % 2 == 1, we use K steps of DPM-Solver-2 and 1 step of DPM-Solver-1.\\n' \\\n",
    "'            - If order == 3:\\n' \\\n",
    "'                - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.\\n' \\\n",
    "'                - If steps % 3 == 0, we use (K - 2) steps of DPM-Solver-3, and 1 step of DPM-Solver-2 and 1 step of DPM-Solver-1.\\n' \\\n",
    "'                - If steps % 3 == 1, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-1.\\n' \\\n",
    "'                - If steps % 3 == 2, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-2.\\n' \\\n",
    "'\\n' \\\n",
    "'        ============================================\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            order: A `int`. The max order for the solver (2 or 3).\\n' \\\n",
    "'            steps: A `int`. The total number of function evaluations (NFE).\\n' \\\n",
    "'            skip_type: A `str`. The type for the spacing of the time steps. We support three types:\\n' \\\n",
    "'                - \\'logSNR\\': uniform logSNR for the time steps.\\n' \\\n",
    "'                - \\'time_uniform\\': uniform time for the time steps. (**Recommended for high-resolutional data**.)\\n' \\\n",
    "'                - \\'time_quadratic\\': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)\\n' \\\n",
    "'            t_T: A `float`. The starting time of the sampling (default is T).\\n' \\\n",
    "'            t_0: A `float`. The ending time of the sampling (default is epsilon).\\n' \\\n",
    "'            device: A torch device.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            orders: A list of the solver order of each step.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if order == 3:\\n' \\\n",
    "'            K = steps // 3 + 1\\n' \\\n",
    "'            if steps % 3 == 0:\\n' \\\n",
    "'                orders = [3,] * (K - 2) + [2, 1]\\n' \\\n",
    "'            elif steps % 3 == 1:\\n' \\\n",
    "'                orders = [3,] * (K - 1) + [1]\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                orders = [3,] * (K - 1) + [2]\\n' \\\n",
    "'        elif order == 2:\\n' \\\n",
    "'            if steps % 2 == 0:\\n' \\\n",
    "'                K = steps // 2\\n' \\\n",
    "'                orders = [2,] * K\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                K = steps // 2 + 1\\n' \\\n",
    "'                orders = [2,] * (K - 1) + [1]\\n' \\\n",
    "'        elif order == 1:\\n' \\\n",
    "'            K = steps\\n' \\\n",
    "'            orders = [1,] * steps\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            raise ValueError(\"\\'order\\' must be \\'1\\' or \\'2\\' or \\'3\\'.\")\\n' \\\n",
    "'        if skip_type == \\'logSNR\\':\\n' \\\n",
    "'            # To reproduce the results in DPM-Solver paper\\n' \\\n",
    "'            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]\\n' \\\n",
    "'        return timesteps_outer, orders\\n' \\\n",
    "'\\n' \\\n",
    "'    def denoise_to_zero_fn(self, x, s):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        return self.data_prediction_fn(x, s)\\n' \\\n",
    "'\\n' \\\n",
    "'    def dpm_solver_first_update(self, x, s, t, model_s=None, return_intermediate=False):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        DPM-Solver-1 (equivalent to DDIM) from time `s` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            s: A pytorch tensor. The starting time, with the shape (1,).\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            model_s: A pytorch tensor. The model function evaluated at time `s`.\\n' \\\n",
    "'                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\\n' \\\n",
    "'            return_intermediate: A `bool`. If true, also return the model value at time `s`.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        ns = self.noise_schedule\\n' \\\n",
    "'        dims = x.dim()\\n' \\\n",
    "'        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\\n' \\\n",
    "'        h = lambda_t - lambda_s\\n' \\\n",
    "'        log_alpha_s, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(t)\\n' \\\n",
    "'        sigma_s, sigma_t = ns.marginal_std(s), ns.marginal_std(t)\\n' \\\n",
    "'        alpha_t = torch.exp(log_alpha_t)\\n' \\\n",
    "'\\n' \\\n",
    "'        if self.algorithm_type == \"dpmsolver++\":\\n' \\\n",
    "'            phi_1 = torch.expm1(-h)\\n' \\\n",
    "'            if model_s is None:\\n' \\\n",
    "'                model_s = self.model_fn(x, s)\\n' \\\n",
    "'            x_t = (\\n' \\\n",
    "'                sigma_t / sigma_s * x\\n' \\\n",
    "'                - alpha_t * phi_1 * model_s\\n' \\\n",
    "'            )\\n' \\\n",
    "'            if return_intermediate:\\n' \\\n",
    "'                return x_t, {\\'model_s\\': model_s}\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                return x_t\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            phi_1 = torch.expm1(h)\\n' \\\n",
    "'            if model_s is None:\\n' \\\n",
    "'                model_s = self.model_fn(x, s)\\n' \\\n",
    "'            x_t = (\\n' \\\n",
    "'                torch.exp(log_alpha_t - log_alpha_s) * x\\n' \\\n",
    "'                - (sigma_t * phi_1) * model_s\\n' \\\n",
    "'            )\\n' \\\n",
    "'            if return_intermediate:\\n' \\\n",
    "'                return x_t, {\\'model_s\\': model_s}\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                return x_t\\n' \\\n",
    "'\\n' \\\n",
    "'    def singlestep_dpm_solver_second_update(self, x, s, t, r1=0.5, model_s=None, return_intermediate=False, solver_type=\\'dpmsolver\\'):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Singlestep solver DPM-Solver-2 from time `s` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            s: A pytorch tensor. The starting time, with the shape (1,).\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            r1: A `float`. The hyperparameter of the second-order solver.\\n' \\\n",
    "'            model_s: A pytorch tensor. The model function evaluated at time `s`.\\n' \\\n",
    "'                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\\n' \\\n",
    "'            return_intermediate: A `bool`. If true, also return the model value at time `s` and `s1` (the intermediate time).\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if solver_type not in [\\'dpmsolver\\', \\'taylor\\']:\\n' \\\n",
    "'            raise ValueError(\"\\'solver_type\\' must be either \\'dpmsolver\\' or \\'taylor\\', got {}\".format(solver_type))\\n' \\\n",
    "'        if r1 is None:\\n' \\\n",
    "'            r1 = 0.5\\n' \\\n",
    "'        ns = self.noise_schedule\\n' \\\n",
    "'        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\\n' \\\n",
    "'        h = lambda_t - lambda_s\\n' \\\n",
    "'        lambda_s1 = lambda_s + r1 * h\\n' \\\n",
    "'        s1 = ns.inverse_lambda(lambda_s1)\\n' \\\n",
    "'        log_alpha_s, log_alpha_s1, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(t)\\n' \\\n",
    "'        sigma_s, sigma_s1, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(t)\\n' \\\n",
    "'        alpha_s1, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_t)\\n' \\\n",
    "'\\n' \\\n",
    "'        if self.algorithm_type == \"dpmsolver++\":\\n' \\\n",
    "'            phi_11 = torch.expm1(-r1 * h)\\n' \\\n",
    "'            phi_1 = torch.expm1(-h)\\n' \\\n",
    "'\\n' \\\n",
    "'            if model_s is None:\\n' \\\n",
    "'                model_s = self.model_fn(x, s)\\n' \\\n",
    "'            x_s1 = (\\n' \\\n",
    "'                (sigma_s1 / sigma_s) * x\\n' \\\n",
    "'                - (alpha_s1 * phi_11) * model_s\\n' \\\n",
    "'            )\\n' \\\n",
    "'            model_s1 = self.model_fn(x_s1, s1)\\n' \\\n",
    "'            if solver_type == \\'dpmsolver\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (sigma_t / sigma_s) * x\\n' \\\n",
    "'                    - (alpha_t * phi_1) * model_s\\n' \\\n",
    "'                    - (0.5 / r1) * (alpha_t * phi_1) * (model_s1 - model_s)\\n' \\\n",
    "'                )\\n' \\\n",
    "'            elif solver_type == \\'taylor\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (sigma_t / sigma_s) * x\\n' \\\n",
    "'                    - (alpha_t * phi_1) * model_s\\n' \\\n",
    "'                    + (1. / r1) * (alpha_t * (phi_1 / h + 1.)) * (model_s1 - model_s)\\n' \\\n",
    "'                )\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            phi_11 = torch.expm1(r1 * h)\\n' \\\n",
    "'            phi_1 = torch.expm1(h)\\n' \\\n",
    "'\\n' \\\n",
    "'            if model_s is None:\\n' \\\n",
    "'                model_s = self.model_fn(x, s)\\n' \\\n",
    "'            x_s1 = (\\n' \\\n",
    "'                torch.exp(log_alpha_s1 - log_alpha_s) * x\\n' \\\n",
    "'                - (sigma_s1 * phi_11) * model_s\\n' \\\n",
    "'            )\\n' \\\n",
    "'            model_s1 = self.model_fn(x_s1, s1)\\n' \\\n",
    "'            if solver_type == \\'dpmsolver\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    torch.exp(log_alpha_t - log_alpha_s) * x\\n' \\\n",
    "'                    - (sigma_t * phi_1) * model_s\\n' \\\n",
    "'                    - (0.5 / r1) * (sigma_t * phi_1) * (model_s1 - model_s)\\n' \\\n",
    "'                )\\n' \\\n",
    "'            elif solver_type == \\'taylor\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    torch.exp(log_alpha_t - log_alpha_s) * x\\n' \\\n",
    "'                    - (sigma_t * phi_1) * model_s\\n' \\\n",
    "'                    - (1. / r1) * (sigma_t * (phi_1 / h - 1.)) * (model_s1 - model_s)\\n' \\\n",
    "'                )\\n' \\\n",
    "'        if return_intermediate:\\n' \\\n",
    "'            return x_t, {\\'model_s\\': model_s, \\'model_s1\\': model_s1}\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            return x_t\\n' \\\n",
    "'\\n' \\\n",
    "'    def singlestep_dpm_solver_third_update(self, x, s, t, r1=1./3., r2=2./3., model_s=None, model_s1=None, return_intermediate=False, solver_type=\\'dpmsolver\\'):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Singlestep solver DPM-Solver-3 from time `s` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            s: A pytorch tensor. The starting time, with the shape (1,).\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            r1: A `float`. The hyperparameter of the third-order solver.\\n' \\\n",
    "'            r2: A `float`. The hyperparameter of the third-order solver.\\n' \\\n",
    "'            model_s: A pytorch tensor. The model function evaluated at time `s`.\\n' \\\n",
    "'                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.\\n' \\\n",
    "'            model_s1: A pytorch tensor. The model function evaluated at time `s1` (the intermediate time given by `r1`).\\n' \\\n",
    "'                If `model_s1` is None, we evaluate the model at `s1`; otherwise we directly use it.\\n' \\\n",
    "'            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if solver_type not in [\\'dpmsolver\\', \\'taylor\\']:\\n' \\\n",
    "'            raise ValueError(\"\\'solver_type\\' must be either \\'dpmsolver\\' or \\'taylor\\', got {}\".format(solver_type))\\n' \\\n",
    "'        if r1 is None:\\n' \\\n",
    "'            r1 = 1. / 3.\\n' \\\n",
    "'        if r2 is None:\\n' \\\n",
    "'            r2 = 2. / 3.\\n' \\\n",
    "'        ns = self.noise_schedule\\n' \\\n",
    "'        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)\\n' \\\n",
    "'        h = lambda_t - lambda_s\\n' \\\n",
    "'        lambda_s1 = lambda_s + r1 * h\\n' \\\n",
    "'        lambda_s2 = lambda_s + r2 * h\\n' \\\n",
    "'        s1 = ns.inverse_lambda(lambda_s1)\\n' \\\n",
    "'        s2 = ns.inverse_lambda(lambda_s2)\\n' \\\n",
    "'        log_alpha_s, log_alpha_s1, log_alpha_s2, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(s2), ns.marginal_log_mean_coeff(t)\\n' \\\n",
    "'        sigma_s, sigma_s1, sigma_s2, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(s2), ns.marginal_std(t)\\n' \\\n",
    "'        alpha_s1, alpha_s2, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_s2), torch.exp(log_alpha_t)\\n' \\\n",
    "'\\n' \\\n",
    "'        if self.algorithm_type == \"dpmsolver++\":\\n' \\\n",
    "'            phi_11 = torch.expm1(-r1 * h)\\n' \\\n",
    "'            phi_12 = torch.expm1(-r2 * h)\\n' \\\n",
    "'            phi_1 = torch.expm1(-h)\\n' \\\n",
    "'            phi_22 = torch.expm1(-r2 * h) / (r2 * h) + 1.\\n' \\\n",
    "'            phi_2 = phi_1 / h + 1.\\n' \\\n",
    "'            phi_3 = phi_2 / h - 0.5\\n' \\\n",
    "'\\n' \\\n",
    "'            if model_s is None:\\n' \\\n",
    "'                model_s = self.model_fn(x, s)\\n' \\\n",
    "'            if model_s1 is None:\\n' \\\n",
    "'                x_s1 = (\\n' \\\n",
    "'                    (sigma_s1 / sigma_s) * x\\n' \\\n",
    "'                    - (alpha_s1 * phi_11) * model_s\\n' \\\n",
    "'                )\\n' \\\n",
    "'                model_s1 = self.model_fn(x_s1, s1)\\n' \\\n",
    "'            x_s2 = (\\n' \\\n",
    "'                (sigma_s2 / sigma_s) * x\\n' \\\n",
    "'                - (alpha_s2 * phi_12) * model_s\\n' \\\n",
    "'                + r2 / r1 * (alpha_s2 * phi_22) * (model_s1 - model_s)\\n' \\\n",
    "'            )\\n' \\\n",
    "'            model_s2 = self.model_fn(x_s2, s2)\\n' \\\n",
    "'            if solver_type == \\'dpmsolver\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (sigma_t / sigma_s) * x\\n' \\\n",
    "'                    - (alpha_t * phi_1) * model_s\\n' \\\n",
    "'                    + (1. / r2) * (alpha_t * phi_2) * (model_s2 - model_s)\\n' \\\n",
    "'                )\\n' \\\n",
    "'            elif solver_type == \\'taylor\\':\\n' \\\n",
    "'                D1_0 = (1. / r1) * (model_s1 - model_s)\\n' \\\n",
    "'                D1_1 = (1. / r2) * (model_s2 - model_s)\\n' \\\n",
    "'                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\\n' \\\n",
    "'                D2 = 2. * (D1_1 - D1_0) / (r2 - r1)\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (sigma_t / sigma_s) * x\\n' \\\n",
    "'                    - (alpha_t * phi_1) * model_s\\n' \\\n",
    "'                    + (alpha_t * phi_2) * D1\\n' \\\n",
    "'                    - (alpha_t * phi_3) * D2\\n' \\\n",
    "'                )\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            phi_11 = torch.expm1(r1 * h)\\n' \\\n",
    "'            phi_12 = torch.expm1(r2 * h)\\n' \\\n",
    "'            phi_1 = torch.expm1(h)\\n' \\\n",
    "'            phi_22 = torch.expm1(r2 * h) / (r2 * h) - 1.\\n' \\\n",
    "'            phi_2 = phi_1 / h - 1.\\n' \\\n",
    "'            phi_3 = phi_2 / h - 0.5\\n' \\\n",
    "'\\n' \\\n",
    "'            if model_s is None:\\n' \\\n",
    "'                model_s = self.model_fn(x, s)\\n' \\\n",
    "'            if model_s1 is None:\\n' \\\n",
    "'                x_s1 = (\\n' \\\n",
    "'                    (torch.exp(log_alpha_s1 - log_alpha_s)) * x\\n' \\\n",
    "'                    - (sigma_s1 * phi_11) * model_s\\n' \\\n",
    "'                )\\n' \\\n",
    "'                model_s1 = self.model_fn(x_s1, s1)\\n' \\\n",
    "'            x_s2 = (\\n' \\\n",
    "'                (torch.exp(log_alpha_s2 - log_alpha_s)) * x\\n' \\\n",
    "'                - (sigma_s2 * phi_12) * model_s\\n' \\\n",
    "'                - r2 / r1 * (sigma_s2 * phi_22) * (model_s1 - model_s)\\n' \\\n",
    "'            )\\n' \\\n",
    "'            model_s2 = self.model_fn(x_s2, s2)\\n' \\\n",
    "'            if solver_type == \\'dpmsolver\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (torch.exp(log_alpha_t - log_alpha_s)) * x\\n' \\\n",
    "'                    - (sigma_t * phi_1) * model_s\\n' \\\n",
    "'                    - (1. / r2) * (sigma_t * phi_2) * (model_s2 - model_s)\\n' \\\n",
    "'                )\\n' \\\n",
    "'            elif solver_type == \\'taylor\\':\\n' \\\n",
    "'                D1_0 = (1. / r1) * (model_s1 - model_s)\\n' \\\n",
    "'                D1_1 = (1. / r2) * (model_s2 - model_s)\\n' \\\n",
    "'                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)\\n' \\\n",
    "'                D2 = 2. * (D1_1 - D1_0) / (r2 - r1)\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (torch.exp(log_alpha_t - log_alpha_s)) * x\\n' \\\n",
    "'                    - (sigma_t * phi_1) * model_s\\n' \\\n",
    "'                    - (sigma_t * phi_2) * D1\\n' \\\n",
    "'                    - (sigma_t * phi_3) * D2\\n' \\\n",
    "'                )\\n' \\\n",
    "'\\n' \\\n",
    "'        if return_intermediate:\\n' \\\n",
    "'            return x_t, {\\'model_s\\': model_s, \\'model_s1\\': model_s1, \\'model_s2\\': model_s2}\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            return x_t\\n' \\\n",
    "'\\n' \\\n",
    "'    def multistep_dpm_solver_second_update(self, x, model_prev_list, t_prev_list, t, solver_type=\"dpmsolver\"):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Multistep solver DPM-Solver-2 from time `t_prev_list[-1]` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            model_prev_list: A list of pytorch tensor. The previous computed model values.\\n' \\\n",
    "'            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if solver_type not in [\\'dpmsolver\\', \\'taylor\\']:\\n' \\\n",
    "'            raise ValueError(\"\\'solver_type\\' must be either \\'dpmsolver\\' or \\'taylor\\', got {}\".format(solver_type))\\n' \\\n",
    "'        ns = self.noise_schedule\\n' \\\n",
    "'        model_prev_1, model_prev_0 = model_prev_list[-2], model_prev_list[-1]\\n' \\\n",
    "'        t_prev_1, t_prev_0 = t_prev_list[-2], t_prev_list[-1]\\n' \\\n",
    "'        lambda_prev_1, lambda_prev_0, lambda_t = ns.marginal_lambda(t_prev_1), ns.marginal_lambda(t_prev_0), ns.marginal_lambda(t)\\n' \\\n",
    "'        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\\n' \\\n",
    "'        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\\n' \\\n",
    "'        alpha_t = torch.exp(log_alpha_t)\\n' \\\n",
    "'\\n' \\\n",
    "'        h_0 = lambda_prev_0 - lambda_prev_1\\n' \\\n",
    "'        h = lambda_t - lambda_prev_0\\n' \\\n",
    "'        r0 = h_0 / h\\n' \\\n",
    "'        D1_0 = (1. / r0) * (model_prev_0 - model_prev_1)\\n' \\\n",
    "'        if self.algorithm_type == \"dpmsolver++\":\\n' \\\n",
    "'            phi_1 = torch.expm1(-h)\\n' \\\n",
    "'            if solver_type == \\'dpmsolver\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (sigma_t / sigma_prev_0) * x\\n' \\\n",
    "'                    - (alpha_t * phi_1) * model_prev_0\\n' \\\n",
    "'                    - 0.5 * (alpha_t * phi_1) * D1_0\\n' \\\n",
    "'                )\\n' \\\n",
    "'            elif solver_type == \\'taylor\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (sigma_t / sigma_prev_0) * x\\n' \\\n",
    "'                    - (alpha_t * phi_1) * model_prev_0\\n' \\\n",
    "'                    + (alpha_t * (phi_1 / h + 1.)) * D1_0\\n' \\\n",
    "'                )\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            phi_1 = torch.expm1(h)\\n' \\\n",
    "'            if solver_type == \\'dpmsolver\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\\n' \\\n",
    "'                    - (sigma_t * phi_1) * model_prev_0\\n' \\\n",
    "'                    - 0.5 * (sigma_t * phi_1) * D1_0\\n' \\\n",
    "'                )\\n' \\\n",
    "'            elif solver_type == \\'taylor\\':\\n' \\\n",
    "'                x_t = (\\n' \\\n",
    "'                    (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\\n' \\\n",
    "'                    - (sigma_t * phi_1) * model_prev_0\\n' \\\n",
    "'                    - (sigma_t * (phi_1 / h - 1.)) * D1_0\\n' \\\n",
    "'                )\\n' \\\n",
    "'        return x_t\\n' \\\n",
    "'\\n' \\\n",
    "'    def multistep_dpm_solver_third_update(self, x, model_prev_list, t_prev_list, t, solver_type=\\'dpmsolver\\'):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Multistep solver DPM-Solver-3 from time `t_prev_list[-1]` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            model_prev_list: A list of pytorch tensor. The previous computed model values.\\n' \\\n",
    "'            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        ns = self.noise_schedule\\n' \\\n",
    "'        model_prev_2, model_prev_1, model_prev_0 = model_prev_list\\n' \\\n",
    "'        t_prev_2, t_prev_1, t_prev_0 = t_prev_list\\n' \\\n",
    "'        lambda_prev_2, lambda_prev_1, lambda_prev_0, lambda_t = ns.marginal_lambda(t_prev_2), ns.marginal_lambda(t_prev_1), ns.marginal_lambda(t_prev_0), ns.marginal_lambda(t)\\n' \\\n",
    "'        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\\n' \\\n",
    "'        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\\n' \\\n",
    "'        alpha_t = torch.exp(log_alpha_t)\\n' \\\n",
    "'\\n' \\\n",
    "'        h_1 = lambda_prev_1 - lambda_prev_2\\n' \\\n",
    "'        h_0 = lambda_prev_0 - lambda_prev_1\\n' \\\n",
    "'        h = lambda_t - lambda_prev_0\\n' \\\n",
    "'        r0, r1 = h_0 / h, h_1 / h\\n' \\\n",
    "'        D1_0 = (1. / r0) * (model_prev_0 - model_prev_1)\\n' \\\n",
    "'        D1_1 = (1. / r1) * (model_prev_1 - model_prev_2)\\n' \\\n",
    "'        D1 = D1_0 + (r0 / (r0 + r1)) * (D1_0 - D1_1)\\n' \\\n",
    "'        D2 = (1. / (r0 + r1)) * (D1_0 - D1_1)\\n' \\\n",
    "'        if self.algorithm_type == \"dpmsolver++\":\\n' \\\n",
    "'            phi_1 = torch.expm1(-h)\\n' \\\n",
    "'            phi_2 = phi_1 / h + 1.\\n' \\\n",
    "'            phi_3 = phi_2 / h - 0.5\\n' \\\n",
    "'            x_t = (\\n' \\\n",
    "'                (sigma_t / sigma_prev_0) * x\\n' \\\n",
    "'                - (alpha_t * phi_1) * model_prev_0\\n' \\\n",
    "'                + (alpha_t * phi_2) * D1\\n' \\\n",
    "'                - (alpha_t * phi_3) * D2\\n' \\\n",
    "'            )\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            phi_1 = torch.expm1(h)\\n' \\\n",
    "'            phi_2 = phi_1 / h - 1.\\n' \\\n",
    "'            phi_3 = phi_2 / h - 0.5\\n' \\\n",
    "'            x_t = (\\n' \\\n",
    "'                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\\n' \\\n",
    "'                - (sigma_t * phi_1) * model_prev_0\\n' \\\n",
    "'                - (sigma_t * phi_2) * D1\\n' \\\n",
    "'                - (sigma_t * phi_3) * D2\\n' \\\n",
    "'            )\\n' \\\n",
    "'        return x_t\\n' \\\n",
    "'\\n' \\\n",
    "'    def singlestep_dpm_solver_update(self, x, s, t, order, return_intermediate=False, solver_type=\\'dpmsolver\\', r1=None, r2=None):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Singlestep DPM-Solver with the order `order` from time `s` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            s: A pytorch tensor. The starting time, with the shape (1,).\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.\\n' \\\n",
    "'            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'            r1: A `float`. The hyperparameter of the second-order or third-order solver.\\n' \\\n",
    "'            r2: A `float`. The hyperparameter of the third-order solver.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if order == 1:\\n' \\\n",
    "'            return self.dpm_solver_first_update(x, s, t, return_intermediate=return_intermediate)\\n' \\\n",
    "'        elif order == 2:\\n' \\\n",
    "'            return self.singlestep_dpm_solver_second_update(x, s, t, return_intermediate=return_intermediate, solver_type=solver_type, r1=r1)\\n' \\\n",
    "'        elif order == 3:\\n' \\\n",
    "'            return self.singlestep_dpm_solver_third_update(x, s, t, return_intermediate=return_intermediate, solver_type=solver_type, r1=r1, r2=r2)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\\n' \\\n",
    "'\\n' \\\n",
    "'    def multistep_dpm_solver_update(self, x, model_prev_list, t_prev_list, t, order, solver_type=\\'dpmsolver\\'):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Multistep DPM-Solver with the order `order` from time `t_prev_list[-1]` to time `t`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `s`.\\n' \\\n",
    "'            model_prev_list: A list of pytorch tensor. The previous computed model values.\\n' \\\n",
    "'            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)\\n' \\\n",
    "'            t: A pytorch tensor. The ending time, with the shape (1,).\\n' \\\n",
    "'            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_t: A pytorch tensor. The approximated solution at time `t`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        if order == 1:\\n' \\\n",
    "'            return self.dpm_solver_first_update(x, t_prev_list[-1], t, model_s=model_prev_list[-1])\\n' \\\n",
    "'        elif order == 2:\\n' \\\n",
    "'            return self.multistep_dpm_solver_second_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)\\n' \\\n",
    "'        elif order == 3:\\n' \\\n",
    "'            return self.multistep_dpm_solver_third_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            raise ValueError(\"Solver order must be 1 or 2 or 3, got {}\".format(order))\\n' \\\n",
    "'\\n' \\\n",
    "'    def dpm_solver_adaptive(self, x, order, t_T, t_0, h_init=0.05, atol=0.0078, rtol=0.05, theta=0.9, t_err=1e-5, solver_type=\\'dpmsolver\\'):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        The adaptive step size solver based on singlestep DPM-Solver.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `t_T`.\\n' \\\n",
    "'            order: A `int`. The (higher) order of the solver. We only support order == 2 or 3.\\n' \\\n",
    "'            t_T: A `float`. The starting time of the sampling (default is T).\\n' \\\n",
    "'            t_0: A `float`. The ending time of the sampling (default is epsilon).\\n' \\\n",
    "'            h_init: A `float`. The initial step size (for logSNR).\\n' \\\n",
    "'            atol: A `float`. The absolute tolerance of the solver. For image data, the default setting is 0.0078, followed [1].\\n' \\\n",
    "'            rtol: A `float`. The relative tolerance of the solver. The default setting is 0.05.\\n' \\\n",
    "'            theta: A `float`. The safety hyperparameter for adapting the step size. The default setting is 0.9, followed [1].\\n' \\\n",
    "'            t_err: A `float`. The tolerance for the time. We solve the diffusion ODE until the absolute error between the\\n' \\\n",
    "'                current time and `t_0` is less than `t_err`. The default setting is 1e-5.\\n' \\\n",
    "'            solver_type: either \\'dpmsolver\\' or \\'taylor\\'. The type for the high-order solvers.\\n' \\\n",
    "'                The type slightly impacts the performance. We recommend to use \\'dpmsolver\\' type.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_0: A pytorch tensor. The approximated solution at time `t_0`.\\n' \\\n",
    "'\\n' \\\n",
    "'        [1] A. Jolicoeur-Martineau, K. Li, R. PichÃ©-Taillefer, T. Kachman, and I. Mitliagkas, \"Gotta go fast when generating data with score-based models,\" arXiv preprint arXiv:2105.14080, 2021.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        ns = self.noise_schedule\\n' \\\n",
    "'        s = t_T * torch.ones((1,)).to(x)\\n' \\\n",
    "'        lambda_s = ns.marginal_lambda(s)\\n' \\\n",
    "'        lambda_0 = ns.marginal_lambda(t_0 * torch.ones_like(s).to(x))\\n' \\\n",
    "'        h = h_init * torch.ones_like(s).to(x)\\n' \\\n",
    "'        x_prev = x\\n' \\\n",
    "'        nfe = 0\\n' \\\n",
    "'        if order == 2:\\n' \\\n",
    "'            r1 = 0.5\\n' \\\n",
    "'            lower_update = lambda x, s, t: self.dpm_solver_first_update(x, s, t, return_intermediate=True)\\n' \\\n",
    "'            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, solver_type=solver_type, **kwargs)\\n' \\\n",
    "'        elif order == 3:\\n' \\\n",
    "'            r1, r2 = 1. / 3., 2. / 3.\\n' \\\n",
    "'            lower_update = lambda x, s, t: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, return_intermediate=True, solver_type=solver_type)\\n' \\\n",
    "'            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_third_update(x, s, t, r1=r1, r2=r2, solver_type=solver_type, **kwargs)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            raise ValueError(\"For adaptive step size solver, order must be 2 or 3, got {}\".format(order))\\n' \\\n",
    "'        while torch.abs((s - t_0)).mean() > t_err:\\n' \\\n",
    "'            t = ns.inverse_lambda(lambda_s + h)\\n' \\\n",
    "'            x_lower, lower_noise_kwargs = lower_update(x, s, t)\\n' \\\n",
    "'            x_higher = higher_update(x, s, t, **lower_noise_kwargs)\\n' \\\n",
    "'            delta = torch.max(torch.ones_like(x).to(x) * atol, rtol * torch.max(torch.abs(x_lower), torch.abs(x_prev)))\\n' \\\n",
    "'            norm_fn = lambda v: torch.sqrt(torch.square(v.reshape((v.shape[0], -1))).mean(dim=-1, keepdim=True))\\n' \\\n",
    "'            E = norm_fn((x_higher - x_lower) / delta).max()\\n' \\\n",
    "'            if torch.all(E <= 1.):\\n' \\\n",
    "'                x = x_higher\\n' \\\n",
    "'                s = t\\n' \\\n",
    "'                x_prev = x_lower\\n' \\\n",
    "'                lambda_s = ns.marginal_lambda(s)\\n' \\\n",
    "'            h = torch.min(theta * h * torch.float_power(E, -1. / order).float(), lambda_0 - lambda_s)\\n' \\\n",
    "'            nfe += order\\n' \\\n",
    "'        print(\\'adaptive solver nfe\\', nfe)\\n' \\\n",
    "'        return x\\n' \\\n",
    "'\\n' \\\n",
    "'    def add_noise(self, x, t, noise=None):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute the noised input xt = alpha_t * x + sigma_t * noise.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A `torch.Tensor` with shape `(batch_size, *shape)`.\\n' \\\n",
    "'            t: A `torch.Tensor` with shape `(t_size,)`.\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            xt with shape `(t_size, batch_size, *shape)`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'        if noise is None:\\n' \\\n",
    "'            noise = torch.randn((t.shape[0], *x.shape), device=x.device)\\n' \\\n",
    "'        x = x.reshape((-1, *x.shape))\\n' \\\n",
    "'        xt = expand_dims(alpha_t, x.dim()) * x + expand_dims(sigma_t, x.dim()) * noise\\n' \\\n",
    "'        if t.shape[0] == 1:\\n' \\\n",
    "'            return xt.squeeze(0)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            return xt\\n' \\\n",
    "'\\n' \\\n",
    "'    def inverse(self, x, steps=20, t_start=None, t_end=None, order=2, skip_type=\\'time_uniform\\',\\n' \\\n",
    "'        method=\\'multistep\\', lower_order_final=True, denoise_to_zero=False, solver_type=\\'dpmsolver\\',\\n' \\\n",
    "'        atol=0.0078, rtol=0.05, return_intermediate=False,\\n' \\\n",
    "'    ):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Inverse the sample `x` from time `t_start` to `t_end` by DPM-Solver.\\n' \\\n",
    "'        For discrete-time DPMs, we use `t_start=1/N`, where `N` is the total time steps during training.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        t_0 = 1. / self.noise_schedule.total_N if t_start is None else t_start\\n' \\\n",
    "'        t_T = self.noise_schedule.T if t_end is None else t_end\\n' \\\n",
    "'        assert t_0 > 0 and t_T > 0, \"Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array\"\\n' \\\n",
    "'        return self.sample(x, steps=steps, t_start=t_0, t_end=t_T, order=order, skip_type=skip_type,\\n' \\\n",
    "'            method=method, lower_order_final=lower_order_final, denoise_to_zero=denoise_to_zero, solver_type=solver_type,\\n' \\\n",
    "'            atol=atol, rtol=rtol, return_intermediate=return_intermediate)\\n' \\\n",
    "'\\n' \\\n",
    "'    def sample(self, x, steps=20, t_start=None, t_end=None, order=2, skip_type=\"time_uniform\",\\n' \\\n",
    "'            method=\"multistep\", lower_order_final=True, denoise_to_zero=False, solver_type=\"dpmsolver\",\\n' \\\n",
    "'            atol=0.0078, rtol=0.05, return_intermediate=False, model_kwargs=None, model_mask_kwargs=None, inpa_inj_sched_prev=False,inpa_inj_sched_prev_cumnoise=False):\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        Compute the sample at time `t_end` by DPM-Solver, given the initial `x` at time `t_start`.\\n' \\\n",
    "'\\n' \\\n",
    "'        Args:\\n' \\\n",
    "'            x: A pytorch tensor. The initial value at time `t_start`.\\n' \\\n",
    "'            steps: A `int`. The total number of function evaluations (NFE).\\n' \\\n",
    "'            t_start: A `float`. The starting time of the sampling.\\n' \\\n",
    "'            t_end: A `float`. The ending time of the sampling.\\n' \\\n",
    "'            order: A `int`. The order of DPM-Solver.\\n' \\\n",
    "'            skip_type: A `str`. The type for the spacing of the time steps.\\n' \\\n",
    "'            method: A `str`. The method for sampling.\\n' \\\n",
    "'            denoise_to_zero: A `bool`. Whether to denoise to time 0 at the final step.\\n' \\\n",
    "'            lower_order_final: A `bool`. Whether to use lower order solvers at the final steps.\\n' \\\n",
    "'            solver_type: A `str`. The taylor expansion type for the solver.\\n' \\\n",
    "'            atol: A `float`. The absolute tolerance of the adaptive step size solver.\\n' \\\n",
    "'            rtol: A `float`. The relative tolerance of the adaptive step size solver.\\n' \\\n",
    "'            return_intermediate: A `bool`. Whether to save the xt at each step.\\n' \\\n",
    "'            model_kwargs: A `dict`. Additional arguments for the model, including inpainting parameters.\\n' \\\n",
    "'\\n' \\\n",
    "'        Returns:\\n' \\\n",
    "'            x_end: A pytorch tensor. The approximated solution at time `t_end`.\\n' \\\n",
    "'        \"\"\"\\n' \\\n",
    "'        t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\\n' \\\n",
    "'        t_T = self.noise_schedule.T if t_start is None else t_start\\n' \\\n",
    "'        assert t_0 > 0 and t_T > 0, \"Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array\"\\n' \\\n",
    "'        if return_intermediate:\\n' \\\n",
    "'            assert method in [\"multistep\", \"singlestep\", \"singlestep_fixed\"], \"Cannot use adaptive solver when saving intermediate values\"\\n' \\\n",
    "'        if self.correcting_xt_fn is not None:\\n' \\\n",
    "'            assert method in [\"multistep\", \"singlestep\", \"singlestep_fixed\"], \"Cannot use adaptive solver when correcting_xt_fn is not None\"\\n' \\\n",
    "'        device = x.device\\n' \\\n",
    "'        intermediates = []\\n' \\\n",
    "'        with torch.no_grad():\\n' \\\n",
    "'            if method == \"adaptive\":\\n' \\\n",
    "'                if inpa_inj_sched_prev and model_mask_kwargs is not None:\\n' \\\n",
    "'                    t_T_tensor = torch.tensor([t_T], device=device, dtype=torch.float16)\\n' \\\n",
    "'                    #print(f\"Initial inpainting for adaptive at t={t_T_tensor.item()}\")\\n' \\\n",
    "'                    gt_keep_mask = torch.zeros(x.shape, device=device)\\n' \\\n",
    "'                    gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) > 0.] = 1\\n' \\\n",
    "'                    gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) < 0.] = 0\\n' \\\n",
    "'                    gt = model_kwargs.get(\\'ref_img\\', torch.zeros_like(x)).to(device)\\n' \\\n",
    "'                    alpha_t = self.noise_schedule.marginal_alpha(t_T_tensor)\\n' \\\n",
    "'                    sigma_t = self.noise_schedule.marginal_std(t_T_tensor)\\n' \\\n",
    "'                    if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                        weighed_gt = self.get_gt_noised(gt, t_T_tensor)\\n' \\\n",
    "'                    else:\\n' \\\n",
    "'                        gt_part = alpha_t * gt\\n' \\\n",
    "'                        noise_part = sigma_t * torch.randn_like(x)\\n' \\\n",
    "'                        weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                    x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                    #self.save_intermediate(x, 0, \"after_initial_inpainting\", device)\\n' \\\n",
    "'                x = self.dpm_solver_adaptive(x, order=order, t_T=t_T, t_0=t_0, atol=atol, rtol=rtol, solver_type=solver_type)\\n' \\\n",
    "'                if self.correcting_xt_fn is not None:\\n' \\\n",
    "'                    x = self.correcting_xt_fn(x, t_0, 0)\\n' \\\n",
    "'                if return_intermediate:\\n' \\\n",
    "'                    intermediates.append(x)\\n' \\\n",
    "'            elif method == \"multistep\":\\n' \\\n",
    "'                assert steps >= order\\n' \\\n",
    "'                timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\\n' \\\n",
    "'                assert timesteps.shape[0] - 1 == steps\\n' \\\n",
    "'                # Init the initial values.\\n' \\\n",
    "'                step = 0\\n' \\\n",
    "'                t = timesteps[step]\\n' \\\n",
    "'                t_prev_list = [t]\\n' \\\n",
    "'                #inital step \\n' \\\n",
    "'                if inpa_inj_sched_prev and model_mask_kwargs is not None:\\n' \\\n",
    "'                    gt_keep_mask = torch.zeros(x.shape, device=x.device)\\n' \\\n",
    "'                    gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) > 0.] = 1\\n' \\\n",
    "'                    gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) < 0.] = 0\\n' \\\n",
    "'                    gt = model_kwargs[\"ref_img\"].to(device)\\n' \\\n",
    "'                    alpha_t = self.noise_schedule.marginal_alpha(t)\\n' \\\n",
    "'                    sigma_t = self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'                    if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                        weighed_gt = self.get_gt_noised(gt, t)\\n' \\\n",
    "'                    else:\\n' \\\n",
    "'                         gt_part = alpha_t * gt\\n' \\\n",
    "'                         noise_part = sigma_t * torch.randn_like(x)\\n' \\\n",
    "'                         weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                    x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                    #self.save_intermediate(x, step, \"after_inpainting\", device)\\n' \\\n",
    "'                model_prev_list = [self.model_fn(x, t)]\\n' \\\n",
    "'                if self.correcting_xt_fn is not None:\\n' \\\n",
    "'                    x = self.correcting_xt_fn(x, t, step)\\n' \\\n",
    "'                if return_intermediate:\\n' \\\n",
    "'                    intermediates.append(x)\\n' \\\n",
    "'                # Init the first `order` values by lower order multistep DPM-Solver.\\n' \\\n",
    "'                for step in range(1, order):\\n' \\\n",
    "'                    t = timesteps[step]\\n' \\\n",
    "'                    # Inpainting logic\\n' \\\n",
    "'                    if inpa_inj_sched_prev and model_mask_kwargs is not None:\\n' \\\n",
    "'                        gt_keep_mask = torch.zeros(x.shape, device=x.device)\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) > 0.] = 1\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) < 0.] = 0\\n' \\\n",
    "'                        gt = model_kwargs[\"ref_img\"].to(device)\\n' \\\n",
    "'                        alpha_t = self.noise_schedule.marginal_alpha(t)\\n' \\\n",
    "'                        sigma_t = self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'                        if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                            # Assume get_gt_noised is implemented elsewhere to add cumulative noise\\n' \\\n",
    "'                            weighed_gt = self.get_gt_noised(gt, t)\\n' \\\n",
    "'                        else:\\n' \\\n",
    "'                             gt_weight = alpha_t\\n' \\\n",
    "'                             gt_part = gt_weight * gt\\n' \\\n",
    "'                             noise_weight = sigma_t\\n' \\\n",
    "'                             noise_part = noise_weight * torch.randn_like(x)\\n' \\\n",
    "'                             weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                        x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                    x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step, solver_type=solver_type)\\n' \\\n",
    "'                    #self.save_intermediate(x, step, \"after_inpainting\", device)\\n' \\\n",
    "'                    if self.correcting_xt_fn is not None:\\n' \\\n",
    "'                        x = self.correcting_xt_fn(x, t, step)\\n' \\\n",
    "'                    if return_intermediate:\\n' \\\n",
    "'                        intermediates.append(x)\\n' \\\n",
    "'                    model_prev_list.append(self.model_fn(x, t))\\n' \\\n",
    "'                    t_prev_list.append(t)\\n' \\\n",
    "'                # Compute the remaining values by `order`-th order multistep DPM-Solver.\\n' \\\n",
    "'                for step in range(order, steps + 1):\\n' \\\n",
    "'                    t = timesteps[step]\\n' \\\n",
    "'                    # Inpainting logic\\n' \\\n",
    "'                    if inpa_inj_sched_prev and model_mask_kwargs is not None:\\n' \\\n",
    "'                        gt_keep_mask = torch.zeros(x.shape, device=x.device)\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) > 0.] = 1\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) < 0.] = 0\\n' \\\n",
    "'                        gt = model_kwargs[\"ref_img\"].to(device)\\n' \\\n",
    "'                        alpha_t = self.noise_schedule.marginal_alpha(t)\\n' \\\n",
    "'                        sigma_t = self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'                        if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                            # Assume get_gt_noised is implemented elsewhere to add cumulative noise\\n' \\\n",
    "'                            weighed_gt = self.get_gt_noised(gt, t)\\n' \\\n",
    "'                        else:\\n' \\\n",
    "'                            gt_weight = alpha_t\\n' \\\n",
    "'                            gt_part = gt_weight * gt\\n' \\\n",
    "'                            noise_weight = sigma_t\\n' \\\n",
    "'                            noise_part = noise_weight * torch.randn_like(x)\\n' \\\n",
    "'                            weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                        x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                        #self.save_intermediate(x, step, \"after_inpainting\", device)\\n' \\\n",
    "'                    if lower_order_final and steps < 10:\\n' \\\n",
    "'                        step_order = min(order, steps + 1 - step)\\n' \\\n",
    "'                    else:\\n' \\\n",
    "'                        step_order = order\\n' \\\n",
    "'                    x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step_order, solver_type=solver_type)\\n' \\\n",
    "'                    if self.correcting_xt_fn is not None:\\n' \\\n",
    "'                        x = self.correcting_xt_fn(x, t, step)\\n' \\\n",
    "'                    if return_intermediate:\\n' \\\n",
    "'                        intermediates.append(x)\\n' \\\n",
    "'                    for i in range(order - 1):\\n' \\\n",
    "'                        t_prev_list[i] = t_prev_list[i + 1]\\n' \\\n",
    "'                        model_prev_list[i] = model_prev_list[i + 1]\\n' \\\n",
    "'                    t_prev_list[-1] = t\\n' \\\n",
    "'                    if step < steps:\\n' \\\n",
    "'                        model_prev_list[-1] = self.model_fn(x, t)\\n' \\\n",
    "'            elif method in [\"singlestep\", \"singlestep_fixed\"]:\\n' \\\n",
    "'                if method == \"singlestep\":\\n' \\\n",
    "'                    timesteps_outer, orders = self.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order, skip_type=skip_type, t_T=t_T, t_0=t_0, device=device)\\n' \\\n",
    "'                elif method == \"singlestep_fixed\":\\n' \\\n",
    "'                    K = steps // order\\n' \\\n",
    "'                    orders = [order,] * K\\n' \\\n",
    "'                    timesteps_outer = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)\\n' \\\n",
    "'                for step, order in enumerate(orders):\\n' \\\n",
    "'                    t = timesteps_outer[step + 1]  # Use the end timestep for inpainting\\n' \\\n",
    "'                    # Initial inpainting for singlestep\\n' \\\n",
    "'                    if inpa_inj_sched_prev and model_mask_kwargs is not None and step == 0:\\n' \\\n",
    "'                        #print(f\"Initial inpainting for singlestep at step {step}, t={t}\")\\n' \\\n",
    "'                        gt_keep_mask = torch.zeros(x.shape, device=device)\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) > 0.] = 1\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) < 0.] = 0\\n' \\\n",
    "'                        gt = model_kwargs.get(\\'ref_img\\', torch.zeros_like(x)).to(device)\\n' \\\n",
    "'                        alpha_t = self.noise_schedule.marginal_alpha(t)\\n' \\\n",
    "'                        sigma_t = self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'                        if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                            weighed_gt = self.get_gt_noised(gt, t)\\n' \\\n",
    "'                        else:\\n' \\\n",
    "'                            gt_part = alpha_t * gt\\n' \\\n",
    "'                            noise_part = sigma_t * torch.randn_like(x)\\n' \\\n",
    "'                            weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                        x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                        #self.save_intermediate(x, step, \"after_initial_inpainting\", device)\\n' \\\n",
    "'                    # Inpainting logic before solver update\\n' \\\n",
    "'                    if inpa_inj_sched_prev and model_mask_kwargs is not None:\\n' \\\n",
    "'                        #print(f\"Inpainting for singlestep at step {step}, t={t}\")\\n' \\\n",
    "'                        gt_keep_mask = torch.zeros(x.shape, device=device)\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) > 0.] = 1\\n' \\\n",
    "'                        gt_keep_mask[model_mask_kwargs[\"ref_img\"].to(device) < 0.] = 0\\n' \\\n",
    "'                        gt = model_kwargs.get(\\'ref_img\\', torch.zeros_like(x)).to(device)\\n' \\\n",
    "'                        alpha_t = self.noise_schedule.marginal_alpha(t)\\n' \\\n",
    "'                        sigma_t = self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'                        if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                            weighed_gt = self.get_gt_noised(gt, t)\\n' \\\n",
    "'                        else:\\n' \\\n",
    "'                            gt_weight = alpha_t\\n' \\\n",
    "'                            gt_part = gt_weight * gt\\n' \\\n",
    "'                            noise_weight = sigma_t\\n' \\\n",
    "'                            noise_part = noise_weight * torch.randn_like(x)\\n' \\\n",
    "'                            weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                        x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                        #self.save_intermediate(x, step, \"after_inpainting\", device)\\n' \\\n",
    "'                    s, t = timesteps_outer[step], timesteps_outer[step + 1]\\n' \\\n",
    "'                    timesteps_inner = self.get_time_steps(skip_type=skip_type, t_T=s.item(), t_0=t.item(), N=order, device=device)\\n' \\\n",
    "'                    lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)\\n' \\\n",
    "'                    h = lambda_inner[-1] - lambda_inner[0]\\n' \\\n",
    "'                    r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h\\n' \\\n",
    "'                    r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h\\n' \\\n",
    "'                    x = self.singlestep_dpm_solver_update(x, s, t, order, solver_type=solver_type, r1=r1, r2=r2)\\n' \\\n",
    "'                    if self.correcting_xt_fn is not None:\\n' \\\n",
    "'                        x = self.correcting_xt_fn(x, t, step)\\n' \\\n",
    "'                    if return_intermediate:\\n' \\\n",
    "'                        intermediates.append(x)\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                raise ValueError(\"Got wrong method {}\".format(method))\\n' \\\n",
    "'            if denoise_to_zero:\\n' \\\n",
    "'                t = torch.ones((1,)).to(device) * t_0\\n' \\\n",
    "'                # Inpainting logic for denoise_to_zero\\n' \\\n",
    "'                if inpa_inj_sched_prev and model_mask_kwargs is not None:\\n' \\\n",
    "'                    gt_keep_mask = torch.zeros(x.shape, device=x.device)\\n' \\\n",
    "'                    gt_keep_mask[model_mask_kwargs[\"ref_img\"] > 0.] = 1\\n' \\\n",
    "'                    gt_keep_mask[model_mask_kwargs[\"ref_img\"] < 0.] = 0\\n' \\\n",
    "'                    gt = model_kwargs.get(\\'ref_img\\', torch.zeros_like(x))\\n' \\\n",
    "'                    alpha_t = self.noise_schedule.marginal_alpha(t)\\n' \\\n",
    "'                    sigma_t = self.noise_schedule.marginal_std(t)\\n' \\\n",
    "'                    if inpa_inj_sched_prev_cumnoise:\\n' \\\n",
    "'                        # Assume get_gt_noised is implemented elsewhere to add cumulative noise\\n' \\\n",
    "'                        weighed_gt = self.get_gt_noised(gt, t)\\n' \\\n",
    "'                    else:\\n' \\\n",
    "'                        gt_weight = alpha_t\\n' \\\n",
    "'                        gt_part = gt_weight * gt\\n' \\\n",
    "'                        noise_weight = sigma_t\\n' \\\n",
    "'                        noise_part = noise_weight * torch.randn_like(x)\\n' \\\n",
    "'                        weighed_gt = gt_part + noise_part\\n' \\\n",
    "'                    x = gt_keep_mask * weighed_gt + (1 - gt_keep_mask) * x\\n' \\\n",
    "'                x = self.denoise_to_zero_fn(x, t)\\n' \\\n",
    "'                if self.correcting_xt_fn is not None:\\n' \\\n",
    "'                    x = self.correcting_xt_fn(x, t, step + 1)\\n' \\\n",
    "'                if return_intermediate:\\n' \\\n",
    "'                    intermediates.append(x)\\n' \\\n",
    "'        if return_intermediate:\\n' \\\n",
    "'            return x, intermediates\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            return x\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'#############################################################\\n' \\\n",
    "'# other utility functions\\n' \\\n",
    "'#############################################################\\n' \\\n",
    "'\\n' \\\n",
    "'def interpolate_fn(x, xp, yp):\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    A piecewise linear function y = f(x), using xp and yp as keypoints.\\n' \\\n",
    "'    We implement f(x) in a differentiable way (i.e. applicable for autograd).\\n' \\\n",
    "'    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\\n' \\\n",
    "'\\n' \\\n",
    "'    Args:\\n' \\\n",
    "'        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\\n' \\\n",
    "'        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\\n' \\\n",
    "'        yp: PyTorch tensor with shape [C, K].\\n' \\\n",
    "'    Returns:\\n' \\\n",
    "'        The function values f(x), with shape [N, C].\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    N, K = x.shape[0], xp.shape[1]\\n' \\\n",
    "'    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\\n' \\\n",
    "'    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\\n' \\\n",
    "'    x_idx = torch.argmin(x_indices, dim=2)\\n' \\\n",
    "'    cand_start_idx = x_idx - 1\\n' \\\n",
    "'    start_idx = torch.where(\\n' \\\n",
    "'        torch.eq(x_idx, 0),\\n' \\\n",
    "'        torch.tensor(1, device=x.device),\\n' \\\n",
    "'        torch.where(\\n' \\\n",
    "'            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\\n' \\\n",
    "'        ),\\n' \\\n",
    "'    )\\n' \\\n",
    "'    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\\n' \\\n",
    "'    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\\n' \\\n",
    "'    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\\n' \\\n",
    "'    start_idx2 = torch.where(\\n' \\\n",
    "'        torch.eq(x_idx, 0),\\n' \\\n",
    "'        torch.tensor(0, device=x.device),\\n' \\\n",
    "'        torch.where(\\n' \\\n",
    "'            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\\n' \\\n",
    "'        ),\\n' \\\n",
    "'    )\\n' \\\n",
    "'    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\\n' \\\n",
    "'    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\\n' \\\n",
    "'    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\\n' \\\n",
    "'    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\\n' \\\n",
    "'    return cand\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'def expand_dims(v, dims):\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    Expand the tensor `v` to the dim `dims`.\\n' \\\n",
    "'\\n' \\\n",
    "'    Args:\\n' \\\n",
    "'        `v`: a PyTorch tensor with shape [N].\\n' \\\n",
    "'        `dim`: a `int`.\\n' \\\n",
    "'    Returns:\\n' \\\n",
    "'        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    return v[(...,) + (None,)*(dims - 1)]\\n'\n",
    "with open('/kaggle/working/p2w/dpm_solver/sampler.py', 'w') as f:\n",
    "    f.write(sampler_str)\n",
    "print(\"Successfully wrote sampler.py to /kaggle/working/p2w/dpm_solver/sampler.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e6bbb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:54.805099Z",
     "iopub.status.busy": "2025-08-05T17:45:54.804831Z",
     "iopub.status.idle": "2025-08-05T17:45:54.823734Z",
     "shell.execute_reply": "2025-08-05T17:45:54.822926Z"
    },
    "papermill": {
     "duration": 0.038216,
     "end_time": "2025-08-05T17:45:54.825442",
     "exception": false,
     "start_time": "2025-08-05T17:45:54.787226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote diffusion.py to /kaggle/working/p2w/runners/diffusion.py\n"
     ]
    }
   ],
   "source": [
    "diffusion_str = 'import os\\n' \\\n",
    "'import logging\\n' \\\n",
    "'import time\\n' \\\n",
    "'import glob\\n' \\\n",
    "'\\n' \\\n",
    "'import blobfile as bf\\n' \\\n",
    "'import numpy as np\\n' \\\n",
    "'import tqdm\\n' \\\n",
    "'import torch\\n' \\\n",
    "'import torch.utils.data as data\\n' \\\n",
    "'import torch.distributed as dist\\n' \\\n",
    "'import torchvision.utils as tvu\\n' \\\n",
    "'import torch.nn.functional as F\\n' \\\n",
    "'\\n' \\\n",
    "'from models.p2_weighing.unet import UNetModel as LWDM_Model\\n' \\\n",
    "'from models.p2_weighing.image_datasets import load_data\\n' \\\n",
    "'from models.p2_weighing import logger\\n' \\\n",
    "'from dpm_solver.sampler import NoiseScheduleVP, model_wrapper, DPM_Solver\\n' \\\n",
    "'from metrics_cal import *\\n' \\\n",
    "'from datasets import get_dataset, data_transform, inverse_data_transform\\n' \\\n",
    "'\\n' \\\n",
    "'def load_data_for_worker(base_samples, batch_size, cond_class):\\n' \\\n",
    "'    with bf.BlobFile(base_samples, \"rb\") as f:\\n' \\\n",
    "'        obj = np.load(f)\\n' \\\n",
    "'        image_arr = obj[\"arr_0\"]\\n' \\\n",
    "'        if cond_class:\\n' \\\n",
    "'            label_arr = obj[\"arr_1\"]\\n' \\\n",
    "'    buffer = []\\n' \\\n",
    "'    label_buffer = []\\n' \\\n",
    "'    while True:\\n' \\\n",
    "'        for i in range(len(image_arr)):\\n' \\\n",
    "'            buffer.append(image_arr[i])\\n' \\\n",
    "'            if cond_class:\\n' \\\n",
    "'                label_buffer.append(label_arr[i])\\n' \\\n",
    "'            if len(buffer) == batch_size:\\n' \\\n",
    "'                batch = torch.from_numpy(np.stack(buffer)).float()\\n' \\\n",
    "'                batch = batch / 127.5 - 1.0\\n' \\\n",
    "'                batch = batch.permute(0, 3, 1, 2)\\n' \\\n",
    "'                res = dict(low_res=batch)\\n' \\\n",
    "'                if cond_class:\\n' \\\n",
    "'                    res[\"y\"] = torch.from_numpy(np.stack(label_buffer))\\n' \\\n",
    "'                yield res\\n' \\\n",
    "'                buffer, label_buffer = [], []\\n' \\\n",
    "'\\n' \\\n",
    "'def load_reference(data_dir, batch_size, image_size, class_cond=False):\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    Load reference images or masks from a directory using load_data.\\n' \\\n",
    "'    Args:\\n' \\\n",
    "'        data_dir: Directory containing .png images.\\n' \\\n",
    "'        batch_size: Number of images per batch.\\n' \\\n",
    "'        image_size: Target image size (height, width).\\n' \\\n",
    "'    Yields:\\n' \\\n",
    "'        Dictionary with \\'ref_img\\' containing a batch of images.\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    data = load_data(\\n' \\\n",
    "'        data_dir=data_dir,\\n' \\\n",
    "'        batch_size=batch_size,\\n' \\\n",
    "'        image_size=image_size,\\n' \\\n",
    "'        class_cond=False,\\n' \\\n",
    "'        deterministic=True,\\n' \\\n",
    "'        random_flip=False,\\n' \\\n",
    "'    )\\n' \\\n",
    "'    for large_batch, model_kwargs in data:\\n' \\\n",
    "'        model_kwargs[\"ref_img\"] = large_batch\\n' \\\n",
    "'        yield model_kwargs\\n' \\\n",
    "'\\n' \\\n",
    "'def torch2hwcuint8(x, clip=False):\\n' \\\n",
    "'    if clip:\\n' \\\n",
    "'        x = torch.clamp(x, -1, 1)\\n' \\\n",
    "'    x = (x + 1.0) / 2.0\\n' \\\n",
    "'    return x\\n' \\\n",
    "'\\n' \\\n",
    "'def upsample_image(image_tensor, target_size=(256, 256)):\\n' \\\n",
    "'    image_tensor = (image_tensor + 1) / 2.0\\n' \\\n",
    "'    image_tensor = image_tensor.clamp(0, 1)\\n' \\\n",
    "'    upscaled_image_tensor = F.interpolate(image_tensor, size=target_size, mode=\\'bilinear\\', align_corners=False)\\n' \\\n",
    "'    upscaled_image_tensor = upscaled_image_tensor * 2.0 - 1\\n' \\\n",
    "'    return upscaled_image_tensor\\n' \\\n",
    "'\\n' \\\n",
    "'def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    Create a beta schedule that discretizes the given alpha_t_bar function,\\n' \\\n",
    "'    which defines the cumulative product of (1-beta) over time from t = [0,1].\\n' \\\n",
    "'    :param num_diffusion_timesteps: the number of betas to produce.\\n' \\\n",
    "'    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\\n' \\\n",
    "'                      produces the cumulative product of (1-beta) up to that\\n' \\\n",
    "'                      part of the diffusion process.\\n' \\\n",
    "'    :param max_beta: the maximum beta to use; use values lower than 1 to\\n' \\\n",
    "'                     prevent singularities.\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    betas = []\\n' \\\n",
    "'    for i in range(num_diffusion_timesteps):\\n' \\\n",
    "'        t1 = i / num_diffusion_timesteps\\n' \\\n",
    "'        t2 = (i + 1) / num_diffusion_timesteps\\n' \\\n",
    "'        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\\n' \\\n",
    "'    return np.array(betas)\\n' \\\n",
    "'\\n' \\\n",
    "'def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\\n' \\\n",
    "'    def sigmoid(x):\\n' \\\n",
    "'        return 1 / (np.exp(-x) + 1)\\n' \\\n",
    "'\\n' \\\n",
    "'    if beta_schedule == \"quad\":\\n' \\\n",
    "'        betas = (\\n' \\\n",
    "'            np.linspace(\\n' \\\n",
    "'                beta_start ** 0.5,\\n' \\\n",
    "'                beta_end ** 0.5,\\n' \\\n",
    "'                num_diffusion_timesteps,\\n' \\\n",
    "'                dtype=np.float64,\\n' \\\n",
    "'            )\\n' \\\n",
    "'            ** 2\\n' \\\n",
    "'        )\\n' \\\n",
    "'    elif beta_schedule == \"linear\":\\n' \\\n",
    "'        betas = np.linspace(\\n' \\\n",
    "'            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\\n' \\\n",
    "'        )\\n' \\\n",
    "'    elif beta_schedule == \"cosine\":\\n' \\\n",
    "'        return betas_for_alpha_bar(\\n' \\\n",
    "'            num_diffusion_timesteps,\\n' \\\n",
    "'            lambda t: np.cos((t + 0.008) / 1.008 * np.pi / 2) ** 2,\\n' \\\n",
    "'        )\\n' \\\n",
    "'    elif beta_schedule == \"const\":\\n' \\\n",
    "'        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\\n' \\\n",
    "'    elif beta_schedule == \"jsd\": # 1/T, 1/(T-2), ..., 1\\n' \\\n",
    "'        betas = 1.0 / np.linspace(\\n' \\\n",
    "'            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\\n' \\\n",
    "'        )\\n' \\\n",
    "'    elif beta_schedule == \"sigmoid\":\\n' \\\n",
    "'        betas = np.linspace(-6, 6, num_diffusion_timesteps)\\n' \\\n",
    "'        betas = sigmoid(betas) * (beta_end - beta_start) + beta_start\\n' \\\n",
    "'    else:\\n' \\\n",
    "'        raise NotImplementedError(beta_schedule)\\n' \\\n",
    "'    assert betas.shape == (num_diffusion_timesteps,)\\n' \\\n",
    "'    return betas\\n' \\\n",
    "'\\n' \\\n",
    "'class Diffusion(object):\\n' \\\n",
    "'    def __init__(self, args, config, rank=None):\\n' \\\n",
    "'        self.args = args\\n' \\\n",
    "'        self.config = config\\n' \\\n",
    "'        if rank is None:\\n' \\\n",
    "'            device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            device = rank\\n' \\\n",
    "'            self.rank = rank\\n' \\\n",
    "'        self.device = device\\n' \\\n",
    "'\\n' \\\n",
    "'        self.model_var_type = self.config.model.var_type \\n' \\\n",
    "'        betas = get_beta_schedule(\\n' \\\n",
    "'            beta_schedule=config.diffusion.beta_schedule,\\n' \\\n",
    "'            beta_start=config.diffusion.beta_start,\\n' \\\n",
    "'            beta_end=config.diffusion.beta_end,\\n' \\\n",
    "'            num_diffusion_timesteps=config.diffusion.num_diffusion_timesteps)\\n' \\\n",
    "'        betas = self.betas = torch.from_numpy(betas).float().to(self.device)\\n' \\\n",
    "'        self.num_timesteps = betas.shape[0]\\n' \\\n",
    "'\\n' \\\n",
    "'        alphas = 1.0 - betas\\n' \\\n",
    "'        alphas_cumprod = alphas.cumprod(dim=0)\\n' \\\n",
    "'        self.alphas_cumprod = alphas_cumprod\\n' \\\n",
    "'        alphas_cumprod_prev = torch.cat(\\n' \\\n",
    "'            [torch.ones(1).to(device), alphas_cumprod[:-1]], dim=0\\n' \\\n",
    "'        )\\n' \\\n",
    "'        posterior_variance = (\\n' \\\n",
    "'            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\\n' \\\n",
    "'        )\\n' \\\n",
    "'        if self.model_var_type == \"fixedlarge\":\\n' \\\n",
    "'            self.logvar = betas.log()\\n' \\\n",
    "'            # torch.cat(\\n' \\\n",
    "'            # [posterior_variance[1:2], betas[1:]], dim=0).log()\\n' \\\n",
    "'        elif self.model_var_type == \"fixedsmall\":\\n' \\\n",
    "'            self.logvar = posterior_variance.clamp(min=1e-20).log()\\n' \\\n",
    "'\\n' \\\n",
    "'    def create_dpm_solver(self, model, config, x, base_samples=None, model_kwargs=None, classifier=None, classifier_scale=0.0):\\n' \\\n",
    "'        \"\"\"Create a DPM_Solver instance with NoiseScheduleVP and necessary functions for noise addition.\"\"\"\\n' \\\n",
    "'\\n' \\\n",
    "'        # Instantiate NoiseScheduleVP\\n' \\\n",
    "'        noise_schedule = NoiseScheduleVP(schedule=\"discrete\", alphas_cumprod=self.alphas_cumprod.to(self.device))\\n' \\\n",
    "'\\n' \\\n",
    "'        # Handle classifier and conditional sampling (moved verbatim from sample_image)\\n' \\\n",
    "'        if self.config.sampling.cond_class:\\n' \\\n",
    "'            if base_samples and \"y\" in base_samples:\\n' \\\n",
    "'                classes = base_samples[\"y\"].to(self.device)\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                classes = torch.randint(low=0, high=config.data.num_classes, size=(x.shape[0],)).to(self.device)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            classes = None\\n' \\\n",
    "'\\n' \\\n",
    "'    # Prepare model_kwargs (moved verbatim from sample_image)\\n' \\\n",
    "'        if base_samples is None:\\n' \\\n",
    "'            model_kwargs = {} if model_kwargs is None else model_kwargs\\n' \\\n",
    "'            if classes is not None:\\n' \\\n",
    "'                model_kwargs = {**model_kwargs, \"y\": classes}\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            model_kwargs = {\"y\": base_samples[\"y\"], \"low_res\": base_samples[\"low_res\"]} if model_kwargs is None else {**model_kwargs, \"y\": base_samples[\"y\"], \"low_res\": base_samples[\"low_res\"]}\\n' \\\n",
    "'\\n' \\\n",
    "'        # Define model_fn as in sample_image\\n' \\\n",
    "'        def model_fn(x, t, **model_kwargs):\\n' \\\n",
    "'            out = model(x, t, **model_kwargs)\\n' \\\n",
    "'            if self.config.model.out_channels == 6:\\n' \\\n",
    "'                out = torch.split(out, 3, dim=1)[0]\\n' \\\n",
    "'            return out\\n' \\\n",
    "'\\n' \\\n",
    "'        # Define classifier_fn as in sample_image\\n' \\\n",
    "'        def classifier_fn(x, t, y, **classifier_kwargs):\\n' \\\n",
    "'            if classifier is None:\\n' \\\n",
    "'                return None\\n' \\\n",
    "'            logits = classifier(x, t)\\n' \\\n",
    "'            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\\n' \\\n",
    "'            return log_probs[range(len(logits)), y.view(-1)]\\n' \\\n",
    "'\\n' \\\n",
    "'        # Create model_fn_continuous as in sample_image\\n' \\\n",
    "'        model_fn_continuous = model_wrapper(\\n' \\\n",
    "'            model_fn,\\n' \\\n",
    "'            noise_schedule,\\n' \\\n",
    "'            model_type=\"noise\",\\n' \\\n",
    "'            model_kwargs=model_kwargs,\\n' \\\n",
    "'            guidance_type=\"uncond\" if classifier is None else \"classifier\",\\n' \\\n",
    "'            condition=model_kwargs[\"y\"] if \"y\" in model_kwargs else None,\\n' \\\n",
    "'            guidance_scale=0.0,\\n' \\\n",
    "'            classifier_fn=classifier_fn,\\n' \\\n",
    "'            classifier_kwargs={},\\n' \\\n",
    "'        )\\n' \\\n",
    "'\\n' \\\n",
    "'        # Instantiate DPM_Solver\\n' \\\n",
    "'        dpm_solver = DPM_Solver(\\n' \\\n",
    "'            model_fn_continuous,\\n' \\\n",
    "'            noise_schedule,\\n' \\\n",
    "'            algorithm_type=self.args.sample_type,\\n' \\\n",
    "'            correcting_x0_fn=\"dynamic_thresholding\" if self.config.sampling.thresholding else None\\n' \\\n",
    "'        )\\n' \\\n",
    "'        return dpm_solver, noise_schedule, classes, model_kwargs\\n' \\\n",
    "'    def sample(self):\\n' \\\n",
    "'        start_time = time.time()\\n' \\\n",
    "'        logger.configure(dir=self.args.image_folder)\\n' \\\n",
    "'        logger.log(\"creating model...\")\\n' \\\n",
    "'        #config = self.config\\n' \\\n",
    "'        if self.config.model.model_type == \"p2-weighing\":\\n' \\\n",
    "'            model_64 = LWDM_Model(\\n' \\\n",
    "'                image_size=self.config.model.image_size_coarse,\\n' \\\n",
    "'                in_channels=self.config.model.in_channels,\\n' \\\n",
    "'                model_channels=self.config.model.model_channels,\\n' \\\n",
    "'                out_channels=self.config.model.out_channels,\\n' \\\n",
    "'                num_res_blocks=self.config.model.num_res_blocks,\\n' \\\n",
    "'                attention_resolutions=self.config.model.attention_resolutions_coarse,\\n' \\\n",
    "'                dropout=self.config.model.dropout,\\n' \\\n",
    "'                channel_mult=self.config.model.channel_mult_coarse,\\n' \\\n",
    "'                conv_resample=self.config.model.conv_resample,\\n' \\\n",
    "'                dims=self.config.model.dims,\\n' \\\n",
    "'                num_classes=self.config.model.num_classes,\\n' \\\n",
    "'                use_checkpoint=self.config.model.use_checkpoint,\\n' \\\n",
    "'                use_fp16=self.config.model.use_fp16,\\n' \\\n",
    "'                num_heads=self.config.model.num_heads,\\n' \\\n",
    "'                num_head_channels=self.config.model.num_head_channels,\\n' \\\n",
    "'                num_heads_upsample=self.config.model.num_heads_upsample,\\n' \\\n",
    "'                use_scale_shift_norm=self.config.model.use_scale_shift_norm,\\n' \\\n",
    "'                resblock_updown=self.config.model.resblock_updown,\\n' \\\n",
    "'                use_new_attention_order=self.config.model.use_new_attention_order\\n' \\\n",
    "'            )\\n' \\\n",
    "'            model_256 = LWDM_Model(\\n' \\\n",
    "'                image_size=self.config.model.image_size_fine,\\n' \\\n",
    "'                in_channels=self.config.model.in_channels,\\n' \\\n",
    "'                model_channels=self.config.model.model_channels,\\n' \\\n",
    "'                out_channels=self.config.model.out_channels,\\n' \\\n",
    "'                num_res_blocks=self.config.model.num_res_blocks,\\n' \\\n",
    "'                attention_resolutions=self.config.model.attention_resolutions,\\n' \\\n",
    "'                dropout=self.config.model.dropout,\\n' \\\n",
    "'                channel_mult=self.config.model.channel_mult_fine,\\n' \\\n",
    "'                conv_resample=self.config.model.conv_resample,\\n' \\\n",
    "'                dims=self.config.model.dims,\\n' \\\n",
    "'                num_classes=self.config.model.num_classes,\\n' \\\n",
    "'                use_checkpoint=self.config.model.use_checkpoint,\\n' \\\n",
    "'                use_fp16=self.config.model.use_fp16,\\n' \\\n",
    "'                num_heads=self.config.model.num_heads,\\n' \\\n",
    "'                num_head_channels=self.config.model.num_head_channels,\\n' \\\n",
    "'                num_heads_upsample=self.config.model.num_heads_upsample,\\n' \\\n",
    "'                use_scale_shift_norm=self.config.model.use_scale_shift_norm,\\n' \\\n",
    "'                resblock_updown=self.config.model.resblock_updown,\\n' \\\n",
    "'                use_new_attention_order=self.config.model.use_new_attention_order\\n' \\\n",
    "'            )\\n' \\\n",
    "'\\n' \\\n",
    "'            model_64 = model_64.to(self.rank)\\n' \\\n",
    "'            model_256 = model_256.to(self.rank)\\n' \\\n",
    "'            map_location = {\"cuda:%d\" % 0: \"cuda:%d\" % self.rank}\\n' \\\n",
    "'\\n' \\\n",
    "'            if \"ckpt_dir_coarse\" in self.config.model.__dict__.keys() and \"ckpt_dir_fine\" in self.config.model.__dict__.keys():\\n' \\\n",
    "'                ckpt_dir_coarse = os.path.expanduser(self.args.model_path_64)\\n' \\\n",
    "'                ckpt_dir_fine = os.path.expanduser(self.args.model_path_256)\\n' \\\n",
    "'                states_coarse = torch.load(\\n' \\\n",
    "'                    ckpt_dir_coarse,\\n' \\\n",
    "'                    map_location=map_location\\n' \\\n",
    "'                )\\n' \\\n",
    "'                states_fine = torch.load(\\n' \\\n",
    "'                    ckpt_dir_fine,\\n' \\\n",
    "'                    map_location=map_location\\n' \\\n",
    "'                )\\n' \\\n",
    "'                model_64.load_state_dict(states_coarse, strict=True)\\n' \\\n",
    "'                model_256.load_state_dict(states_fine, strict=True)\\n' \\\n",
    "'                if self.config.model.use_fp16:\\n' \\\n",
    "'                    model_64.convert_to_fp16()\\n' \\\n",
    "'                    model_256.convert_to_fp16()\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                raise NotImplementedError(\"ckpt_dir_coarse or ckpt_dir_fine not defined\")\\n' \\\n",
    "'\\n' \\\n",
    "'            classifier = None\\n' \\\n",
    "'\\n' \\\n",
    "'            model_64.eval()\\n' \\\n",
    "'            model_256.eval()\\n' \\\n",
    "'\\n' \\\n",
    "'\\n' \\\n",
    "'            if self.config.model.is_upsampling:\\n' \\\n",
    "'                base_samples_total = load_data_for_worker(self.args.base_samples, self.config.sampling.batch_size, self.config.sampling.cond_class)\\n' \\\n",
    "'\\n' \\\n",
    "'            elif self.args.base_samples and self.args.mask_path:\\n' \\\n",
    "'                logger.log(\"loading data...\")\\n' \\\n",
    "'                # Load data for both resolutions\\n' \\\n",
    "'                ref_data_64 = load_reference(\\n' \\\n",
    "'                    self.args.base_samples,\\n' \\\n",
    "'                    self.config.sampling.batch_size,\\n' \\\n",
    "'                    self.config.model.image_size_coarse,\\n' \\\n",
    "'                    class_cond=self.config.sampling.cond_class,\\n' \\\n",
    "'                )\\n' \\\n",
    "'                mask_data_64 = load_reference(\\n' \\\n",
    "'                    self.args.mask_path,\\n' \\\n",
    "'                    self.config.sampling.batch_size,\\n' \\\n",
    "'                    self.config.model.image_size_coarse,\\n' \\\n",
    "'                    class_cond=self.config.sampling.cond_class,\\n' \\\n",
    "'                )\\n' \\\n",
    "'                ref_data_256 = load_reference(\\n' \\\n",
    "'                    self.args.base_samples,\\n' \\\n",
    "'                    self.config.sampling.batch_size,\\n' \\\n",
    "'                    self.config.model.image_size_fine,\\n' \\\n",
    "'                    class_cond=self.config.sampling.cond_class,\\n' \\\n",
    "'                )\\n' \\\n",
    "'                mask_data_256 = load_reference(\\n' \\\n",
    "'                    self.args.mask_path,\\n' \\\n",
    "'                    self.config.sampling.batch_size,\\n' \\\n",
    "'                    self.config.model.image_size_fine,\\n' \\\n",
    "'                    class_cond=self.config.sampling.cond_class,\\n' \\\n",
    "'                )\\n' \\\n",
    "'                # Metrics initialization\\n' \\\n",
    "'                metrics_file_path = os.path.join(self.args.image_folder, \"metrics_log.txt\")\\n' \\\n",
    "'                lpips_value = 0.\\n' \\\n",
    "'                coarse_lpips_value = 0.\\n' \\\n",
    "'                psnr_value = 0.\\n' \\\n",
    "'                coarse_psnr_value = 0.\\n' \\\n",
    "'                ssim_value = 0.\\n' \\\n",
    "'                coarse_ssim_value = 0.\\n' \\\n",
    "'                l1_value = 0.\\n' \\\n",
    "'                coarse_l1_value = 0.\\n' \\\n",
    "'\\n' \\\n",
    "'                # Log conditions\\n' \\\n",
    "'                with open(metrics_file_path, \"a\") as metrics_file:\\n' \\\n",
    "'                    metrics_file.write(f\"Condition:\\\\n\")\\n' \\\n",
    "'                    metrics_file.write(f\"\\\\tmask_path: {self.args.mask_path}\\\\n\")\\n' \\\n",
    "'                    metrics_file.write(f\"\\\\ttimesteps_coarse: {self.args.timesteps_coarse}\\\\n\")\\n' \\\n",
    "'                    metrics_file.write(f\"\\\\ttimesteps_fine: {self.args.timesteps}\\\\n\")\\n' \\\n",
    "'                    metrics_file.write(f\"\\\\tskip_type: {self.args.skip_type}\\\\n\")\\n' \\\n",
    "'                    metrics_file.write(f\"\\\\tsample_type: {self.args.sample_type}\\\\n\")\\n' \\\n",
    "'                    metrics_file.write(f\"\\\\n\")\\n' \\\n",
    "'\\n' \\\n",
    "'                logger.log(\"creating samples...\")\\n' \\\n",
    "'                count = 0\\n' \\\n",
    "'                all_items = os.listdir(self.args.base_samples)\\n' \\\n",
    "'                num_inputs = self.config.sampling.total_N\\n' \\\n",
    "'\\n' \\\n",
    "'                while count < num_inputs:\\n' \\\n",
    "'                    model_mask_kwargs_64 = next(mask_data_64)\\n' \\\n",
    "'                    model_kwargs_64 = next(ref_data_64)\\n' \\\n",
    "'                    model_mask_kwargs_64 = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in model_mask_kwargs_64.items()}\\n' \\\n",
    "'                    model_kwargs_64 = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in model_kwargs_64.items()}\\n' \\\n",
    "'                    model_mask_kwargs_256 = next(mask_data_256)\\n' \\\n",
    "'                    model_kwargs_256 = next(ref_data_256)\\n' \\\n",
    "'                    model_mask_kwargs_256 = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in model_mask_kwargs_256.items()}\\n' \\\n",
    "'                    model_kwargs_256 = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in model_kwargs_256.items()}\\n' \\\n",
    "'                    gt = model_kwargs_256[\"ref_img\"]\\n' \\\n",
    "'                    mask = model_mask_kwargs_256[\"ref_img\"]\\n' \\\n",
    "'                    #print(\"gt shape: {}\".format(gt.shape))\\n' \\\n",
    "'                    #print(\"mask shape: {}\".format(mask.shape))\\n' \\\n",
    "'\\n' \\\n",
    "'                    if self.args.use_inverse_masks:\\n' \\\n",
    "'                        model_mask_kwargs_64[\"ref_img\"] = model_mask_kwargs_64[\"ref_img\"] * (-1)\\n' \\\n",
    "'                        model_mask_kwargs_256[\"ref_img\"] = model_mask_kwargs_256[\"ref_img\"] * (-1)\\n' \\\n",
    "'\\n' \\\n",
    "'                    base_samples=None\\n' \\\n",
    "'                    mask = (mask > 0).float()\\n' \\\n",
    "'                    x_64 = torch.randn(self.config.sampling.batch_size, self.config.data.channels, self.config.model.image_size_coarse, self.config.model.image_size_coarse, device=self.device)\\n' \\\n",
    "'                    dpm_solver_64, noise_schedule_64, classes_64, _ = self.create_dpm_solver(model_64, self.config, x_64, base_samples, model_kwargs_64, classifier=None, classifier_scale=0.0)\\n' \\\n",
    "'                    sample_coarse = self.sample_image(x_64, model_64,timesteps=self.args.timesteps_coarse, model_kwargs=model_kwargs_64, model_mask_kwargs=model_mask_kwargs_64, inpa_inj_sched_prev=self.args.inpa_inj_sched_prev, inpa_inj_sched_prev_cumnoise=self.args.inpa_inj_sched_prev_cumnoise, dpm_solver=dpm_solver_64)\\n' \\\n",
    "'                    sample_coarse_256 = upsample_image(sample_coarse)\\n' \\\n",
    "'\\n' \\\n",
    "'                    #t_T_fine = torch.tensor([self.args.timesteps], device=self.device)\\n' \\\n",
    "'                    # Prepare noise for fine inpainting using DPM_Solver\\'s add_noise\\n' \\\n",
    "'                    t_T_fine = torch.tensor([self.args.timesteps] * self.config.sampling.batch_size, device=self.device)\\n' \\\n",
    "'                    noise = torch.randn_like(sample_coarse_256)\\n' \\\n",
    "'                    dpm_solver_256, noise_schedule_256, classes_256, _ = self.create_dpm_solver(model_256, self.config, sample_coarse_256, base_samples, model_kwargs_256, classifier=None, classifier_scale=0.0)\\n' \\\n",
    "'                    noised_coarse_256 = dpm_solver_256.add_noise(sample_coarse_256, t_T_fine, noise)\\n' \\\n",
    "'\\n' \\\n",
    "'                    # Update model_kwargs for fine inpainting\\n' \\\n",
    "'                    model_fine_kwargs = model_kwargs_256\\n' \\\n",
    "'                    model_fine_kwargs[\"ref_img\"] = sample_coarse_256 * (1 - mask) + model_kwargs_256[\"ref_img\"] * mask \\n' \\\n",
    "'                    sample_fine = self.sample_image(noised_coarse_256, model_256,timesteps=self.args.timesteps, model_kwargs=model_fine_kwargs, model_mask_kwargs=model_mask_kwargs_256, inpa_inj_sched_prev=self.args.inpa_inj_sched_prev, inpa_inj_sched_prev_cumnoise=self.args.inpa_inj_sched_prev_cumnoise, dpm_solver=dpm_solver_256)\\n' \\\n",
    "'                    logger.log(\"sample_fine completed.\") \\n' \\\n",
    "'                    # Save images and calculate metrics\\n' \\\n",
    "'                    for i in range(self.config.sampling.batch_size):\\n' \\\n",
    "'                        os.makedirs(os.path.join(self.args.image_folder, \"gtImg\"), exist_ok=True)\\n' \\\n",
    "'                        os.makedirs(os.path.join(self.args.image_folder, \"inputImg\"), exist_ok=True)\\n' \\\n",
    "'                        os.makedirs(os.path.join(self.args.image_folder, \"sampledImg\"), exist_ok=True)\\n' \\\n",
    "'                        os.makedirs(os.path.join(self.args.image_folder, \"outImg\"), exist_ok=True)\\n' \\\n",
    "'                        os.makedirs(os.path.join(self.args.image_folder, \"coarseImg\"), exist_ok=True)\\n' \\\n",
    "'\\n' \\\n",
    "'                        out_gtImg_path = os.path.join(self.args.image_folder, \"gtImg\", f\"{str(count + i).zfill(4)}.png\")\\n' \\\n",
    "'                        out_inputImg_path = os.path.join(self.args.image_folder, \"inputImg\", f\"{str(count + i).zfill(4)}.png\")\\n' \\\n",
    "'                        out_sampledImg_path = os.path.join(self.args.image_folder, \"sampledImg\", f\"{str(count + i).zfill(4)}.png\")\\n' \\\n",
    "'                        out_outImg_path = os.path.join(self.args.image_folder, \"outImg\", f\"{str(count + i).zfill(4)}.png\")\\n' \\\n",
    "'                        out_coarseImg_path = os.path.join(self.args.image_folder, \"coarseImg\", f\"{str(count + i).zfill(4)}.png\")\\n' \\\n",
    "'\\n' \\\n",
    "'                        tmp_ones = torch.ones_like(gt[i]) * (-1)\\n' \\\n",
    "'                        inputImg = gt[i] * mask[i] + (1 - mask[i]) * tmp_ones\\n' \\\n",
    "'                        sampledImg = sample_fine[i].unsqueeze(0)\\n' \\\n",
    "'                        outImg = mask[i] * inputImg + (1 - mask[i]) * sampledImg\\n' \\\n",
    "'                        coarseImg = sample_coarse_256[i].unsqueeze(0)\\n' \\\n",
    "'                        out_coarseImg = mask[i] * inputImg + (1 - mask[i]) * coarseImg\\n' \\\n",
    "'                        gtImg = gt[i].reshape(outImg.shape).to(outImg.device)\\n' \\\n",
    "'\\n' \\\n",
    "'                        tvu.save_image(inverse_data_transform(self.config, gtImg), out_gtImg_path, nrow=1)\\n' \\\n",
    "'                        tvu.save_image(inverse_data_transform(self.config, inputImg), out_inputImg_path, nrow=1)\\n' \\\n",
    "'                        tvu.save_image(inverse_data_transform(self.config, sampledImg), out_sampledImg_path, nrow=1)\\n' \\\n",
    "'                        tvu.save_image(inverse_data_transform(self.config, outImg), out_outImg_path, nrow=1)\\n' \\\n",
    "'                        tvu.save_image(inverse_data_transform(self.config, out_coarseImg), out_coarseImg_path, nrow=1)\\n' \\\n",
    "'\\n' \\\n",
    "'                    count += self.config.sampling.batch_size\\n' \\\n",
    "'                    with open(metrics_file_path, \"a\") as metrics_file:\\n' \\\n",
    "'                        metrics_file.write(f\"Coarse {count} samples LPIPS: {coarse_lpips_value / count:.4f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"Coarse {count} samples PSNR: {coarse_psnr_value / count:.4f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"Coarse {count} samples SSIM: {coarse_ssim_value / count:.4f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"Coarse {count} samples L1(%): {coarse_l1_value / count * 100:.2f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"{count} samples LPIPS: {lpips_value / count:.4f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"{count} samples PSNR: {psnr_value / count:.4f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"{count} samples SSIM: {ssim_value / count:.4f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"{count} samples L1(%): {l1_value / count * 100:.2f}\\\\n\")\\n' \\\n",
    "'                        metrics_file.write(f\"\\\\n\")\\n' \\\n",
    "'\\n' \\\n",
    "'                    logger.log(f\"created {count} samples\")\\n' \\\n",
    "'\\n' \\\n",
    "'                logger.log(\"sampling complete\")\\n' \\\n",
    "'                end_time = time.time()\\n' \\\n",
    "'                total_time = end_time - start_time\\n' \\\n",
    "'                each_time = total_time / count\\n' \\\n",
    "'                logger.log(f\"Total time: {total_time}.\")\\n' \\\n",
    "'                logger.log(f\"Each time: {each_time}.\")\\n' \\\n",
    "'                #metrics_file.write(f\"{total_time}s for {count} samples\\\\n\")\\n' \\\n",
    "'                #metrics_file.write(f\"{each_time}s for 1 sample\\\\n\")\\n' \\\n",
    "'                #metrics_file.write(f\"\\\\n\")\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                raise NotImplementedError(\"Base sample or mask path not defined\")\\n' \\\n",
    "'\\n' \\\n",
    "'        if self.args.fid:\\n' \\\n",
    "'            if not os.path.exists(os.path.join(self.args.exp, \"fid.npy\")):\\n' \\\n",
    "'                self.sample_fid(model_64, model_256, classifier=classifier)\\n' \\\n",
    "'                torch.distributed.barrier()\\n' \\\n",
    "'                if self.rank == 0:\\n' \\\n",
    "'                    print(\"Computed FID...\")\\n' \\\n",
    "'                    #fid = calculate_fid_given_paths((self.config.sampling.fid_stats_dir, self.args.image_folder), batch_size=self.config.sampling.fid_batch_size, device=self.device, dims=2048, num_workers=4)\\n' \\\n",
    "'                    #print(\"FID: {}\".format(fid))\\n' \\\n",
    "'                    #np.save(os.path.join(self.args.exp, \"fid\"), fid)\\n' \\\n",
    "'        elif self.args.sample_only:\\n' \\\n",
    "'            self.sample_n_images(model, classifier=classifier)\\n' \\\n",
    "'            torch.distributed.barrier()\\n' \\\n",
    "'            if self.rank == 0:\\n' \\\n",
    "'                print(\"Begin to compute samples...\")\\n' \\\n",
    "'        #else:\\n' \\\n",
    "'            #raise NotImplementedError(\"Sample procedure not defined\")\\n' \\\n",
    "'    def sample_fid(self, model_64,model_256, classifier=None):\\n' \\\n",
    "'        pass\\n' \\\n",
    "'    def sample_image(self, x, model, timesteps, last=True, model_kwargs=None, model_mask_kwargs=None, inpa_inj_sched_prev=True, inpa_inj_sched_prev_cumnoise=False, dpm_solver_order=3, skip_type=\"time_uniform\", dpm_solver_method=\"singlestep\", dpm_solver_type=\"dpmsolver\", denoise=False, lower_order_final=False, thresholding=False, atol=0.0078, rtol=0.05, dpm_solver=None):\\n' \\\n",
    "'        assert last\\n' \\\n",
    "'        #config = self.config\\n' \\\n",
    "'        # DPM-Solver sampling\\n' \\\n",
    "'        if self.args.sample_type in [\"dpmsolver\", \"dpmsolver++\"]:\\n' \\\n",
    "'            x = dpm_solver.sample(\\n' \\\n",
    "'                x,\\n' \\\n",
    "'                steps=(timesteps - 1 if denoise else timesteps),\\n' \\\n",
    "'                order=dpm_solver_order,\\n' \\\n",
    "'                skip_type=skip_type,\\n' \\\n",
    "'                method=dpm_solver_method,\\n' \\\n",
    "'                solver_type=dpm_solver_type,\\n' \\\n",
    "'                lower_order_final=lower_order_final,\\n' \\\n",
    "'                denoise_to_zero=denoise,\\n' \\\n",
    "'                atol=atol,\\n' \\\n",
    "'                rtol=rtol,\\n' \\\n",
    "'                model_kwargs=model_kwargs,\\n' \\\n",
    "'                model_mask_kwargs=model_mask_kwargs,\\n' \\\n",
    "'                inpa_inj_sched_prev=inpa_inj_sched_prev,\\n' \\\n",
    "'                inpa_inj_sched_prev_cumnoise=inpa_inj_sched_prev_cumnoise\\n' \\\n",
    "'            )\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            raise NotImplementedError(f\"Sample type {self.args.sample_type} not supported\")\\n' \\\n",
    "'\\n' \\\n",
    "'        return x\\n' \\\n",
    "'    def test(self):\\n' \\\n",
    "'        pass\\n'\n",
    "with open('/kaggle/working/p2w/runners/diffusion.py', 'w') as f:\n",
    "    f.write(diffusion_str)\n",
    "print(\"Successfully wrote diffusion.py to /kaggle/working/p2w/runners/diffusion.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8cbae44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:54.860381Z",
     "iopub.status.busy": "2025-08-05T17:45:54.860102Z",
     "iopub.status.idle": "2025-08-05T17:45:54.871547Z",
     "shell.execute_reply": "2025-08-05T17:45:54.870691Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.029681,
     "end_time": "2025-08-05T17:45:54.873009",
     "exception": false,
     "start_time": "2025-08-05T17:45:54.843328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote main.py to /kaggle/working/p2w/main.py\n"
     ]
    }
   ],
   "source": [
    "main_str= 'import argparse\\n' \\\n",
    "'import traceback\\n' \\\n",
    "'import shutil\\n' \\\n",
    "'import logging\\n' \\\n",
    "'import yaml\\n' \\\n",
    "'import sys\\n' \\\n",
    "'import os\\n' \\\n",
    "'import torch\\n' \\\n",
    "'import numpy as np\\n' \\\n",
    "'import torch.utils.tensorboard as tb\\n' \\\n",
    "'import torch.distributed as dist\\n' \\\n",
    "'import torch.multiprocessing as mp\\n' \\\n",
    "'\\n' \\\n",
    "'from runners.diffusion import Diffusion\\n' \\\n",
    "'\\n' \\\n",
    "'torch.set_printoptions(sci_mode=False)\\n' \\\n",
    "'\\n' \\\n",
    "'def str2bool(v):\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\\n' \\\n",
    "'    \"\"\"\\n' \\\n",
    "'    if isinstance(v, bool):\\n' \\\n",
    "'        return v\\n' \\\n",
    "'    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\\n' \\\n",
    "'        return True\\n' \\\n",
    "'    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\\n' \\\n",
    "'        return False\\n' \\\n",
    "'    else:\\n' \\\n",
    "'        raise argparse.ArgumentTypeError(\"boolean value expected\")\\n' \\\n",
    "'\\n' \\\n",
    "'def parse_args_and_config():\\n' \\\n",
    "'    parser = argparse.ArgumentParser(description=globals()[\"__doc__\"])\\n' \\\n",
    "'\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--config\", type=str, required=True, help=\"Path to the config file\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\"--seed\", type=int, default=1234, help=\"Random seed\")\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--exp\", type=str, default=\"exp\", help=\"Path for saving running related data.\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--doc\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        required=False,\\n' \\\n",
    "'        default=\"\",\\n' \\\n",
    "'        help=\"A string for documentation purpose. Will be the name of the log folder.\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--comment\", type=str, default=\"\", help=\"A string for experiment comment\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--verbose\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=\"info\",\\n' \\\n",
    "'        help=\"Verbose level: info | debug | warning | critical\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\"--test\", action=\"store_true\", help=\"Whether to test the model\")\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--sample\",\\n' \\\n",
    "'        action=\"store_true\",\\n' \\\n",
    "'        help=\"Whether to produce samples from the model\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\"--fid\", action=\"store_true\")\\n' \\\n",
    "'    parser.add_argument(\"--interpolation\", action=\"store_true\")\\n' \\\n",
    "'    parser.add_argument(\"--sample_only\", action=\"store_true\")\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--resume_training\", action=\"store_true\", help=\"Whether to resume training\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"-i\",\\n' \\\n",
    "'        \"--image_folder\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=\"images\",\\n' \\\n",
    "'        help=\"The folder name of samples\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--ni\",\\n' \\\n",
    "'        action=\"store_true\",\\n' \\\n",
    "'        help=\"No interaction. Suitable for Slurm Job launcher\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--sample_type\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=\"generalized\",\\n' \\\n",
    "'        help=\"sampling approach (\\'generalized\\'(DDIM) or \\'ddpm_noisy\\'(DDPM) or \\'dpmsolver\\' or \\'dpmsolver++\\')\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--skip_type\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=\"time_uniform\",\\n' \\\n",
    "'        help=\"skip according to (\\'uniform\\' or \\'quadratic\\' for DDIM/DDPM; \\'logSNR\\' or \\'time_uniform\\' or \\'time_quadratic\\' for DPM-Solver)\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--base_samples\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=None,\\n' \\\n",
    "'        help=\"base samples for upsampling and inpainting, *.npz\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--timesteps\", type=int, default=1000, help=\"number of steps involved\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--timesteps_coarse\", type=int, default=1000, help=\"number of steps for coarse inpainting\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--dpm_solver_order\", type=int, default=3, help=\"order of dpm-solver\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--eta\",\\n' \\\n",
    "'        type=float,\\n' \\\n",
    "'        default=0.0,\\n' \\\n",
    "'        help=\"eta used to control the variances of sigma\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--fixed_class\", type=int, default=None, help=\"fixed class label for conditional sampling\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--dpm_solver_atol\", type=float, default=0.0078, help=\"atol for adaptive step size algorithm\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--dpm_solver_rtol\", type=float, default=0.05, help=\"rtol for adaptive step size algorithm\"\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--dpm_solver_method\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=\"singlestep\",\\n' \\\n",
    "'        help=\"method of dpm_solver (\\'adaptive\\' or \\'singlestep\\' or \\'multistep\\' or \\'singlestep_fixed\\'\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\\n' \\\n",
    "'        \"--dpm_solver_type\",\\n' \\\n",
    "'        type=str,\\n' \\\n",
    "'        default=\"dpm_solver\",\\n' \\\n",
    "'        help=\"type of dpm_solver (\\'dpm_solver\\' or \\'taylor\\'\",\\n' \\\n",
    "'    )\\n' \\\n",
    "'    parser.add_argument(\"--scale\", type=float, default=None)\\n' \\\n",
    "'    parser.add_argument(\"--denoise\", action=\"store_true\", default=False)\\n' \\\n",
    "'    parser.add_argument(\"--lower_order_final\", action=\"store_true\", default=False)\\n' \\\n",
    "'    parser.add_argument(\"--thresholding\", action=\"store_true\", default=False)\\n' \\\n",
    "'    parser.add_argument(\"--sequence\", action=\"store_true\")\\n' \\\n",
    "'    parser.add_argument(\"--port\", type=str, default=\"12355\")\\n' \\\n",
    "'    parser.add_argument(\"--p2_gamma\", type=float, default=1.0, help=\"P2 Weighing gamma parameter\")\\n' \\\n",
    "'    parser.add_argument(\"--p2_k\", type=float, default=1.0, help=\"P2 Weighing k parameter\")\\n' \\\n",
    "'    parser.add_argument(\"--mask_path\", type=str, default=None, help=\"path to mask images for inpainting, directory\")\\n' \\\n",
    "'    parser.add_argument(\"--inpa_inj_sched_prev\", action=\"store_true\", default=True, help=\"use previous schedule for inpainting injection\")\\n' \\\n",
    "'    parser.add_argument(\"--inpa_inj_sched_prev_cumnoise\", action=\"store_true\", default=False, help=\"use cumulative noise for inpainting injection\")\\n' \\\n",
    "'    parser.add_argument(\"--use_inverse_masks\", type=str2bool, default=False, help=\"invert mask values for inpainting\")\\n' \\\n",
    "'\\n' \\\n",
    "'    args = parser.parse_args()\\n' \\\n",
    "'    args.log_path = os.path.join(args.exp, \"logs\", args.doc)\\n' \\\n",
    "'\\n' \\\n",
    "'    # parse config file\\n' \\\n",
    "'    with open(os.path.join(\"configs\", args.config), \"r\") as f:\\n' \\\n",
    "'        config = yaml.safe_load(f)\\n' \\\n",
    "'    new_config = dict2namespace(config)\\n' \\\n",
    "'\\n' \\\n",
    "'    # Map model paths from config to args\\n' \\\n",
    "'    args.model_path_64 = config[\\'model\\'][\\'ckpt_dir_coarse\\']\\n' \\\n",
    "'    args.model_path_256 = config[\\'model\\'][\\'ckpt_dir_fine\\']\\n' \\\n",
    "'\\n' \\\n",
    "'    tb_path = os.path.join(args.exp, \"tensorboard\", args.doc)\\n' \\\n",
    "'\\n' \\\n",
    "'    if not args.test and not args.sample:\\n' \\\n",
    "'        if not args.resume_training:\\n' \\\n",
    "'            if os.path.exists(args.log_path):\\n' \\\n",
    "'                overwrite = False\\n' \\\n",
    "'                if args.ni:\\n' \\\n",
    "'                    overwrite = True\\n' \\\n",
    "'                else:\\n' \\\n",
    "'                    response = input(\"Folder already exists. Overwrite? (Y/N)\")\\n' \\\n",
    "'                    if response.upper() == \"Y\":\\n' \\\n",
    "'                        overwrite = True\\n' \\\n",
    "'\\n' \\\n",
    "'                if overwrite:\\n' \\\n",
    "'                    shutil.rmtree(args.log_path)\\n' \\\n",
    "'                    shutil.rmtree(tb_path)\\n' \\\n",
    "'                    os.makedirs(args.log_path)\\n' \\\n",
    "'                    if os.path.exists(tb_path):\\n' \\\n",
    "'                        shutil.rmtree(tb_path)\\n' \\\n",
    "'                else:\\n' \\\n",
    "'                    print(\"Folder exists. Program halted.\")\\n' \\\n",
    "'                    sys.exit(0)\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                os.makedirs(args.log_path)\\n' \\\n",
    "'\\n' \\\n",
    "'            with open(os.path.join(args.log_path, \"config.yml\"), \"w\") as f:\\n' \\\n",
    "'                yaml.dump(new_config, f, default_flow_style=False)\\n' \\\n",
    "'\\n' \\\n",
    "'        new_config.tb_logger = tb.SummaryWriter(log_dir=tb_path)\\n' \\\n",
    "'        # setup logger\\n' \\\n",
    "'        level = getattr(logging, args.verbose.upper(), None)\\n' \\\n",
    "'        if not isinstance(level, int):\\n' \\\n",
    "'            raise ValueError(\"level {} not supported\".format(args.verbose))\\n' \\\n",
    "'\\n' \\\n",
    "'        handler1 = logging.StreamHandler()\\n' \\\n",
    "'        handler2 = logging.FileHandler(os.path.join(args.log_path, \"stdout.txt\"))\\n' \\\n",
    "'        formatter = logging.Formatter(\\n' \\\n",
    "'            \"%(levelname)s - %(filename)s - %(asctime)s - %(message)s\"\\n' \\\n",
    "'        )\\n' \\\n",
    "'        handler1.setFormatter(formatter)\\n' \\\n",
    "'        handler2.setFormatter(formatter)\\n' \\\n",
    "'        logger = logging.getLogger()\\n' \\\n",
    "'        logger.addHandler(handler1)\\n' \\\n",
    "'        logger.addHandler(handler2)\\n' \\\n",
    "'        logger.setLevel(level)\\n' \\\n",
    "'\\n' \\\n",
    "'    else:\\n' \\\n",
    "'        level = getattr(logging, args.verbose.upper(), None)\\n' \\\n",
    "'        if not isinstance(level, int):\\n' \\\n",
    "'            raise ValueError(\"level {} not supported\".format(args.verbose))\\n' \\\n",
    "'\\n' \\\n",
    "'        handler1 = logging.StreamHandler()\\n' \\\n",
    "'        formatter = logging.Formatter(\\n' \\\n",
    "'            \"%(levelname)s - %(filename)s - %(asctime)s - %(message)s\"\\n' \\\n",
    "'        )\\n' \\\n",
    "'        handler1.setFormatter(formatter)\\n' \\\n",
    "'        logger = logging.getLogger()\\n' \\\n",
    "'        logger.addHandler(handler1)\\n' \\\n",
    "'        logger.setLevel(level)\\n' \\\n",
    "'\\n' \\\n",
    "'        if args.sample:\\n' \\\n",
    "'            os.makedirs(os.path.join(args.exp, \"image_samples\"), exist_ok=True)\\n' \\\n",
    "'            args.image_folder = os.path.join(\\n' \\\n",
    "'                args.exp, \"image_samples\", args.image_folder\\n' \\\n",
    "'            )\\n' \\\n",
    "'            if not os.path.exists(args.image_folder):\\n' \\\n",
    "'                os.makedirs(args.image_folder)\\n' \\\n",
    "'            else:\\n' \\\n",
    "'                if not (args.fid or args.interpolation or args.sample_only):\\n' \\\n",
    "'                    overwrite = False\\n' \\\n",
    "'                    if args.ni:\\n' \\\n",
    "'                        overwrite = True\\n' \\\n",
    "'                    else:\\n' \\\n",
    "'                        response = input(\\n' \\\n",
    "'                            f\"Image folder {args.image_folder} already exists. Overwrite? (Y/N)\"\\n' \\\n",
    "'                        )\\n' \\\n",
    "'                        if response.upper() == \"Y\":\\n' \\\n",
    "'                            overwrite = True\\n' \\\n",
    "'\\n' \\\n",
    "'                    if overwrite:\\n' \\\n",
    "'                        shutil.rmtree(args.image_folder)\\n' \\\n",
    "'                        os.makedirs(args.image_folder)\\n' \\\n",
    "'                    else:\\n' \\\n",
    "'                        print(\"Output image folder exists. Program halted.\")\\n' \\\n",
    "'                        sys.exit(0)\\n' \\\n",
    "'\\n' \\\n",
    "'    # add device\\n' \\\n",
    "'    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n' \\\n",
    "'    logging.info(\"Using device: {}\".format(device))\\n' \\\n",
    "'    new_config.device = device\\n' \\\n",
    "'\\n' \\\n",
    "'    torch.backends.cudnn.benchmark = True\\n' \\\n",
    "'\\n' \\\n",
    "'    return args, new_config\\n' \\\n",
    "'\\n' \\\n",
    "'def dict2namespace(config):\\n' \\\n",
    "'    namespace = argparse.Namespace()\\n' \\\n",
    "'    for key, value in config.items():\\n' \\\n",
    "'        if isinstance(value, dict):\\n' \\\n",
    "'            new_value = dict2namespace(value)\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            new_value = value\\n' \\\n",
    "'        setattr(namespace, key, new_value)\\n' \\\n",
    "'    return namespace\\n' \\\n",
    "'\\n' \\\n",
    "'def main():\\n' \\\n",
    "'    args, config = parse_args_and_config()\\n' \\\n",
    "'    logging.info(\"Writing log file to {}\".format(args.log_path))\\n' \\\n",
    "'    logging.info(\"Exp instance id = {}\".format(os.getpid()))\\n' \\\n",
    "'    logging.info(\"Exp comment = {}\".format(args.comment))\\n' \\\n",
    "'\\n' \\\n",
    "'    world_size = torch.cuda.device_count()\\n' \\\n",
    "'    mp.spawn(sample,\\n' \\\n",
    "'            args=(world_size, args, config),\\n' \\\n",
    "'            nprocs=world_size,\\n' \\\n",
    "'            join=True)\\n' \\\n",
    "'\\n' \\\n",
    "'def sample(rank, world_size, args, config):\\n' \\\n",
    "'    os.environ[\\'MASTER_ADDR\\'] = \\'localhost\\'\\n' \\\n",
    "'    os.environ[\\'MASTER_PORT\\'] = args.port\\n' \\\n",
    "'    # initialize the process group\\n' \\\n",
    "'    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\\n' \\\n",
    "'    # set random seed\\n' \\\n",
    "'    torch.manual_seed(args.seed + rank)\\n' \\\n",
    "'    np.random.seed(args.seed + rank)\\n' \\\n",
    "'    if torch.cuda.is_available():\\n' \\\n",
    "'        torch.cuda.manual_seed_all(args.seed + rank)\\n' \\\n",
    "'\\n' \\\n",
    "'    try:\\n' \\\n",
    "'        runner = Diffusion(args, config, rank=rank)\\n' \\\n",
    "'        if args.sample:\\n' \\\n",
    "'            runner.sample()\\n' \\\n",
    "'        elif args.test:\\n' \\\n",
    "'            runner.test()\\n' \\\n",
    "'        else:\\n' \\\n",
    "'            runner.train()\\n' \\\n",
    "'    except Exception:\\n' \\\n",
    "'        logging.error(traceback.format_exc())\\n' \\\n",
    "'    dist.destroy_process_group()\\n' \\\n",
    "'\\n' \\\n",
    "'if __name__ == \"__main__\":\\n' \\\n",
    "'    sys.exit(main())\\n'\n",
    "with open('/kaggle/working/p2w/main.py', 'w') as f:\n",
    "    f.write(main_str)\n",
    "print(\"Successfully wrote main.py to /kaggle/working/p2w/main.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f95b85cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:54.906241Z",
     "iopub.status.busy": "2025-08-05T17:45:54.905626Z",
     "iopub.status.idle": "2025-08-05T17:45:54.912088Z",
     "shell.execute_reply": "2025-08-05T17:45:54.911225Z"
    },
    "papermill": {
     "duration": 0.025063,
     "end_time": "2025-08-05T17:45:54.913295",
     "exception": false,
     "start_time": "2025-08-05T17:45:54.888232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote celebahq.yml to /kaggle/working/p2w/configs/celebahq.yml\n"
     ]
    }
   ],
   "source": [
    "yaml_content='''\n",
    "data:\n",
    "    dataset: \"CELEBA\"\n",
    "    category: null\n",
    "    #category not null for lsun\n",
    "    image_size: 256\n",
    "    channels: 3\n",
    "    logit_transform: false\n",
    "    # logit_transform not in p2w \n",
    "    uniform_dequantization: false\n",
    "    # uniform_dequantization not in p2w \n",
    "    gaussian_dequantization: false\n",
    "    #gaussian_dequantization not in p2w\n",
    "    random_flip: true\n",
    "    #random_flip matches p2w\n",
    "    rescaled: true\n",
    "    #rescaled matches p2w where this is hardcoded to always convert [0,255] to [-1,1]\n",
    "    num_workers: 8\n",
    "    #num_workers matches our mobile lab setup for p2w our checkpoint\n",
    "    num_classes: null\n",
    "    # was 1 originally but uncond image generation works with null.\n",
    "\n",
    "model:\n",
    "#only problem is matching num_heads to training checkpoint\n",
    "#no rescale_learned_sigma in dpmsolver unlike p2w. not needed for sampling but needed in training\n",
    "    model_type: \"p2-weighing\"\n",
    "    is_upsampling: false\n",
    "    image_size_coarse: 64\n",
    "    image_size_fine: 256\n",
    "    in_channels: 3\n",
    "    model_channels: 128\n",
    "    out_channels: 6\n",
    "    num_res_blocks: 1\n",
    "    attention_resolutions: [16]\n",
    "    attention_resolutions_coarse: [4]\n",
    "    dropout: 0.0\n",
    "    channel_mult_coarse: [1, 2, 3, 4]\n",
    "    channel_mult_fine: [1, 1, 2, 2, 4, 4]\n",
    "    conv_resample: true\n",
    "    dims: 2\n",
    "    num_classes: null\n",
    "    use_checkpoint: false\n",
    "    use_fp16: true\n",
    "    num_heads: 8\n",
    "    num_head_channels: 64\n",
    "    num_heads_upsample: -1\n",
    "    use_scale_shift_norm: true\n",
    "    resblock_updown: true\n",
    "    use_new_attention_order: false\n",
    "    var_type: fixedlarge\n",
    "    p2_gamma: 1\n",
    "    p2_k: 1\n",
    "    ema: true\n",
    "    ema_rate: 0.9999\n",
    "    ckpt_dir_coarse: \"./ddpm_ckpt/celeba/64_ema_0.9999_1000000.pt\"\n",
    "    ckpt_dir_fine: \"./ddpm_ckpt/celeba/256_ema_0.9999_1000000.pt\"\n",
    "    \n",
    "diffusion:\n",
    "#all matches p2w\n",
    "    beta_schedule: linear\n",
    "    beta_start: 0.0001\n",
    "    beta_end: 0.02\n",
    "    num_diffusion_timesteps: 1000\n",
    "\n",
    "sampling:\n",
    "    total_N: 1500\n",
    "    batch_size: 1\n",
    "    last_only: True\n",
    "    fid_stats_dir: \"./fid_stats/VIRTUAL_lsun_bedroom256.npz\"\n",
    "    fid_total_samples: 10\n",
    "    total_samples: 10 #for sample_n_images()\n",
    "    fid_batch_size: 1\n",
    "    cond_class: false\n",
    "    classifier_scale: 0.0\n",
    "    base_samples: \"./demo_split/celebahq/val\"\n",
    "    mask_path: \"./demo_split/mask/thick\"\n",
    "    inpa_inj_sched_prev: true\n",
    "    inpa_inj_sched_prev_cumnoise: false\n",
    "    use_inverse_masks: false\n",
    "    thresholding: false\n",
    "    n_sample: 1\n",
    "'''\n",
    "with open('/kaggle/working/p2w/configs/celebahq.yml', 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "print(\"Successfully wrote celebahq.yml to /kaggle/working/p2w/configs/celebahq.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6df62016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:54.946079Z",
     "iopub.status.busy": "2025-08-05T17:45:54.945755Z",
     "iopub.status.idle": "2025-08-05T17:45:54.951714Z",
     "shell.execute_reply": "2025-08-05T17:45:54.950769Z"
    },
    "papermill": {
     "duration": 0.024903,
     "end_time": "2025-08-05T17:45:54.953857",
     "exception": false,
     "start_time": "2025-08-05T17:45:54.928954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote inpaint.sh to /kaggle/working/p2w/inpaint.sh\n"
     ]
    }
   ],
   "source": [
    "inpaint_sh_content = '''#!/bin/bash\n",
    "data=\"celebahq\"\n",
    "sampleMethod=\"dpmsolver++\"\n",
    "type=\"dpmsolver\"\n",
    "steps=\"75\"  # Maps to t_T_fine\n",
    "steps_coarse=\"250\"  # Maps to t_T\n",
    "DIS=\"time_uniform\"\n",
    "order=\"3\"\n",
    "method=\"multistep\"\n",
    "workdir=\"experiments/\"$data\"/\"$steps_coarse\"_\"$steps\"_\"$DIS\"_type-\"$type\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python main.py \\\n",
    "  --config $data\".yml\" \\\n",
    "  --exp=$workdir \\\n",
    "  --sample \\\n",
    "  --timesteps=$steps \\\n",
    "  --timesteps_coarse=$steps_coarse \\\n",
    "  --eta 0 \\\n",
    "  --ni \\\n",
    "  --skip_type=$DIS \\\n",
    "  --sample_type=$sampleMethod \\\n",
    "  --dpm_solver_order=$order \\\n",
    "  --dpm_solver_method=$method \\\n",
    "  --dpm_solver_type=$type \\\n",
    "  --base_samples ./demo_split/celebahq/val \\\n",
    "  --mask_path ./demo_split/mask/thick \\\n",
    "  --use_inverse_masks False \\\n",
    "  --image_folder results/celeba/thick \\\n",
    "  --seed 1234 \\\n",
    "  --port 12355\n",
    "'''\n",
    "with open('/kaggle/working/p2w/inpaint.sh', 'w') as f:\n",
    "    f.write(inpaint_sh_content)\n",
    "print(\"Successfully wrote inpaint.sh to /kaggle/working/p2w/inpaint.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1922fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:55.000803Z",
     "iopub.status.busy": "2025-08-05T17:45:54.999707Z",
     "iopub.status.idle": "2025-08-05T17:45:55.133828Z",
     "shell.execute_reply": "2025-08-05T17:45:55.132131Z"
    },
    "papermill": {
     "duration": 0.163195,
     "end_time": "2025-08-05T17:45:55.136210",
     "exception": false,
     "start_time": "2025-08-05T17:45:54.973015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make it executable\n",
    "!chmod +x /kaggle/working/p2w/inpaint.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "386cbe46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T17:45:55.246078Z",
     "iopub.status.busy": "2025-08-05T17:45:55.245233Z",
     "iopub.status.idle": "2025-08-05T22:41:31.033088Z",
     "shell.execute_reply": "2025-08-05T22:41:31.032114Z"
    },
    "papermill": {
     "duration": 17735.807504,
     "end_time": "2025-08-05T22:41:31.034721",
     "exception": false,
     "start_time": "2025-08-05T17:45:55.227217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - main.py - 2025-08-05 17:46:00,340 - Using device: cuda\r\n",
      "INFO - main.py - 2025-08-05 17:46:00,341 - Writing log file to experiments/celebahq/250_75_time_uniform_type-dpmsolver/logs/\r\n",
      "INFO - main.py - 2025-08-05 17:46:00,341 - Exp instance id = 2594\r\n",
      "INFO - main.py - 2025-08-05 17:46:00,341 - Exp comment = \r\n",
      "Logging to experiments/celebahq/250_75_time_uniform_type-dpmsolver/image_samples/results/celeba/thick\r\n",
      "creating model...\r\n",
      "loading data...\r\n",
      "creating samples...\r\n",
      "sample_fine completed.\r\n",
      "created 1 samples\r\n",
      "sample_fine completed.\r\n",
      "created 2 samples\r\n",
      "sample_fine completed.\r\n",
      "created 3 samples\r\n",
      "sample_fine completed.\r\n",
      "created 4 samples\r\n",
      "sample_fine completed.\r\n",
      "created 5 samples\r\n",
      "sample_fine completed.\r\n",
      "created 6 samples\r\n",
      "sample_fine completed.\r\n",
      "created 7 samples\r\n",
      "sample_fine completed.\r\n",
      "created 8 samples\r\n",
      "sample_fine completed.\r\n",
      "created 9 samples\r\n",
      "sample_fine completed.\r\n",
      "created 10 samples\r\n",
      "sample_fine completed.\r\n",
      "created 11 samples\r\n",
      "sample_fine completed.\r\n",
      "created 12 samples\r\n",
      "sample_fine completed.\r\n",
      "created 13 samples\r\n",
      "sample_fine completed.\r\n",
      "created 14 samples\r\n",
      "sample_fine completed.\r\n",
      "created 15 samples\r\n",
      "sample_fine completed.\r\n",
      "created 16 samples\r\n",
      "sample_fine completed.\r\n",
      "created 17 samples\r\n",
      "sample_fine completed.\r\n",
      "created 18 samples\r\n",
      "sample_fine completed.\r\n",
      "created 19 samples\r\n",
      "sample_fine completed.\r\n",
      "created 20 samples\r\n",
      "sample_fine completed.\r\n",
      "created 21 samples\r\n",
      "sample_fine completed.\r\n",
      "created 22 samples\r\n",
      "sample_fine completed.\r\n",
      "created 23 samples\r\n",
      "sample_fine completed.\r\n",
      "created 24 samples\r\n",
      "sample_fine completed.\r\n",
      "created 25 samples\r\n",
      "sample_fine completed.\r\n",
      "created 26 samples\r\n",
      "sample_fine completed.\r\n",
      "created 27 samples\r\n",
      "sample_fine completed.\r\n",
      "created 28 samples\r\n",
      "sample_fine completed.\r\n",
      "created 29 samples\r\n",
      "sample_fine completed.\r\n",
      "created 30 samples\r\n",
      "sample_fine completed.\r\n",
      "created 31 samples\r\n",
      "sample_fine completed.\r\n",
      "created 32 samples\r\n",
      "sample_fine completed.\r\n",
      "created 33 samples\r\n",
      "sample_fine completed.\r\n",
      "created 34 samples\r\n",
      "sample_fine completed.\r\n",
      "created 35 samples\r\n",
      "sample_fine completed.\r\n",
      "created 36 samples\r\n",
      "sample_fine completed.\r\n",
      "created 37 samples\r\n",
      "sample_fine completed.\r\n",
      "created 38 samples\r\n",
      "sample_fine completed.\r\n",
      "created 39 samples\r\n",
      "sample_fine completed.\r\n",
      "created 40 samples\r\n",
      "sample_fine completed.\r\n",
      "created 41 samples\r\n",
      "sample_fine completed.\r\n",
      "created 42 samples\r\n",
      "sample_fine completed.\r\n",
      "created 43 samples\r\n",
      "sample_fine completed.\r\n",
      "created 44 samples\r\n",
      "sample_fine completed.\r\n",
      "created 45 samples\r\n",
      "sample_fine completed.\r\n",
      "created 46 samples\r\n",
      "sample_fine completed.\r\n",
      "created 47 samples\r\n",
      "sample_fine completed.\r\n",
      "created 48 samples\r\n",
      "sample_fine completed.\r\n",
      "created 49 samples\r\n",
      "sample_fine completed.\r\n",
      "created 50 samples\r\n",
      "sample_fine completed.\r\n",
      "created 51 samples\r\n",
      "sample_fine completed.\r\n",
      "created 52 samples\r\n",
      "sample_fine completed.\r\n",
      "created 53 samples\r\n",
      "sample_fine completed.\r\n",
      "created 54 samples\r\n",
      "sample_fine completed.\r\n",
      "created 55 samples\r\n",
      "sample_fine completed.\r\n",
      "created 56 samples\r\n",
      "sample_fine completed.\r\n",
      "created 57 samples\r\n",
      "sample_fine completed.\r\n",
      "created 58 samples\r\n",
      "sample_fine completed.\r\n",
      "created 59 samples\r\n",
      "sample_fine completed.\r\n",
      "created 60 samples\r\n",
      "sample_fine completed.\r\n",
      "created 61 samples\r\n",
      "sample_fine completed.\r\n",
      "created 62 samples\r\n",
      "sample_fine completed.\r\n",
      "created 63 samples\r\n",
      "sample_fine completed.\r\n",
      "created 64 samples\r\n",
      "sample_fine completed.\r\n",
      "created 65 samples\r\n",
      "sample_fine completed.\r\n",
      "created 66 samples\r\n",
      "sample_fine completed.\r\n",
      "created 67 samples\r\n",
      "sample_fine completed.\r\n",
      "created 68 samples\r\n",
      "sample_fine completed.\r\n",
      "created 69 samples\r\n",
      "sample_fine completed.\r\n",
      "created 70 samples\r\n",
      "sample_fine completed.\r\n",
      "created 71 samples\r\n",
      "sample_fine completed.\r\n",
      "created 72 samples\r\n",
      "sample_fine completed.\r\n",
      "created 73 samples\r\n",
      "sample_fine completed.\r\n",
      "created 74 samples\r\n",
      "sample_fine completed.\r\n",
      "created 75 samples\r\n",
      "sample_fine completed.\r\n",
      "created 76 samples\r\n",
      "sample_fine completed.\r\n",
      "created 77 samples\r\n",
      "sample_fine completed.\r\n",
      "created 78 samples\r\n",
      "sample_fine completed.\r\n",
      "created 79 samples\r\n",
      "sample_fine completed.\r\n",
      "created 80 samples\r\n",
      "sample_fine completed.\r\n",
      "created 81 samples\r\n",
      "sample_fine completed.\r\n",
      "created 82 samples\r\n",
      "sample_fine completed.\r\n",
      "created 83 samples\r\n",
      "sample_fine completed.\r\n",
      "created 84 samples\r\n",
      "sample_fine completed.\r\n",
      "created 85 samples\r\n",
      "sample_fine completed.\r\n",
      "created 86 samples\r\n",
      "sample_fine completed.\r\n",
      "created 87 samples\r\n",
      "sample_fine completed.\r\n",
      "created 88 samples\r\n",
      "sample_fine completed.\r\n",
      "created 89 samples\r\n",
      "sample_fine completed.\r\n",
      "created 90 samples\r\n",
      "sample_fine completed.\r\n",
      "created 91 samples\r\n",
      "sample_fine completed.\r\n",
      "created 92 samples\r\n",
      "sample_fine completed.\r\n",
      "created 93 samples\r\n",
      "sample_fine completed.\r\n",
      "created 94 samples\r\n",
      "sample_fine completed.\r\n",
      "created 95 samples\r\n",
      "sample_fine completed.\r\n",
      "created 96 samples\r\n",
      "sample_fine completed.\r\n",
      "created 97 samples\r\n",
      "sample_fine completed.\r\n",
      "created 98 samples\r\n",
      "sample_fine completed.\r\n",
      "created 99 samples\r\n",
      "sample_fine completed.\r\n",
      "created 100 samples\r\n",
      "sample_fine completed.\r\n",
      "created 101 samples\r\n",
      "sample_fine completed.\r\n",
      "created 102 samples\r\n",
      "sample_fine completed.\r\n",
      "created 103 samples\r\n",
      "sample_fine completed.\r\n",
      "created 104 samples\r\n",
      "sample_fine completed.\r\n",
      "created 105 samples\r\n",
      "sample_fine completed.\r\n",
      "created 106 samples\r\n",
      "sample_fine completed.\r\n",
      "created 107 samples\r\n",
      "sample_fine completed.\r\n",
      "created 108 samples\r\n",
      "sample_fine completed.\r\n",
      "created 109 samples\r\n",
      "sample_fine completed.\r\n",
      "created 110 samples\r\n",
      "sample_fine completed.\r\n",
      "created 111 samples\r\n",
      "sample_fine completed.\r\n",
      "created 112 samples\r\n",
      "sample_fine completed.\r\n",
      "created 113 samples\r\n",
      "sample_fine completed.\r\n",
      "created 114 samples\r\n",
      "sample_fine completed.\r\n",
      "created 115 samples\r\n",
      "sample_fine completed.\r\n",
      "created 116 samples\r\n",
      "sample_fine completed.\r\n",
      "created 117 samples\r\n",
      "sample_fine completed.\r\n",
      "created 118 samples\r\n",
      "sample_fine completed.\r\n",
      "created 119 samples\r\n",
      "sample_fine completed.\r\n",
      "created 120 samples\r\n",
      "sample_fine completed.\r\n",
      "created 121 samples\r\n",
      "sample_fine completed.\r\n",
      "created 122 samples\r\n",
      "sample_fine completed.\r\n",
      "created 123 samples\r\n",
      "sample_fine completed.\r\n",
      "created 124 samples\r\n",
      "sample_fine completed.\r\n",
      "created 125 samples\r\n",
      "sample_fine completed.\r\n",
      "created 126 samples\r\n",
      "sample_fine completed.\r\n",
      "created 127 samples\r\n",
      "sample_fine completed.\r\n",
      "created 128 samples\r\n",
      "sample_fine completed.\r\n",
      "created 129 samples\r\n",
      "sample_fine completed.\r\n",
      "created 130 samples\r\n",
      "sample_fine completed.\r\n",
      "created 131 samples\r\n",
      "sample_fine completed.\r\n",
      "created 132 samples\r\n",
      "sample_fine completed.\r\n",
      "created 133 samples\r\n",
      "sample_fine completed.\r\n",
      "created 134 samples\r\n",
      "sample_fine completed.\r\n",
      "created 135 samples\r\n",
      "sample_fine completed.\r\n",
      "created 136 samples\r\n",
      "sample_fine completed.\r\n",
      "created 137 samples\r\n",
      "sample_fine completed.\r\n",
      "created 138 samples\r\n",
      "sample_fine completed.\r\n",
      "created 139 samples\r\n",
      "sample_fine completed.\r\n",
      "created 140 samples\r\n",
      "sample_fine completed.\r\n",
      "created 141 samples\r\n",
      "sample_fine completed.\r\n",
      "created 142 samples\r\n",
      "sample_fine completed.\r\n",
      "created 143 samples\r\n",
      "sample_fine completed.\r\n",
      "created 144 samples\r\n",
      "sample_fine completed.\r\n",
      "created 145 samples\r\n",
      "sample_fine completed.\r\n",
      "created 146 samples\r\n",
      "sample_fine completed.\r\n",
      "created 147 samples\r\n",
      "sample_fine completed.\r\n",
      "created 148 samples\r\n",
      "sample_fine completed.\r\n",
      "created 149 samples\r\n",
      "sample_fine completed.\r\n",
      "created 150 samples\r\n",
      "sample_fine completed.\r\n",
      "created 151 samples\r\n",
      "sample_fine completed.\r\n",
      "created 152 samples\r\n",
      "sample_fine completed.\r\n",
      "created 153 samples\r\n",
      "sample_fine completed.\r\n",
      "created 154 samples\r\n",
      "sample_fine completed.\r\n",
      "created 155 samples\r\n",
      "sample_fine completed.\r\n",
      "created 156 samples\r\n",
      "sample_fine completed.\r\n",
      "created 157 samples\r\n",
      "sample_fine completed.\r\n",
      "created 158 samples\r\n",
      "sample_fine completed.\r\n",
      "created 159 samples\r\n",
      "sample_fine completed.\r\n",
      "created 160 samples\r\n",
      "sample_fine completed.\r\n",
      "created 161 samples\r\n",
      "sample_fine completed.\r\n",
      "created 162 samples\r\n",
      "sample_fine completed.\r\n",
      "created 163 samples\r\n",
      "sample_fine completed.\r\n",
      "created 164 samples\r\n",
      "sample_fine completed.\r\n",
      "created 165 samples\r\n",
      "sample_fine completed.\r\n",
      "created 166 samples\r\n",
      "sample_fine completed.\r\n",
      "created 167 samples\r\n",
      "sample_fine completed.\r\n",
      "created 168 samples\r\n",
      "sample_fine completed.\r\n",
      "created 169 samples\r\n",
      "sample_fine completed.\r\n",
      "created 170 samples\r\n",
      "sample_fine completed.\r\n",
      "created 171 samples\r\n",
      "sample_fine completed.\r\n",
      "created 172 samples\r\n",
      "sample_fine completed.\r\n",
      "created 173 samples\r\n",
      "sample_fine completed.\r\n",
      "created 174 samples\r\n",
      "sample_fine completed.\r\n",
      "created 175 samples\r\n",
      "sample_fine completed.\r\n",
      "created 176 samples\r\n",
      "sample_fine completed.\r\n",
      "created 177 samples\r\n",
      "sample_fine completed.\r\n",
      "created 178 samples\r\n",
      "sample_fine completed.\r\n",
      "created 179 samples\r\n",
      "sample_fine completed.\r\n",
      "created 180 samples\r\n",
      "sample_fine completed.\r\n",
      "created 181 samples\r\n",
      "sample_fine completed.\r\n",
      "created 182 samples\r\n",
      "sample_fine completed.\r\n",
      "created 183 samples\r\n",
      "sample_fine completed.\r\n",
      "created 184 samples\r\n",
      "sample_fine completed.\r\n",
      "created 185 samples\r\n",
      "sample_fine completed.\r\n",
      "created 186 samples\r\n",
      "sample_fine completed.\r\n",
      "created 187 samples\r\n",
      "sample_fine completed.\r\n",
      "created 188 samples\r\n",
      "sample_fine completed.\r\n",
      "created 189 samples\r\n",
      "sample_fine completed.\r\n",
      "created 190 samples\r\n",
      "sample_fine completed.\r\n",
      "created 191 samples\r\n",
      "sample_fine completed.\r\n",
      "created 192 samples\r\n",
      "sample_fine completed.\r\n",
      "created 193 samples\r\n",
      "sample_fine completed.\r\n",
      "created 194 samples\r\n",
      "sample_fine completed.\r\n",
      "created 195 samples\r\n",
      "sample_fine completed.\r\n",
      "created 196 samples\r\n",
      "sample_fine completed.\r\n",
      "created 197 samples\r\n",
      "sample_fine completed.\r\n",
      "created 198 samples\r\n",
      "sample_fine completed.\r\n",
      "created 199 samples\r\n",
      "sample_fine completed.\r\n",
      "created 200 samples\r\n",
      "sample_fine completed.\r\n",
      "created 201 samples\r\n",
      "sample_fine completed.\r\n",
      "created 202 samples\r\n",
      "sample_fine completed.\r\n",
      "created 203 samples\r\n",
      "sample_fine completed.\r\n",
      "created 204 samples\r\n",
      "sample_fine completed.\r\n",
      "created 205 samples\r\n",
      "sample_fine completed.\r\n",
      "created 206 samples\r\n",
      "sample_fine completed.\r\n",
      "created 207 samples\r\n",
      "sample_fine completed.\r\n",
      "created 208 samples\r\n",
      "sample_fine completed.\r\n",
      "created 209 samples\r\n",
      "sample_fine completed.\r\n",
      "created 210 samples\r\n",
      "sample_fine completed.\r\n",
      "created 211 samples\r\n",
      "sample_fine completed.\r\n",
      "created 212 samples\r\n",
      "sample_fine completed.\r\n",
      "created 213 samples\r\n",
      "sample_fine completed.\r\n",
      "created 214 samples\r\n",
      "sample_fine completed.\r\n",
      "created 215 samples\r\n",
      "sample_fine completed.\r\n",
      "created 216 samples\r\n",
      "sample_fine completed.\r\n",
      "created 217 samples\r\n",
      "sample_fine completed.\r\n",
      "created 218 samples\r\n",
      "sample_fine completed.\r\n",
      "created 219 samples\r\n",
      "sample_fine completed.\r\n",
      "created 220 samples\r\n",
      "sample_fine completed.\r\n",
      "created 221 samples\r\n",
      "sample_fine completed.\r\n",
      "created 222 samples\r\n",
      "sample_fine completed.\r\n",
      "created 223 samples\r\n",
      "sample_fine completed.\r\n",
      "created 224 samples\r\n",
      "sample_fine completed.\r\n",
      "created 225 samples\r\n",
      "sample_fine completed.\r\n",
      "created 226 samples\r\n",
      "sample_fine completed.\r\n",
      "created 227 samples\r\n",
      "sample_fine completed.\r\n",
      "created 228 samples\r\n",
      "sample_fine completed.\r\n",
      "created 229 samples\r\n",
      "sample_fine completed.\r\n",
      "created 230 samples\r\n",
      "sample_fine completed.\r\n",
      "created 231 samples\r\n",
      "sample_fine completed.\r\n",
      "created 232 samples\r\n",
      "sample_fine completed.\r\n",
      "created 233 samples\r\n",
      "sample_fine completed.\r\n",
      "created 234 samples\r\n",
      "sample_fine completed.\r\n",
      "created 235 samples\r\n",
      "sample_fine completed.\r\n",
      "created 236 samples\r\n",
      "sample_fine completed.\r\n",
      "created 237 samples\r\n",
      "sample_fine completed.\r\n",
      "created 238 samples\r\n",
      "sample_fine completed.\r\n",
      "created 239 samples\r\n",
      "sample_fine completed.\r\n",
      "created 240 samples\r\n",
      "sample_fine completed.\r\n",
      "created 241 samples\r\n",
      "sample_fine completed.\r\n",
      "created 242 samples\r\n",
      "sample_fine completed.\r\n",
      "created 243 samples\r\n",
      "sample_fine completed.\r\n",
      "created 244 samples\r\n",
      "sample_fine completed.\r\n",
      "created 245 samples\r\n",
      "sample_fine completed.\r\n",
      "created 246 samples\r\n",
      "sample_fine completed.\r\n",
      "created 247 samples\r\n",
      "sample_fine completed.\r\n",
      "created 248 samples\r\n",
      "sample_fine completed.\r\n",
      "created 249 samples\r\n",
      "sample_fine completed.\r\n",
      "created 250 samples\r\n",
      "sample_fine completed.\r\n",
      "created 251 samples\r\n",
      "sample_fine completed.\r\n",
      "created 252 samples\r\n",
      "sample_fine completed.\r\n",
      "created 253 samples\r\n",
      "sample_fine completed.\r\n",
      "created 254 samples\r\n",
      "sample_fine completed.\r\n",
      "created 255 samples\r\n",
      "sample_fine completed.\r\n",
      "created 256 samples\r\n",
      "sample_fine completed.\r\n",
      "created 257 samples\r\n",
      "sample_fine completed.\r\n",
      "created 258 samples\r\n",
      "sample_fine completed.\r\n",
      "created 259 samples\r\n",
      "sample_fine completed.\r\n",
      "created 260 samples\r\n",
      "sample_fine completed.\r\n",
      "created 261 samples\r\n",
      "sample_fine completed.\r\n",
      "created 262 samples\r\n",
      "sample_fine completed.\r\n",
      "created 263 samples\r\n",
      "sample_fine completed.\r\n",
      "created 264 samples\r\n",
      "sample_fine completed.\r\n",
      "created 265 samples\r\n",
      "sample_fine completed.\r\n",
      "created 266 samples\r\n",
      "sample_fine completed.\r\n",
      "created 267 samples\r\n",
      "sample_fine completed.\r\n",
      "created 268 samples\r\n",
      "sample_fine completed.\r\n",
      "created 269 samples\r\n",
      "sample_fine completed.\r\n",
      "created 270 samples\r\n",
      "sample_fine completed.\r\n",
      "created 271 samples\r\n",
      "sample_fine completed.\r\n",
      "created 272 samples\r\n",
      "sample_fine completed.\r\n",
      "created 273 samples\r\n",
      "sample_fine completed.\r\n",
      "created 274 samples\r\n",
      "sample_fine completed.\r\n",
      "created 275 samples\r\n",
      "sample_fine completed.\r\n",
      "created 276 samples\r\n",
      "sample_fine completed.\r\n",
      "created 277 samples\r\n",
      "sample_fine completed.\r\n",
      "created 278 samples\r\n",
      "sample_fine completed.\r\n",
      "created 279 samples\r\n",
      "sample_fine completed.\r\n",
      "created 280 samples\r\n",
      "sample_fine completed.\r\n",
      "created 281 samples\r\n",
      "sample_fine completed.\r\n",
      "created 282 samples\r\n",
      "sample_fine completed.\r\n",
      "created 283 samples\r\n",
      "sample_fine completed.\r\n",
      "created 284 samples\r\n",
      "sample_fine completed.\r\n",
      "created 285 samples\r\n",
      "sample_fine completed.\r\n",
      "created 286 samples\r\n",
      "sample_fine completed.\r\n",
      "created 287 samples\r\n",
      "sample_fine completed.\r\n",
      "created 288 samples\r\n",
      "sample_fine completed.\r\n",
      "created 289 samples\r\n",
      "sample_fine completed.\r\n",
      "created 290 samples\r\n",
      "sample_fine completed.\r\n",
      "created 291 samples\r\n",
      "sample_fine completed.\r\n",
      "created 292 samples\r\n",
      "sample_fine completed.\r\n",
      "created 293 samples\r\n",
      "sample_fine completed.\r\n",
      "created 294 samples\r\n",
      "sample_fine completed.\r\n",
      "created 295 samples\r\n",
      "sample_fine completed.\r\n",
      "created 296 samples\r\n",
      "sample_fine completed.\r\n",
      "created 297 samples\r\n",
      "sample_fine completed.\r\n",
      "created 298 samples\r\n",
      "sample_fine completed.\r\n",
      "created 299 samples\r\n",
      "sample_fine completed.\r\n",
      "created 300 samples\r\n",
      "sample_fine completed.\r\n",
      "created 301 samples\r\n",
      "sample_fine completed.\r\n",
      "created 302 samples\r\n",
      "sample_fine completed.\r\n",
      "created 303 samples\r\n",
      "sample_fine completed.\r\n",
      "created 304 samples\r\n",
      "sample_fine completed.\r\n",
      "created 305 samples\r\n",
      "sample_fine completed.\r\n",
      "created 306 samples\r\n",
      "sample_fine completed.\r\n",
      "created 307 samples\r\n",
      "sample_fine completed.\r\n",
      "created 308 samples\r\n",
      "sample_fine completed.\r\n",
      "created 309 samples\r\n",
      "sample_fine completed.\r\n",
      "created 310 samples\r\n",
      "sample_fine completed.\r\n",
      "created 311 samples\r\n",
      "sample_fine completed.\r\n",
      "created 312 samples\r\n",
      "sample_fine completed.\r\n",
      "created 313 samples\r\n",
      "sample_fine completed.\r\n",
      "created 314 samples\r\n",
      "sample_fine completed.\r\n",
      "created 315 samples\r\n",
      "sample_fine completed.\r\n",
      "created 316 samples\r\n",
      "sample_fine completed.\r\n",
      "created 317 samples\r\n",
      "sample_fine completed.\r\n",
      "created 318 samples\r\n",
      "sample_fine completed.\r\n",
      "created 319 samples\r\n",
      "sample_fine completed.\r\n",
      "created 320 samples\r\n",
      "sample_fine completed.\r\n",
      "created 321 samples\r\n",
      "sample_fine completed.\r\n",
      "created 322 samples\r\n",
      "sample_fine completed.\r\n",
      "created 323 samples\r\n",
      "sample_fine completed.\r\n",
      "created 324 samples\r\n",
      "sample_fine completed.\r\n",
      "created 325 samples\r\n",
      "sample_fine completed.\r\n",
      "created 326 samples\r\n",
      "sample_fine completed.\r\n",
      "created 327 samples\r\n",
      "sample_fine completed.\r\n",
      "created 328 samples\r\n",
      "sample_fine completed.\r\n",
      "created 329 samples\r\n",
      "sample_fine completed.\r\n",
      "created 330 samples\r\n",
      "sample_fine completed.\r\n",
      "created 331 samples\r\n",
      "sample_fine completed.\r\n",
      "created 332 samples\r\n",
      "sample_fine completed.\r\n",
      "created 333 samples\r\n",
      "sample_fine completed.\r\n",
      "created 334 samples\r\n",
      "sample_fine completed.\r\n",
      "created 335 samples\r\n",
      "sample_fine completed.\r\n",
      "created 336 samples\r\n",
      "sample_fine completed.\r\n",
      "created 337 samples\r\n",
      "sample_fine completed.\r\n",
      "created 338 samples\r\n",
      "sample_fine completed.\r\n",
      "created 339 samples\r\n",
      "sample_fine completed.\r\n",
      "created 340 samples\r\n",
      "sample_fine completed.\r\n",
      "created 341 samples\r\n",
      "sample_fine completed.\r\n",
      "created 342 samples\r\n",
      "sample_fine completed.\r\n",
      "created 343 samples\r\n",
      "sample_fine completed.\r\n",
      "created 344 samples\r\n",
      "sample_fine completed.\r\n",
      "created 345 samples\r\n",
      "sample_fine completed.\r\n",
      "created 346 samples\r\n",
      "sample_fine completed.\r\n",
      "created 347 samples\r\n",
      "sample_fine completed.\r\n",
      "created 348 samples\r\n",
      "sample_fine completed.\r\n",
      "created 349 samples\r\n",
      "sample_fine completed.\r\n",
      "created 350 samples\r\n",
      "sample_fine completed.\r\n",
      "created 351 samples\r\n",
      "sample_fine completed.\r\n",
      "created 352 samples\r\n",
      "sample_fine completed.\r\n",
      "created 353 samples\r\n",
      "sample_fine completed.\r\n",
      "created 354 samples\r\n",
      "sample_fine completed.\r\n",
      "created 355 samples\r\n",
      "sample_fine completed.\r\n",
      "created 356 samples\r\n",
      "sample_fine completed.\r\n",
      "created 357 samples\r\n",
      "sample_fine completed.\r\n",
      "created 358 samples\r\n",
      "sample_fine completed.\r\n",
      "created 359 samples\r\n",
      "sample_fine completed.\r\n",
      "created 360 samples\r\n",
      "sample_fine completed.\r\n",
      "created 361 samples\r\n",
      "sample_fine completed.\r\n",
      "created 362 samples\r\n",
      "sample_fine completed.\r\n",
      "created 363 samples\r\n",
      "sample_fine completed.\r\n",
      "created 364 samples\r\n",
      "sample_fine completed.\r\n",
      "created 365 samples\r\n",
      "sample_fine completed.\r\n",
      "created 366 samples\r\n",
      "sample_fine completed.\r\n",
      "created 367 samples\r\n",
      "sample_fine completed.\r\n",
      "created 368 samples\r\n",
      "sample_fine completed.\r\n",
      "created 369 samples\r\n",
      "sample_fine completed.\r\n",
      "created 370 samples\r\n",
      "sample_fine completed.\r\n",
      "created 371 samples\r\n",
      "sample_fine completed.\r\n",
      "created 372 samples\r\n",
      "sample_fine completed.\r\n",
      "created 373 samples\r\n",
      "sample_fine completed.\r\n",
      "created 374 samples\r\n",
      "sample_fine completed.\r\n",
      "created 375 samples\r\n",
      "sample_fine completed.\r\n",
      "created 376 samples\r\n",
      "sample_fine completed.\r\n",
      "created 377 samples\r\n",
      "sample_fine completed.\r\n",
      "created 378 samples\r\n",
      "sample_fine completed.\r\n",
      "created 379 samples\r\n",
      "sample_fine completed.\r\n",
      "created 380 samples\r\n",
      "sample_fine completed.\r\n",
      "created 381 samples\r\n",
      "sample_fine completed.\r\n",
      "created 382 samples\r\n",
      "sample_fine completed.\r\n",
      "created 383 samples\r\n",
      "sample_fine completed.\r\n",
      "created 384 samples\r\n",
      "sample_fine completed.\r\n",
      "created 385 samples\r\n",
      "sample_fine completed.\r\n",
      "created 386 samples\r\n",
      "sample_fine completed.\r\n",
      "created 387 samples\r\n",
      "sample_fine completed.\r\n",
      "created 388 samples\r\n",
      "sample_fine completed.\r\n",
      "created 389 samples\r\n",
      "sample_fine completed.\r\n",
      "created 390 samples\r\n",
      "sample_fine completed.\r\n",
      "created 391 samples\r\n",
      "sample_fine completed.\r\n",
      "created 392 samples\r\n",
      "sample_fine completed.\r\n",
      "created 393 samples\r\n",
      "sample_fine completed.\r\n",
      "created 394 samples\r\n",
      "sample_fine completed.\r\n",
      "created 395 samples\r\n",
      "sample_fine completed.\r\n",
      "created 396 samples\r\n",
      "sample_fine completed.\r\n",
      "created 397 samples\r\n",
      "sample_fine completed.\r\n",
      "created 398 samples\r\n",
      "sample_fine completed.\r\n",
      "created 399 samples\r\n",
      "sample_fine completed.\r\n",
      "created 400 samples\r\n",
      "sample_fine completed.\r\n",
      "created 401 samples\r\n",
      "sample_fine completed.\r\n",
      "created 402 samples\r\n",
      "sample_fine completed.\r\n",
      "created 403 samples\r\n",
      "sample_fine completed.\r\n",
      "created 404 samples\r\n",
      "sample_fine completed.\r\n",
      "created 405 samples\r\n",
      "sample_fine completed.\r\n",
      "created 406 samples\r\n",
      "sample_fine completed.\r\n",
      "created 407 samples\r\n",
      "sample_fine completed.\r\n",
      "created 408 samples\r\n",
      "sample_fine completed.\r\n",
      "created 409 samples\r\n",
      "sample_fine completed.\r\n",
      "created 410 samples\r\n",
      "sample_fine completed.\r\n",
      "created 411 samples\r\n",
      "sample_fine completed.\r\n",
      "created 412 samples\r\n",
      "sample_fine completed.\r\n",
      "created 413 samples\r\n",
      "sample_fine completed.\r\n",
      "created 414 samples\r\n",
      "sample_fine completed.\r\n",
      "created 415 samples\r\n",
      "sample_fine completed.\r\n",
      "created 416 samples\r\n",
      "sample_fine completed.\r\n",
      "created 417 samples\r\n",
      "sample_fine completed.\r\n",
      "created 418 samples\r\n",
      "sample_fine completed.\r\n",
      "created 419 samples\r\n",
      "sample_fine completed.\r\n",
      "created 420 samples\r\n",
      "sample_fine completed.\r\n",
      "created 421 samples\r\n",
      "sample_fine completed.\r\n",
      "created 422 samples\r\n",
      "sample_fine completed.\r\n",
      "created 423 samples\r\n",
      "sample_fine completed.\r\n",
      "created 424 samples\r\n",
      "sample_fine completed.\r\n",
      "created 425 samples\r\n",
      "sample_fine completed.\r\n",
      "created 426 samples\r\n",
      "sample_fine completed.\r\n",
      "created 427 samples\r\n",
      "sample_fine completed.\r\n",
      "created 428 samples\r\n",
      "sample_fine completed.\r\n",
      "created 429 samples\r\n",
      "sample_fine completed.\r\n",
      "created 430 samples\r\n",
      "sample_fine completed.\r\n",
      "created 431 samples\r\n",
      "sample_fine completed.\r\n",
      "created 432 samples\r\n",
      "sample_fine completed.\r\n",
      "created 433 samples\r\n",
      "sample_fine completed.\r\n",
      "created 434 samples\r\n",
      "sample_fine completed.\r\n",
      "created 435 samples\r\n",
      "sample_fine completed.\r\n",
      "created 436 samples\r\n",
      "sample_fine completed.\r\n",
      "created 437 samples\r\n",
      "sample_fine completed.\r\n",
      "created 438 samples\r\n",
      "sample_fine completed.\r\n",
      "created 439 samples\r\n",
      "sample_fine completed.\r\n",
      "created 440 samples\r\n",
      "sample_fine completed.\r\n",
      "created 441 samples\r\n",
      "sample_fine completed.\r\n",
      "created 442 samples\r\n",
      "sample_fine completed.\r\n",
      "created 443 samples\r\n",
      "sample_fine completed.\r\n",
      "created 444 samples\r\n",
      "sample_fine completed.\r\n",
      "created 445 samples\r\n",
      "sample_fine completed.\r\n",
      "created 446 samples\r\n",
      "sample_fine completed.\r\n",
      "created 447 samples\r\n",
      "sample_fine completed.\r\n",
      "created 448 samples\r\n",
      "sample_fine completed.\r\n",
      "created 449 samples\r\n",
      "sample_fine completed.\r\n",
      "created 450 samples\r\n",
      "sample_fine completed.\r\n",
      "created 451 samples\r\n",
      "sample_fine completed.\r\n",
      "created 452 samples\r\n",
      "sample_fine completed.\r\n",
      "created 453 samples\r\n",
      "sample_fine completed.\r\n",
      "created 454 samples\r\n",
      "sample_fine completed.\r\n",
      "created 455 samples\r\n",
      "sample_fine completed.\r\n",
      "created 456 samples\r\n",
      "sample_fine completed.\r\n",
      "created 457 samples\r\n",
      "sample_fine completed.\r\n",
      "created 458 samples\r\n",
      "sample_fine completed.\r\n",
      "created 459 samples\r\n",
      "sample_fine completed.\r\n",
      "created 460 samples\r\n",
      "sample_fine completed.\r\n",
      "created 461 samples\r\n",
      "sample_fine completed.\r\n",
      "created 462 samples\r\n",
      "sample_fine completed.\r\n",
      "created 463 samples\r\n",
      "sample_fine completed.\r\n",
      "created 464 samples\r\n",
      "sample_fine completed.\r\n",
      "created 465 samples\r\n",
      "sample_fine completed.\r\n",
      "created 466 samples\r\n",
      "sample_fine completed.\r\n",
      "created 467 samples\r\n",
      "sample_fine completed.\r\n",
      "created 468 samples\r\n",
      "sample_fine completed.\r\n",
      "created 469 samples\r\n",
      "sample_fine completed.\r\n",
      "created 470 samples\r\n",
      "sample_fine completed.\r\n",
      "created 471 samples\r\n",
      "sample_fine completed.\r\n",
      "created 472 samples\r\n",
      "sample_fine completed.\r\n",
      "created 473 samples\r\n",
      "sample_fine completed.\r\n",
      "created 474 samples\r\n",
      "sample_fine completed.\r\n",
      "created 475 samples\r\n",
      "sample_fine completed.\r\n",
      "created 476 samples\r\n",
      "sample_fine completed.\r\n",
      "created 477 samples\r\n",
      "sample_fine completed.\r\n",
      "created 478 samples\r\n",
      "sample_fine completed.\r\n",
      "created 479 samples\r\n",
      "sample_fine completed.\r\n",
      "created 480 samples\r\n",
      "sample_fine completed.\r\n",
      "created 481 samples\r\n",
      "sample_fine completed.\r\n",
      "created 482 samples\r\n",
      "sample_fine completed.\r\n",
      "created 483 samples\r\n",
      "sample_fine completed.\r\n",
      "created 484 samples\r\n",
      "sample_fine completed.\r\n",
      "created 485 samples\r\n",
      "sample_fine completed.\r\n",
      "created 486 samples\r\n",
      "sample_fine completed.\r\n",
      "created 487 samples\r\n",
      "sample_fine completed.\r\n",
      "created 488 samples\r\n",
      "sample_fine completed.\r\n",
      "created 489 samples\r\n",
      "sample_fine completed.\r\n",
      "created 490 samples\r\n",
      "sample_fine completed.\r\n",
      "created 491 samples\r\n",
      "sample_fine completed.\r\n",
      "created 492 samples\r\n",
      "sample_fine completed.\r\n",
      "created 493 samples\r\n",
      "sample_fine completed.\r\n",
      "created 494 samples\r\n",
      "sample_fine completed.\r\n",
      "created 495 samples\r\n",
      "sample_fine completed.\r\n",
      "created 496 samples\r\n",
      "sample_fine completed.\r\n",
      "created 497 samples\r\n",
      "sample_fine completed.\r\n",
      "created 498 samples\r\n",
      "sample_fine completed.\r\n",
      "created 499 samples\r\n",
      "sample_fine completed.\r\n",
      "created 500 samples\r\n",
      "sample_fine completed.\r\n",
      "created 501 samples\r\n",
      "sample_fine completed.\r\n",
      "created 502 samples\r\n",
      "sample_fine completed.\r\n",
      "created 503 samples\r\n",
      "sample_fine completed.\r\n",
      "created 504 samples\r\n",
      "sample_fine completed.\r\n",
      "created 505 samples\r\n",
      "sample_fine completed.\r\n",
      "created 506 samples\r\n",
      "sample_fine completed.\r\n",
      "created 507 samples\r\n",
      "sample_fine completed.\r\n",
      "created 508 samples\r\n",
      "sample_fine completed.\r\n",
      "created 509 samples\r\n",
      "sample_fine completed.\r\n",
      "created 510 samples\r\n",
      "sample_fine completed.\r\n",
      "created 511 samples\r\n",
      "sample_fine completed.\r\n",
      "created 512 samples\r\n",
      "sample_fine completed.\r\n",
      "created 513 samples\r\n",
      "sample_fine completed.\r\n",
      "created 514 samples\r\n",
      "sample_fine completed.\r\n",
      "created 515 samples\r\n",
      "sample_fine completed.\r\n",
      "created 516 samples\r\n",
      "sample_fine completed.\r\n",
      "created 517 samples\r\n",
      "sample_fine completed.\r\n",
      "created 518 samples\r\n",
      "sample_fine completed.\r\n",
      "created 519 samples\r\n",
      "sample_fine completed.\r\n",
      "created 520 samples\r\n",
      "sample_fine completed.\r\n",
      "created 521 samples\r\n",
      "sample_fine completed.\r\n",
      "created 522 samples\r\n",
      "sample_fine completed.\r\n",
      "created 523 samples\r\n",
      "sample_fine completed.\r\n",
      "created 524 samples\r\n",
      "sample_fine completed.\r\n",
      "created 525 samples\r\n",
      "sample_fine completed.\r\n",
      "created 526 samples\r\n",
      "sample_fine completed.\r\n",
      "created 527 samples\r\n",
      "sample_fine completed.\r\n",
      "created 528 samples\r\n",
      "sample_fine completed.\r\n",
      "created 529 samples\r\n",
      "sample_fine completed.\r\n",
      "created 530 samples\r\n",
      "sample_fine completed.\r\n",
      "created 531 samples\r\n",
      "sample_fine completed.\r\n",
      "created 532 samples\r\n",
      "sample_fine completed.\r\n",
      "created 533 samples\r\n",
      "sample_fine completed.\r\n",
      "created 534 samples\r\n",
      "sample_fine completed.\r\n",
      "created 535 samples\r\n",
      "sample_fine completed.\r\n",
      "created 536 samples\r\n",
      "sample_fine completed.\r\n",
      "created 537 samples\r\n",
      "sample_fine completed.\r\n",
      "created 538 samples\r\n",
      "sample_fine completed.\r\n",
      "created 539 samples\r\n",
      "sample_fine completed.\r\n",
      "created 540 samples\r\n",
      "sample_fine completed.\r\n",
      "created 541 samples\r\n",
      "sample_fine completed.\r\n",
      "created 542 samples\r\n",
      "sample_fine completed.\r\n",
      "created 543 samples\r\n",
      "sample_fine completed.\r\n",
      "created 544 samples\r\n",
      "sample_fine completed.\r\n",
      "created 545 samples\r\n",
      "sample_fine completed.\r\n",
      "created 546 samples\r\n",
      "sample_fine completed.\r\n",
      "created 547 samples\r\n",
      "sample_fine completed.\r\n",
      "created 548 samples\r\n",
      "sample_fine completed.\r\n",
      "created 549 samples\r\n",
      "sample_fine completed.\r\n",
      "created 550 samples\r\n",
      "sample_fine completed.\r\n",
      "created 551 samples\r\n",
      "sample_fine completed.\r\n",
      "created 552 samples\r\n",
      "sample_fine completed.\r\n",
      "created 553 samples\r\n",
      "sample_fine completed.\r\n",
      "created 554 samples\r\n",
      "sample_fine completed.\r\n",
      "created 555 samples\r\n",
      "sample_fine completed.\r\n",
      "created 556 samples\r\n",
      "sample_fine completed.\r\n",
      "created 557 samples\r\n",
      "sample_fine completed.\r\n",
      "created 558 samples\r\n",
      "sample_fine completed.\r\n",
      "created 559 samples\r\n",
      "sample_fine completed.\r\n",
      "created 560 samples\r\n",
      "sample_fine completed.\r\n",
      "created 561 samples\r\n",
      "sample_fine completed.\r\n",
      "created 562 samples\r\n",
      "sample_fine completed.\r\n",
      "created 563 samples\r\n",
      "sample_fine completed.\r\n",
      "created 564 samples\r\n",
      "sample_fine completed.\r\n",
      "created 565 samples\r\n",
      "sample_fine completed.\r\n",
      "created 566 samples\r\n",
      "sample_fine completed.\r\n",
      "created 567 samples\r\n",
      "sample_fine completed.\r\n",
      "created 568 samples\r\n",
      "sample_fine completed.\r\n",
      "created 569 samples\r\n",
      "sample_fine completed.\r\n",
      "created 570 samples\r\n",
      "sample_fine completed.\r\n",
      "created 571 samples\r\n",
      "sample_fine completed.\r\n",
      "created 572 samples\r\n",
      "sample_fine completed.\r\n",
      "created 573 samples\r\n",
      "sample_fine completed.\r\n",
      "created 574 samples\r\n",
      "sample_fine completed.\r\n",
      "created 575 samples\r\n",
      "sample_fine completed.\r\n",
      "created 576 samples\r\n",
      "sample_fine completed.\r\n",
      "created 577 samples\r\n",
      "sample_fine completed.\r\n",
      "created 578 samples\r\n",
      "sample_fine completed.\r\n",
      "created 579 samples\r\n",
      "sample_fine completed.\r\n",
      "created 580 samples\r\n",
      "sample_fine completed.\r\n",
      "created 581 samples\r\n",
      "sample_fine completed.\r\n",
      "created 582 samples\r\n",
      "sample_fine completed.\r\n",
      "created 583 samples\r\n",
      "sample_fine completed.\r\n",
      "created 584 samples\r\n",
      "sample_fine completed.\r\n",
      "created 585 samples\r\n",
      "sample_fine completed.\r\n",
      "created 586 samples\r\n",
      "sample_fine completed.\r\n",
      "created 587 samples\r\n",
      "sample_fine completed.\r\n",
      "created 588 samples\r\n",
      "sample_fine completed.\r\n",
      "created 589 samples\r\n",
      "sample_fine completed.\r\n",
      "created 590 samples\r\n",
      "sample_fine completed.\r\n",
      "created 591 samples\r\n",
      "sample_fine completed.\r\n",
      "created 592 samples\r\n",
      "sample_fine completed.\r\n",
      "created 593 samples\r\n",
      "sample_fine completed.\r\n",
      "created 594 samples\r\n",
      "sample_fine completed.\r\n",
      "created 595 samples\r\n",
      "sample_fine completed.\r\n",
      "created 596 samples\r\n",
      "sample_fine completed.\r\n",
      "created 597 samples\r\n",
      "sample_fine completed.\r\n",
      "created 598 samples\r\n",
      "sample_fine completed.\r\n",
      "created 599 samples\r\n",
      "sample_fine completed.\r\n",
      "created 600 samples\r\n",
      "sample_fine completed.\r\n",
      "created 601 samples\r\n",
      "sample_fine completed.\r\n",
      "created 602 samples\r\n",
      "sample_fine completed.\r\n",
      "created 603 samples\r\n",
      "sample_fine completed.\r\n",
      "created 604 samples\r\n",
      "sample_fine completed.\r\n",
      "created 605 samples\r\n",
      "sample_fine completed.\r\n",
      "created 606 samples\r\n",
      "sample_fine completed.\r\n",
      "created 607 samples\r\n",
      "sample_fine completed.\r\n",
      "created 608 samples\r\n",
      "sample_fine completed.\r\n",
      "created 609 samples\r\n",
      "sample_fine completed.\r\n",
      "created 610 samples\r\n",
      "sample_fine completed.\r\n",
      "created 611 samples\r\n",
      "sample_fine completed.\r\n",
      "created 612 samples\r\n",
      "sample_fine completed.\r\n",
      "created 613 samples\r\n",
      "sample_fine completed.\r\n",
      "created 614 samples\r\n",
      "sample_fine completed.\r\n",
      "created 615 samples\r\n",
      "sample_fine completed.\r\n",
      "created 616 samples\r\n",
      "sample_fine completed.\r\n",
      "created 617 samples\r\n",
      "sample_fine completed.\r\n",
      "created 618 samples\r\n",
      "sample_fine completed.\r\n",
      "created 619 samples\r\n",
      "sample_fine completed.\r\n",
      "created 620 samples\r\n",
      "sample_fine completed.\r\n",
      "created 621 samples\r\n",
      "sample_fine completed.\r\n",
      "created 622 samples\r\n",
      "sample_fine completed.\r\n",
      "created 623 samples\r\n",
      "sample_fine completed.\r\n",
      "created 624 samples\r\n",
      "sample_fine completed.\r\n",
      "created 625 samples\r\n",
      "sample_fine completed.\r\n",
      "created 626 samples\r\n",
      "sample_fine completed.\r\n",
      "created 627 samples\r\n",
      "sample_fine completed.\r\n",
      "created 628 samples\r\n",
      "sample_fine completed.\r\n",
      "created 629 samples\r\n",
      "sample_fine completed.\r\n",
      "created 630 samples\r\n",
      "sample_fine completed.\r\n",
      "created 631 samples\r\n",
      "sample_fine completed.\r\n",
      "created 632 samples\r\n",
      "sample_fine completed.\r\n",
      "created 633 samples\r\n",
      "sample_fine completed.\r\n",
      "created 634 samples\r\n",
      "sample_fine completed.\r\n",
      "created 635 samples\r\n",
      "sample_fine completed.\r\n",
      "created 636 samples\r\n",
      "sample_fine completed.\r\n",
      "created 637 samples\r\n",
      "sample_fine completed.\r\n",
      "created 638 samples\r\n",
      "sample_fine completed.\r\n",
      "created 639 samples\r\n",
      "sample_fine completed.\r\n",
      "created 640 samples\r\n",
      "sample_fine completed.\r\n",
      "created 641 samples\r\n",
      "sample_fine completed.\r\n",
      "created 642 samples\r\n",
      "sample_fine completed.\r\n",
      "created 643 samples\r\n",
      "sample_fine completed.\r\n",
      "created 644 samples\r\n",
      "sample_fine completed.\r\n",
      "created 645 samples\r\n",
      "sample_fine completed.\r\n",
      "created 646 samples\r\n",
      "sample_fine completed.\r\n",
      "created 647 samples\r\n",
      "sample_fine completed.\r\n",
      "created 648 samples\r\n",
      "sample_fine completed.\r\n",
      "created 649 samples\r\n",
      "sample_fine completed.\r\n",
      "created 650 samples\r\n",
      "sample_fine completed.\r\n",
      "created 651 samples\r\n",
      "sample_fine completed.\r\n",
      "created 652 samples\r\n",
      "sample_fine completed.\r\n",
      "created 653 samples\r\n",
      "sample_fine completed.\r\n",
      "created 654 samples\r\n",
      "sample_fine completed.\r\n",
      "created 655 samples\r\n",
      "sample_fine completed.\r\n",
      "created 656 samples\r\n",
      "sample_fine completed.\r\n",
      "created 657 samples\r\n",
      "sample_fine completed.\r\n",
      "created 658 samples\r\n",
      "sample_fine completed.\r\n",
      "created 659 samples\r\n",
      "sample_fine completed.\r\n",
      "created 660 samples\r\n",
      "sample_fine completed.\r\n",
      "created 661 samples\r\n",
      "sample_fine completed.\r\n",
      "created 662 samples\r\n",
      "sample_fine completed.\r\n",
      "created 663 samples\r\n",
      "sample_fine completed.\r\n",
      "created 664 samples\r\n",
      "sample_fine completed.\r\n",
      "created 665 samples\r\n",
      "sample_fine completed.\r\n",
      "created 666 samples\r\n",
      "sample_fine completed.\r\n",
      "created 667 samples\r\n",
      "sample_fine completed.\r\n",
      "created 668 samples\r\n",
      "sample_fine completed.\r\n",
      "created 669 samples\r\n",
      "sample_fine completed.\r\n",
      "created 670 samples\r\n",
      "sample_fine completed.\r\n",
      "created 671 samples\r\n",
      "sample_fine completed.\r\n",
      "created 672 samples\r\n",
      "sample_fine completed.\r\n",
      "created 673 samples\r\n",
      "sample_fine completed.\r\n",
      "created 674 samples\r\n",
      "sample_fine completed.\r\n",
      "created 675 samples\r\n",
      "sample_fine completed.\r\n",
      "created 676 samples\r\n",
      "sample_fine completed.\r\n",
      "created 677 samples\r\n",
      "sample_fine completed.\r\n",
      "created 678 samples\r\n",
      "sample_fine completed.\r\n",
      "created 679 samples\r\n",
      "sample_fine completed.\r\n",
      "created 680 samples\r\n",
      "sample_fine completed.\r\n",
      "created 681 samples\r\n",
      "sample_fine completed.\r\n",
      "created 682 samples\r\n",
      "sample_fine completed.\r\n",
      "created 683 samples\r\n",
      "sample_fine completed.\r\n",
      "created 684 samples\r\n",
      "sample_fine completed.\r\n",
      "created 685 samples\r\n",
      "sample_fine completed.\r\n",
      "created 686 samples\r\n",
      "sample_fine completed.\r\n",
      "created 687 samples\r\n",
      "sample_fine completed.\r\n",
      "created 688 samples\r\n",
      "sample_fine completed.\r\n",
      "created 689 samples\r\n",
      "sample_fine completed.\r\n",
      "created 690 samples\r\n",
      "sample_fine completed.\r\n",
      "created 691 samples\r\n",
      "sample_fine completed.\r\n",
      "created 692 samples\r\n",
      "sample_fine completed.\r\n",
      "created 693 samples\r\n",
      "sample_fine completed.\r\n",
      "created 694 samples\r\n",
      "sample_fine completed.\r\n",
      "created 695 samples\r\n",
      "sample_fine completed.\r\n",
      "created 696 samples\r\n",
      "sample_fine completed.\r\n",
      "created 697 samples\r\n",
      "sample_fine completed.\r\n",
      "created 698 samples\r\n",
      "sample_fine completed.\r\n",
      "created 699 samples\r\n",
      "sample_fine completed.\r\n",
      "created 700 samples\r\n",
      "sample_fine completed.\r\n",
      "created 701 samples\r\n",
      "sample_fine completed.\r\n",
      "created 702 samples\r\n",
      "sample_fine completed.\r\n",
      "created 703 samples\r\n",
      "sample_fine completed.\r\n",
      "created 704 samples\r\n",
      "sample_fine completed.\r\n",
      "created 705 samples\r\n",
      "sample_fine completed.\r\n",
      "created 706 samples\r\n",
      "sample_fine completed.\r\n",
      "created 707 samples\r\n",
      "sample_fine completed.\r\n",
      "created 708 samples\r\n",
      "sample_fine completed.\r\n",
      "created 709 samples\r\n",
      "sample_fine completed.\r\n",
      "created 710 samples\r\n",
      "sample_fine completed.\r\n",
      "created 711 samples\r\n",
      "sample_fine completed.\r\n",
      "created 712 samples\r\n",
      "sample_fine completed.\r\n",
      "created 713 samples\r\n",
      "sample_fine completed.\r\n",
      "created 714 samples\r\n",
      "sample_fine completed.\r\n",
      "created 715 samples\r\n",
      "sample_fine completed.\r\n",
      "created 716 samples\r\n",
      "sample_fine completed.\r\n",
      "created 717 samples\r\n",
      "sample_fine completed.\r\n",
      "created 718 samples\r\n",
      "sample_fine completed.\r\n",
      "created 719 samples\r\n",
      "sample_fine completed.\r\n",
      "created 720 samples\r\n",
      "sample_fine completed.\r\n",
      "created 721 samples\r\n",
      "sample_fine completed.\r\n",
      "created 722 samples\r\n",
      "sample_fine completed.\r\n",
      "created 723 samples\r\n",
      "sample_fine completed.\r\n",
      "created 724 samples\r\n",
      "sample_fine completed.\r\n",
      "created 725 samples\r\n",
      "sample_fine completed.\r\n",
      "created 726 samples\r\n",
      "sample_fine completed.\r\n",
      "created 727 samples\r\n",
      "sample_fine completed.\r\n",
      "created 728 samples\r\n",
      "sample_fine completed.\r\n",
      "created 729 samples\r\n",
      "sample_fine completed.\r\n",
      "created 730 samples\r\n",
      "sample_fine completed.\r\n",
      "created 731 samples\r\n",
      "sample_fine completed.\r\n",
      "created 732 samples\r\n",
      "sample_fine completed.\r\n",
      "created 733 samples\r\n",
      "sample_fine completed.\r\n",
      "created 734 samples\r\n",
      "sample_fine completed.\r\n",
      "created 735 samples\r\n",
      "sample_fine completed.\r\n",
      "created 736 samples\r\n",
      "sample_fine completed.\r\n",
      "created 737 samples\r\n",
      "sample_fine completed.\r\n",
      "created 738 samples\r\n",
      "sample_fine completed.\r\n",
      "created 739 samples\r\n",
      "sample_fine completed.\r\n",
      "created 740 samples\r\n",
      "sample_fine completed.\r\n",
      "created 741 samples\r\n",
      "sample_fine completed.\r\n",
      "created 742 samples\r\n",
      "sample_fine completed.\r\n",
      "created 743 samples\r\n",
      "sample_fine completed.\r\n",
      "created 744 samples\r\n",
      "sample_fine completed.\r\n",
      "created 745 samples\r\n",
      "sample_fine completed.\r\n",
      "created 746 samples\r\n",
      "sample_fine completed.\r\n",
      "created 747 samples\r\n",
      "sample_fine completed.\r\n",
      "created 748 samples\r\n",
      "sample_fine completed.\r\n",
      "created 749 samples\r\n",
      "sample_fine completed.\r\n",
      "created 750 samples\r\n",
      "sample_fine completed.\r\n",
      "created 751 samples\r\n",
      "sample_fine completed.\r\n",
      "created 752 samples\r\n",
      "sample_fine completed.\r\n",
      "created 753 samples\r\n",
      "sample_fine completed.\r\n",
      "created 754 samples\r\n",
      "sample_fine completed.\r\n",
      "created 755 samples\r\n",
      "sample_fine completed.\r\n",
      "created 756 samples\r\n",
      "sample_fine completed.\r\n",
      "created 757 samples\r\n",
      "sample_fine completed.\r\n",
      "created 758 samples\r\n",
      "sample_fine completed.\r\n",
      "created 759 samples\r\n",
      "sample_fine completed.\r\n",
      "created 760 samples\r\n",
      "sample_fine completed.\r\n",
      "created 761 samples\r\n",
      "sample_fine completed.\r\n",
      "created 762 samples\r\n",
      "sample_fine completed.\r\n",
      "created 763 samples\r\n",
      "sample_fine completed.\r\n",
      "created 764 samples\r\n",
      "sample_fine completed.\r\n",
      "created 765 samples\r\n",
      "sample_fine completed.\r\n",
      "created 766 samples\r\n",
      "sample_fine completed.\r\n",
      "created 767 samples\r\n",
      "sample_fine completed.\r\n",
      "created 768 samples\r\n",
      "sample_fine completed.\r\n",
      "created 769 samples\r\n",
      "sample_fine completed.\r\n",
      "created 770 samples\r\n",
      "sample_fine completed.\r\n",
      "created 771 samples\r\n",
      "sample_fine completed.\r\n",
      "created 772 samples\r\n",
      "sample_fine completed.\r\n",
      "created 773 samples\r\n",
      "sample_fine completed.\r\n",
      "created 774 samples\r\n",
      "sample_fine completed.\r\n",
      "created 775 samples\r\n",
      "sample_fine completed.\r\n",
      "created 776 samples\r\n",
      "sample_fine completed.\r\n",
      "created 777 samples\r\n",
      "sample_fine completed.\r\n",
      "created 778 samples\r\n",
      "sample_fine completed.\r\n",
      "created 779 samples\r\n",
      "sample_fine completed.\r\n",
      "created 780 samples\r\n",
      "sample_fine completed.\r\n",
      "created 781 samples\r\n",
      "sample_fine completed.\r\n",
      "created 782 samples\r\n",
      "sample_fine completed.\r\n",
      "created 783 samples\r\n",
      "sample_fine completed.\r\n",
      "created 784 samples\r\n",
      "sample_fine completed.\r\n",
      "created 785 samples\r\n",
      "sample_fine completed.\r\n",
      "created 786 samples\r\n",
      "sample_fine completed.\r\n",
      "created 787 samples\r\n",
      "sample_fine completed.\r\n",
      "created 788 samples\r\n",
      "sample_fine completed.\r\n",
      "created 789 samples\r\n",
      "sample_fine completed.\r\n",
      "created 790 samples\r\n",
      "sample_fine completed.\r\n",
      "created 791 samples\r\n",
      "sample_fine completed.\r\n",
      "created 792 samples\r\n",
      "sample_fine completed.\r\n",
      "created 793 samples\r\n",
      "sample_fine completed.\r\n",
      "created 794 samples\r\n",
      "sample_fine completed.\r\n",
      "created 795 samples\r\n",
      "sample_fine completed.\r\n",
      "created 796 samples\r\n",
      "sample_fine completed.\r\n",
      "created 797 samples\r\n",
      "sample_fine completed.\r\n",
      "created 798 samples\r\n",
      "sample_fine completed.\r\n",
      "created 799 samples\r\n",
      "sample_fine completed.\r\n",
      "created 800 samples\r\n",
      "sample_fine completed.\r\n",
      "created 801 samples\r\n",
      "sample_fine completed.\r\n",
      "created 802 samples\r\n",
      "sample_fine completed.\r\n",
      "created 803 samples\r\n",
      "sample_fine completed.\r\n",
      "created 804 samples\r\n",
      "sample_fine completed.\r\n",
      "created 805 samples\r\n",
      "sample_fine completed.\r\n",
      "created 806 samples\r\n",
      "sample_fine completed.\r\n",
      "created 807 samples\r\n",
      "sample_fine completed.\r\n",
      "created 808 samples\r\n",
      "sample_fine completed.\r\n",
      "created 809 samples\r\n",
      "sample_fine completed.\r\n",
      "created 810 samples\r\n",
      "sample_fine completed.\r\n",
      "created 811 samples\r\n",
      "sample_fine completed.\r\n",
      "created 812 samples\r\n",
      "sample_fine completed.\r\n",
      "created 813 samples\r\n",
      "sample_fine completed.\r\n",
      "created 814 samples\r\n",
      "sample_fine completed.\r\n",
      "created 815 samples\r\n",
      "sample_fine completed.\r\n",
      "created 816 samples\r\n",
      "sample_fine completed.\r\n",
      "created 817 samples\r\n",
      "sample_fine completed.\r\n",
      "created 818 samples\r\n",
      "sample_fine completed.\r\n",
      "created 819 samples\r\n",
      "sample_fine completed.\r\n",
      "created 820 samples\r\n",
      "sample_fine completed.\r\n",
      "created 821 samples\r\n",
      "sample_fine completed.\r\n",
      "created 822 samples\r\n",
      "sample_fine completed.\r\n",
      "created 823 samples\r\n",
      "sample_fine completed.\r\n",
      "created 824 samples\r\n",
      "sample_fine completed.\r\n",
      "created 825 samples\r\n",
      "sample_fine completed.\r\n",
      "created 826 samples\r\n",
      "sample_fine completed.\r\n",
      "created 827 samples\r\n",
      "sample_fine completed.\r\n",
      "created 828 samples\r\n",
      "sample_fine completed.\r\n",
      "created 829 samples\r\n",
      "sample_fine completed.\r\n",
      "created 830 samples\r\n",
      "sample_fine completed.\r\n",
      "created 831 samples\r\n",
      "sample_fine completed.\r\n",
      "created 832 samples\r\n",
      "sample_fine completed.\r\n",
      "created 833 samples\r\n",
      "sample_fine completed.\r\n",
      "created 834 samples\r\n",
      "sample_fine completed.\r\n",
      "created 835 samples\r\n",
      "sample_fine completed.\r\n",
      "created 836 samples\r\n",
      "sample_fine completed.\r\n",
      "created 837 samples\r\n",
      "sample_fine completed.\r\n",
      "created 838 samples\r\n",
      "sample_fine completed.\r\n",
      "created 839 samples\r\n",
      "sample_fine completed.\r\n",
      "created 840 samples\r\n",
      "sample_fine completed.\r\n",
      "created 841 samples\r\n",
      "sample_fine completed.\r\n",
      "created 842 samples\r\n",
      "sample_fine completed.\r\n",
      "created 843 samples\r\n",
      "sample_fine completed.\r\n",
      "created 844 samples\r\n",
      "sample_fine completed.\r\n",
      "created 845 samples\r\n",
      "sample_fine completed.\r\n",
      "created 846 samples\r\n",
      "sample_fine completed.\r\n",
      "created 847 samples\r\n",
      "sample_fine completed.\r\n",
      "created 848 samples\r\n",
      "sample_fine completed.\r\n",
      "created 849 samples\r\n",
      "sample_fine completed.\r\n",
      "created 850 samples\r\n",
      "sample_fine completed.\r\n",
      "created 851 samples\r\n",
      "sample_fine completed.\r\n",
      "created 852 samples\r\n",
      "sample_fine completed.\r\n",
      "created 853 samples\r\n",
      "sample_fine completed.\r\n",
      "created 854 samples\r\n",
      "sample_fine completed.\r\n",
      "created 855 samples\r\n",
      "sample_fine completed.\r\n",
      "created 856 samples\r\n",
      "sample_fine completed.\r\n",
      "created 857 samples\r\n",
      "sample_fine completed.\r\n",
      "created 858 samples\r\n",
      "sample_fine completed.\r\n",
      "created 859 samples\r\n",
      "sample_fine completed.\r\n",
      "created 860 samples\r\n",
      "sample_fine completed.\r\n",
      "created 861 samples\r\n",
      "sample_fine completed.\r\n",
      "created 862 samples\r\n",
      "sample_fine completed.\r\n",
      "created 863 samples\r\n",
      "sample_fine completed.\r\n",
      "created 864 samples\r\n",
      "sample_fine completed.\r\n",
      "created 865 samples\r\n",
      "sample_fine completed.\r\n",
      "created 866 samples\r\n",
      "sample_fine completed.\r\n",
      "created 867 samples\r\n",
      "sample_fine completed.\r\n",
      "created 868 samples\r\n",
      "sample_fine completed.\r\n",
      "created 869 samples\r\n",
      "sample_fine completed.\r\n",
      "created 870 samples\r\n",
      "sample_fine completed.\r\n",
      "created 871 samples\r\n",
      "sample_fine completed.\r\n",
      "created 872 samples\r\n",
      "sample_fine completed.\r\n",
      "created 873 samples\r\n",
      "sample_fine completed.\r\n",
      "created 874 samples\r\n",
      "sample_fine completed.\r\n",
      "created 875 samples\r\n",
      "sample_fine completed.\r\n",
      "created 876 samples\r\n",
      "sample_fine completed.\r\n",
      "created 877 samples\r\n",
      "sample_fine completed.\r\n",
      "created 878 samples\r\n",
      "sample_fine completed.\r\n",
      "created 879 samples\r\n",
      "sample_fine completed.\r\n",
      "created 880 samples\r\n",
      "sample_fine completed.\r\n",
      "created 881 samples\r\n",
      "sample_fine completed.\r\n",
      "created 882 samples\r\n",
      "sample_fine completed.\r\n",
      "created 883 samples\r\n",
      "sample_fine completed.\r\n",
      "created 884 samples\r\n",
      "sample_fine completed.\r\n",
      "created 885 samples\r\n",
      "sample_fine completed.\r\n",
      "created 886 samples\r\n",
      "sample_fine completed.\r\n",
      "created 887 samples\r\n",
      "sample_fine completed.\r\n",
      "created 888 samples\r\n",
      "sample_fine completed.\r\n",
      "created 889 samples\r\n",
      "sample_fine completed.\r\n",
      "created 890 samples\r\n",
      "sample_fine completed.\r\n",
      "created 891 samples\r\n",
      "sample_fine completed.\r\n",
      "created 892 samples\r\n",
      "sample_fine completed.\r\n",
      "created 893 samples\r\n",
      "sample_fine completed.\r\n",
      "created 894 samples\r\n",
      "sample_fine completed.\r\n",
      "created 895 samples\r\n",
      "sample_fine completed.\r\n",
      "created 896 samples\r\n",
      "sample_fine completed.\r\n",
      "created 897 samples\r\n",
      "sample_fine completed.\r\n",
      "created 898 samples\r\n",
      "sample_fine completed.\r\n",
      "created 899 samples\r\n",
      "sample_fine completed.\r\n",
      "created 900 samples\r\n",
      "sample_fine completed.\r\n",
      "created 901 samples\r\n",
      "sample_fine completed.\r\n",
      "created 902 samples\r\n",
      "sample_fine completed.\r\n",
      "created 903 samples\r\n",
      "sample_fine completed.\r\n",
      "created 904 samples\r\n",
      "sample_fine completed.\r\n",
      "created 905 samples\r\n",
      "sample_fine completed.\r\n",
      "created 906 samples\r\n",
      "sample_fine completed.\r\n",
      "created 907 samples\r\n",
      "sample_fine completed.\r\n",
      "created 908 samples\r\n",
      "sample_fine completed.\r\n",
      "created 909 samples\r\n",
      "sample_fine completed.\r\n",
      "created 910 samples\r\n",
      "sample_fine completed.\r\n",
      "created 911 samples\r\n",
      "sample_fine completed.\r\n",
      "created 912 samples\r\n",
      "sample_fine completed.\r\n",
      "created 913 samples\r\n",
      "sample_fine completed.\r\n",
      "created 914 samples\r\n",
      "sample_fine completed.\r\n",
      "created 915 samples\r\n",
      "sample_fine completed.\r\n",
      "created 916 samples\r\n",
      "sample_fine completed.\r\n",
      "created 917 samples\r\n",
      "sample_fine completed.\r\n",
      "created 918 samples\r\n",
      "sample_fine completed.\r\n",
      "created 919 samples\r\n",
      "sample_fine completed.\r\n",
      "created 920 samples\r\n",
      "sample_fine completed.\r\n",
      "created 921 samples\r\n",
      "sample_fine completed.\r\n",
      "created 922 samples\r\n",
      "sample_fine completed.\r\n",
      "created 923 samples\r\n",
      "sample_fine completed.\r\n",
      "created 924 samples\r\n",
      "sample_fine completed.\r\n",
      "created 925 samples\r\n",
      "sample_fine completed.\r\n",
      "created 926 samples\r\n",
      "sample_fine completed.\r\n",
      "created 927 samples\r\n",
      "sample_fine completed.\r\n",
      "created 928 samples\r\n",
      "sample_fine completed.\r\n",
      "created 929 samples\r\n",
      "sample_fine completed.\r\n",
      "created 930 samples\r\n",
      "sample_fine completed.\r\n",
      "created 931 samples\r\n",
      "sample_fine completed.\r\n",
      "created 932 samples\r\n",
      "sample_fine completed.\r\n",
      "created 933 samples\r\n",
      "sample_fine completed.\r\n",
      "created 934 samples\r\n",
      "sample_fine completed.\r\n",
      "created 935 samples\r\n",
      "sample_fine completed.\r\n",
      "created 936 samples\r\n",
      "sample_fine completed.\r\n",
      "created 937 samples\r\n",
      "sample_fine completed.\r\n",
      "created 938 samples\r\n",
      "sample_fine completed.\r\n",
      "created 939 samples\r\n",
      "sample_fine completed.\r\n",
      "created 940 samples\r\n",
      "sample_fine completed.\r\n",
      "created 941 samples\r\n",
      "sample_fine completed.\r\n",
      "created 942 samples\r\n",
      "sample_fine completed.\r\n",
      "created 943 samples\r\n",
      "sample_fine completed.\r\n",
      "created 944 samples\r\n",
      "sample_fine completed.\r\n",
      "created 945 samples\r\n",
      "sample_fine completed.\r\n",
      "created 946 samples\r\n",
      "sample_fine completed.\r\n",
      "created 947 samples\r\n",
      "sample_fine completed.\r\n",
      "created 948 samples\r\n",
      "sample_fine completed.\r\n",
      "created 949 samples\r\n",
      "sample_fine completed.\r\n",
      "created 950 samples\r\n",
      "sample_fine completed.\r\n",
      "created 951 samples\r\n",
      "sample_fine completed.\r\n",
      "created 952 samples\r\n",
      "sample_fine completed.\r\n",
      "created 953 samples\r\n",
      "sample_fine completed.\r\n",
      "created 954 samples\r\n",
      "sample_fine completed.\r\n",
      "created 955 samples\r\n",
      "sample_fine completed.\r\n",
      "created 956 samples\r\n",
      "sample_fine completed.\r\n",
      "created 957 samples\r\n",
      "sample_fine completed.\r\n",
      "created 958 samples\r\n",
      "sample_fine completed.\r\n",
      "created 959 samples\r\n",
      "sample_fine completed.\r\n",
      "created 960 samples\r\n",
      "sample_fine completed.\r\n",
      "created 961 samples\r\n",
      "sample_fine completed.\r\n",
      "created 962 samples\r\n",
      "sample_fine completed.\r\n",
      "created 963 samples\r\n",
      "sample_fine completed.\r\n",
      "created 964 samples\r\n",
      "sample_fine completed.\r\n",
      "created 965 samples\r\n",
      "sample_fine completed.\r\n",
      "created 966 samples\r\n",
      "sample_fine completed.\r\n",
      "created 967 samples\r\n",
      "sample_fine completed.\r\n",
      "created 968 samples\r\n",
      "sample_fine completed.\r\n",
      "created 969 samples\r\n",
      "sample_fine completed.\r\n",
      "created 970 samples\r\n",
      "sample_fine completed.\r\n",
      "created 971 samples\r\n",
      "sample_fine completed.\r\n",
      "created 972 samples\r\n",
      "sample_fine completed.\r\n",
      "created 973 samples\r\n",
      "sample_fine completed.\r\n",
      "created 974 samples\r\n",
      "sample_fine completed.\r\n",
      "created 975 samples\r\n",
      "sample_fine completed.\r\n",
      "created 976 samples\r\n",
      "sample_fine completed.\r\n",
      "created 977 samples\r\n",
      "sample_fine completed.\r\n",
      "created 978 samples\r\n",
      "sample_fine completed.\r\n",
      "created 979 samples\r\n",
      "sample_fine completed.\r\n",
      "created 980 samples\r\n",
      "sample_fine completed.\r\n",
      "created 981 samples\r\n",
      "sample_fine completed.\r\n",
      "created 982 samples\r\n",
      "sample_fine completed.\r\n",
      "created 983 samples\r\n",
      "sample_fine completed.\r\n",
      "created 984 samples\r\n",
      "sample_fine completed.\r\n",
      "created 985 samples\r\n",
      "sample_fine completed.\r\n",
      "created 986 samples\r\n",
      "sample_fine completed.\r\n",
      "created 987 samples\r\n",
      "sample_fine completed.\r\n",
      "created 988 samples\r\n",
      "sample_fine completed.\r\n",
      "created 989 samples\r\n",
      "sample_fine completed.\r\n",
      "created 990 samples\r\n",
      "sample_fine completed.\r\n",
      "created 991 samples\r\n",
      "sample_fine completed.\r\n",
      "created 992 samples\r\n",
      "sample_fine completed.\r\n",
      "created 993 samples\r\n",
      "sample_fine completed.\r\n",
      "created 994 samples\r\n",
      "sample_fine completed.\r\n",
      "created 995 samples\r\n",
      "sample_fine completed.\r\n",
      "created 996 samples\r\n",
      "sample_fine completed.\r\n",
      "created 997 samples\r\n",
      "sample_fine completed.\r\n",
      "created 998 samples\r\n",
      "sample_fine completed.\r\n",
      "created 999 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1000 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1001 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1002 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1003 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1004 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1005 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1006 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1007 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1008 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1009 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1010 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1011 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1012 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1013 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1014 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1015 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1016 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1017 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1018 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1019 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1020 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1021 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1022 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1023 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1024 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1025 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1026 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1027 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1028 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1029 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1030 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1031 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1032 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1033 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1034 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1035 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1036 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1037 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1038 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1039 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1040 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1041 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1042 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1043 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1044 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1045 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1046 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1047 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1048 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1049 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1050 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1051 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1052 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1053 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1054 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1055 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1056 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1057 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1058 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1059 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1060 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1061 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1062 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1063 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1064 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1065 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1066 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1067 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1068 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1069 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1070 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1071 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1072 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1073 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1074 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1075 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1076 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1077 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1078 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1079 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1080 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1081 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1082 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1083 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1084 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1085 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1086 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1087 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1088 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1089 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1090 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1091 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1092 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1093 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1094 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1095 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1096 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1097 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1098 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1099 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1100 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1101 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1102 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1103 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1104 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1105 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1106 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1107 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1108 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1109 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1110 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1111 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1112 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1113 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1114 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1115 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1116 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1117 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1118 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1119 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1120 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1121 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1122 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1123 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1124 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1125 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1126 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1127 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1128 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1129 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1130 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1131 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1132 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1133 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1134 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1135 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1136 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1137 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1138 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1139 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1140 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1141 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1142 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1143 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1144 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1145 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1146 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1147 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1148 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1149 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1150 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1151 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1152 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1153 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1154 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1155 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1156 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1157 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1158 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1159 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1160 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1161 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1162 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1163 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1164 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1165 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1166 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1167 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1168 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1169 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1170 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1171 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1172 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1173 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1174 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1175 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1176 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1177 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1178 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1179 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1180 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1181 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1182 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1183 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1184 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1185 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1186 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1187 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1188 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1189 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1190 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1191 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1192 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1193 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1194 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1195 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1196 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1197 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1198 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1199 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1200 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1201 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1202 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1203 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1204 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1205 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1206 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1207 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1208 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1209 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1210 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1211 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1212 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1213 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1214 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1215 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1216 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1217 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1218 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1219 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1220 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1221 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1222 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1223 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1224 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1225 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1226 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1227 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1228 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1229 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1230 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1231 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1232 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1233 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1234 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1235 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1236 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1237 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1238 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1239 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1240 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1241 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1242 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1243 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1244 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1245 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1246 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1247 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1248 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1249 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1250 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1251 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1252 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1253 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1254 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1255 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1256 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1257 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1258 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1259 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1260 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1261 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1262 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1263 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1264 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1265 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1266 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1267 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1268 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1269 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1270 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1271 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1272 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1273 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1274 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1275 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1276 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1277 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1278 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1279 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1280 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1281 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1282 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1283 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1284 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1285 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1286 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1287 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1288 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1289 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1290 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1291 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1292 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1293 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1294 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1295 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1296 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1297 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1298 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1299 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1300 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1301 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1302 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1303 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1304 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1305 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1306 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1307 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1308 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1309 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1310 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1311 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1312 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1313 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1314 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1315 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1316 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1317 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1318 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1319 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1320 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1321 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1322 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1323 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1324 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1325 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1326 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1327 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1328 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1329 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1330 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1331 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1332 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1333 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1334 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1335 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1336 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1337 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1338 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1339 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1340 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1341 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1342 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1343 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1344 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1345 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1346 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1347 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1348 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1349 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1350 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1351 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1352 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1353 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1354 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1355 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1356 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1357 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1358 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1359 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1360 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1361 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1362 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1363 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1364 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1365 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1366 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1367 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1368 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1369 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1370 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1371 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1372 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1373 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1374 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1375 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1376 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1377 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1378 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1379 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1380 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1381 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1382 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1383 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1384 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1385 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1386 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1387 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1388 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1389 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1390 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1391 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1392 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1393 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1394 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1395 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1396 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1397 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1398 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1399 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1400 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1401 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1402 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1403 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1404 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1405 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1406 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1407 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1408 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1409 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1410 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1411 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1412 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1413 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1414 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1415 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1416 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1417 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1418 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1419 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1420 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1421 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1422 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1423 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1424 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1425 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1426 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1427 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1428 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1429 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1430 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1431 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1432 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1433 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1434 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1435 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1436 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1437 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1438 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1439 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1440 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1441 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1442 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1443 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1444 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1445 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1446 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1447 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1448 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1449 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1450 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1451 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1452 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1453 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1454 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1455 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1456 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1457 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1458 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1459 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1460 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1461 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1462 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1463 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1464 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1465 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1466 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1467 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1468 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1469 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1470 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1471 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1472 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1473 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1474 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1475 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1476 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1477 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1478 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1479 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1480 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1481 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1482 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1483 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1484 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1485 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1486 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1487 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1488 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1489 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1490 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1491 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1492 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1493 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1494 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1495 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1496 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1497 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1498 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1499 samples\r\n",
      "sample_fine completed.\r\n",
      "created 1500 samples\r\n",
      "sampling complete\r\n",
      "Total time: 17721.489540338516.\r\n",
      "Each time: 11.814326360225678.\r\n"
     ]
    }
   ],
   "source": [
    "!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2wd && \\\n",
    " cd /kaggle/working/p2w && ./inpaint.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7be9639",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-08-05T22:41:31.329967Z",
     "iopub.status.busy": "2025-08-05T22:41:31.328936Z",
     "iopub.status.idle": "2025-08-05T22:41:31.337798Z",
     "shell.execute_reply": "2025-08-05T22:41:31.337157Z"
    },
    "papermill": {
     "duration": 0.159154,
     "end_time": "2025-08-05T22:41:31.339060",
     "exception": false,
     "start_time": "2025-08-05T22:41:31.179906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "def show_images_grid(save_dir):\n",
    "    # Define the path to the outImg folder\n",
    "    out_img_dir = save_dir\n",
    "    \n",
    "    # List all PNG files in the outImg directory\n",
    "    image_files = sorted([f for f in os.listdir(out_img_dir) if f.endswith(\".png\")])\n",
    "    print(f\"Found {len(image_files)} images in {out_img_dir}\")\n",
    "    \n",
    "    # Define the grid size (e.g., 2 rows x 3 columns for 6 images)\n",
    "    num_images = len(image_files)\n",
    "    cols = 3  # Number of columns in the grid\n",
    "    rows = math.ceil(num_images / cols)  # Calculate rows needed\n",
    "    \n",
    "    # Create a figure and axes for the grid\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    \n",
    "    # Flatten axes array for easy iteration (handles both 1D and 2D cases)\n",
    "    axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
    "    \n",
    "    # Load and plot each image\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        if idx >= len(axes):  # Stop if we run out of grid spaces\n",
    "            break\n",
    "        # Load the image\n",
    "        image_path = os.path.join(out_img_dir, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Display the image in the grid\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(image_file, fontsize=10)\n",
    "        axes[idx].axis(\"off\")  # Hide axes for cleaner display\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for idx in range(len(image_files), len(axes)):\n",
    "        axes[idx].axis(\"off\")\n",
    "    \n",
    "    # Adjust layout and display the grid\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5df97648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T22:41:31.630660Z",
     "iopub.status.busy": "2025-08-05T22:41:31.630307Z",
     "iopub.status.idle": "2025-08-05T22:41:31.638375Z",
     "shell.execute_reply": "2025-08-05T22:41:31.637481Z"
    },
    "papermill": {
     "duration": 0.158612,
     "end_time": "2025-08-05T22:41:31.639853",
     "exception": false,
     "start_time": "2025-08-05T22:41:31.481241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote metrics.py to /kaggle/working/p2w/metrics.py\n"
     ]
    }
   ],
   "source": [
    "metric_str = '# === Metric Imports ===\\n'\\\n",
    "'from lpips import LPIPS\\n'\\\n",
    "'import torch\\n'\\\n",
    "'import torch.nn.functional as F\\n'\\\n",
    "'from math import log10\\n'\\\n",
    "'from torchmetrics.functional.image import structural_similarity_index_measure\\n'\\\n",
    "'from PIL import Image\\n'\\\n",
    "'from torchvision import transforms\\n'\\\n",
    "'import os\\n'\\\n",
    "'import sys\\n'\\\n",
    "'import io\\n'\\\n",
    "'import subprocess\\n'\\\n",
    "'os.environ[\"TORCH_HOME\"] = \"/kaggle/working/p2w/.cache/torch/\"\\n'\\\n",
    "'\\n'\\\n",
    "'# === Install FID if not installed ===\\n'\\\n",
    "'#!pip install -q pytorch-fid\\n'\\\n",
    "'\\n'\\\n",
    "'# === Metric Calculation Functions ===\\n'\\\n",
    "'def check_range(img):\\n'\\\n",
    "'    assert torch.min(img) >= -1.0, \"Minimum pixel value is less than -1.0\"\\n'\\\n",
    "'    assert torch.max(img) <= 1.0, \"Maximum pixel value is greater than 1.0\"\\n'\\\n",
    "'def calculate_lpips(img1, img2):\\n'\\\n",
    "'    check_range(img1)\\n'\\\n",
    "'    check_range(img2)\\n'\\\n",
    "'    lpips_model = LPIPS(net=\"alex\", model_path=\"/kaggle/working/p2w/.cache/lpips/alex.pth\").to(img2.device)\\n'\\\n",
    "'    return lpips_model(img1.to(img2.device), img2).item()\\n'\\\n",
    "'def calculate_ssim(img1, img2):\\n'\\\n",
    "'    check_range(img1)\\n'\\\n",
    "'    check_range(img2)\\n'\\\n",
    "'    img1 = (img1 + 1.0) / 2.0\\n'\\\n",
    "'    img2 = (img2 + 1.0) / 2.0\\n'\\\n",
    "'    return structural_similarity_index_measure(img1.to(img2.device), img2)\\n'\\\n",
    "'def calculate_psnr(img1, img2):\\n'\\\n",
    "'    check_range(img1)\\n'\\\n",
    "'    check_range(img2)\\n'\\\n",
    "'    img1 = (img1 + 1.0) / 2.0\\n'\\\n",
    "'    img2 = (img2 + 1.0) / 2.0\\n'\\\n",
    "'    mse = F.mse_loss(img1, img2)\\n'\\\n",
    "'    return 20 * log10(1.0 / torch.sqrt(mse))\\n'\\\n",
    "'def calculate_l1(img1, img2):\\n'\\\n",
    "'    check_range(img1)\\n'\\\n",
    "'    check_range(img2)\\n'\\\n",
    "'    img1 = (img1 + 1.0) / 2.0\\n'\\\n",
    "'    img2 = (img2 + 1.0) / 2.0\\n'\\\n",
    "'    return F.l1_loss(img1, img2).item()\\n'\\\n",
    "'def normalize_to_neg1_1(t):\\n'\\\n",
    "'    return t * 2 - 1\\n'\\\n",
    "'def avg(lst): return sum(lst) / len(lst) if lst else 0\\n'\\\n",
    "'# === Set Paths ===\\n'\\\n",
    "'gt_dir = \\'/kaggle/working/p2w/experiments/celebahq/250_75_time_uniform_type-dpmsolver/image_samples/results/celeba/thick/gtImg\\'\\n'\\\n",
    "'out_dir = \\'/kaggle/working/p2w/experiments/celebahq/250_75_time_uniform_type-dpmsolver/image_samples/results/celeba/thick/outImg\\'\\n'\\\n",
    "'output_path = \"/kaggle/working/inpainting_metrics_report.txt\"\\n'\\\n",
    "'# === Redirect Print to File ===\\n'\\\n",
    "'f = open(output_path, \"w\")\\n'\\\n",
    "'sys.stdout = io.TextIOWrapper(open(output_path, \\'wb\\'))\\n'\\\n",
    "'# === Preprocessing ===\\n'\\\n",
    "'transform = transforms.Compose([\\n'\\\n",
    "'    transforms.ToTensor(),\\n'\\\n",
    "'    transforms.Resize((256, 256)),\\n'\\\n",
    "'])\\n'\\\n",
    "'lpips_model = LPIPS(net=\"alex\", model_path=\"/kaggle/working/p2w/.cache/lpips/alex.pth\")\\n'\\\n",
    "'lpips_scores, ssim_scores, psnr_scores, l1_scores = [], [], [], []\\n'\\\n",
    "'filenames = sorted([\\n'\\\n",
    "'    f for f in os.listdir(out_dir)\\n'\\\n",
    "'    if f.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\'))\\n'\\\n",
    "'])\\n'\\\n",
    "'print(f\" Found {len(filenames)} image pairs to evaluate.\\\\n\")\\n'\\\n",
    "'for fname in filenames:\\n'\\\n",
    "'    path_out = os.path.join(out_dir, fname)\\n'\\\n",
    "'    path_gt = os.path.join(gt_dir, fname)\\n'\\\n",
    "'    if not os.path.exists(path_gt):\\n'\\\n",
    "'        print(f\" Missing ground truth for: {fname}, skipping.\")\\n'\\\n",
    "'        continue\\n'\\\n",
    "'    try:\\n'\\\n",
    "'        img_out = normalize_to_neg1_1(transform(Image.open(path_out).convert(\"RGB\")).unsqueeze(0))\\n'\\\n",
    "'        img_gt = normalize_to_neg1_1(transform(Image.open(path_gt).convert(\"RGB\")).unsqueeze(0))\\n'\\\n",
    "'        lpips_val = lpips_model(img_out.to(img_gt.device), img_gt.to(img_gt.device)).item()\\n'\\\n",
    "'        ssim_val = calculate_ssim(img_out, img_gt)\\n'\\\n",
    "'        psnr_val = calculate_psnr(img_out, img_gt)\\n'\\\n",
    "'        l1_val = calculate_l1(img_out, img_gt)\\n'\\\n",
    "'        lpips_scores.append(lpips_val)\\n'\\\n",
    "'        ssim_scores.append(ssim_val.item() if torch.is_tensor(ssim_val) else ssim_val)\\n'\\\n",
    "'        psnr_scores.append(psnr_val)\\n'\\\n",
    "'        l1_scores.append(l1_val)\\n'\\\n",
    "'    except Exception as e:\\n'\\\n",
    "'        print(f\" Error on {fname}: {e}\")\\n'\\\n",
    "'# === Print Final Results ===\\n'\\\n",
    "'print(\"\\\\n Overall Evaluation Results:\")\\n'\\\n",
    "'print(f\"LPIPS : {avg(lpips_scores):.4f} ↓ (lower is better)\")\\n'\\\n",
    "'print(f\"SSIM  : {avg(ssim_scores):.4f} ↑ (higher is better)\")\\n'\\\n",
    "'print(f\"PSNR  : {avg(psnr_scores):.2f} dB ↑ (higher is better)\")\\n'\\\n",
    "'print(f\"L1    : {avg(l1_scores):.4f} ↓ (lower is better)\")\\n'\\\n",
    "'# === FID Calculation (CLI) ===\\n'\\\n",
    "'print(\"\\\\n Running FID Calculation using pytorch-fid...\")\\n'\\\n",
    "'try:\\n'\\\n",
    "'    result = subprocess.run(\\n'\\\n",
    "'        [\"python\", \"-m\", \"pytorch_fid\", gt_dir, out_dir],\\n'\\\n",
    "'        capture_output=True, text=True\\n'\\\n",
    "'    )\\n'\\\n",
    "'    if result.returncode == 0:\\n'\\\n",
    "'        fid_output = result.stdout.strip()\\n'\\\n",
    "'        print(f\"FID   : {fid_output.splitlines()[-1]} ↓ (lower is better)\")\\n'\\\n",
    "'    else:\\n'\\\n",
    "'        print(\" FID calculation failed:\", result.stderr.strip())\\n'\\\n",
    "'except Exception as e:\\n'\\\n",
    "'    print(f\" FID subprocess error: {e}\")\\n'\\\n",
    "'# === Restore Output ===\\n'\\\n",
    "'sys.stdout = sys.__stdout__\\n'\\\n",
    "'print(f\"\\\\n Report saved to: {output_path}\")\\n'\n",
    "with open('/kaggle/working/p2w/metrics.py', 'w') as f:\n",
    "    f.write(metric_str)\n",
    "print(\"Successfully wrote metrics.py to /kaggle/working/p2w/metrics.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afcf1b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T22:41:32.032130Z",
     "iopub.status.busy": "2025-08-05T22:41:32.031859Z",
     "iopub.status.idle": "2025-08-05T22:43:56.534370Z",
     "shell.execute_reply": "2025-08-05T22:43:56.533198Z"
    },
    "papermill": {
     "duration": 144.746158,
     "end_time": "2025-08-05T22:43:56.536097",
     "exception": false,
     "start_time": "2025-08-05T22:41:31.789939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/miniconda/envs/p2wd/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\r\n",
      "  warnings.warn(\r\n",
      "/kaggle/working/miniconda/envs/p2wd/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\r\n",
      "  warnings.warn(msg)\r\n",
      "\r\n",
      " Report saved to: /kaggle/working/inpainting_metrics_report.txt\r\n"
     ]
    }
   ],
   "source": [
    "!source /kaggle/working/miniconda/etc/profile.d/conda.sh && conda activate p2wd && \\\n",
    "cd /kaggle/working/p2w && python metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e125e899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T22:43:56.837782Z",
     "iopub.status.busy": "2025-08-05T22:43:56.837429Z",
     "iopub.status.idle": "2025-08-05T22:44:21.071231Z",
     "shell.execute_reply": "2025-08-05T22:44:21.070197Z"
    },
    "papermill": {
     "duration": 24.385521,
     "end_time": "2025-08-05T22:44:21.072824",
     "exception": false,
     "start_time": "2025-08-05T22:43:56.687303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 images in /kaggle/working/p2w/experiments/celebahq/250_75_time_uniform_type-dpmsolver/image_samples/results/celeba/thick\n",
      "Images zipped as /kaggle/working/mdii_multistep_250_75_time_uniform_type_cfs_256_celebahq_thick.zip\n",
      "ZIP file created at /kaggle/working/mdii_multistep_250_75_time_uniform_type_cfs_256_celebahq_thick.zip. Download it from the Kaggle output panel.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory containing the images (Kaggle path)\n",
    "image_dir = '/kaggle/working/p2w/experiments/celebahq/250_75_time_uniform_type-dpmsolver/image_samples/results/celeba/thick'\n",
    "\n",
    "# Output path for the ZIP file (in Kaggle's writable directory)\n",
    "output_dir = '/kaggle/working'\n",
    "zip_name = os.path.join(output_dir, f'mdii_multistep_250_75_time_uniform_type_cfs_256_celebahq_thick.zip')\n",
    "\n",
    "# Function to collect all image files\n",
    "def collect_images(directory):\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')  # Add more extensions if needed\n",
    "    image_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    return image_files\n",
    "\n",
    "# Function to zip images\n",
    "def zip_images(image_files, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in image_files:\n",
    "            zipf.write(file, os.path.relpath(file, image_dir))\n",
    "    print(f\"Images zipped as {zip_path}\")\n",
    "\n",
    "# Collect all images\n",
    "image_files = collect_images(image_dir)\n",
    "print(f\"Found {len(image_files)} images in {image_dir}\")\n",
    "\n",
    "# Zip all images\n",
    "zip_images(image_files, zip_name)\n",
    "\n",
    "print(f\"ZIP file created at {zip_name}. Download it from the Kaggle output panel.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18202.482314,
   "end_time": "2025-08-05T22:44:21.861546",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-05T17:40:59.379232",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
